{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l5dlTFSIyikG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5dlTFSIyikG",
        "outputId": "45523d84-cc99-4837-f862-7000cac161d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.8.3)\n",
            "Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.14.1)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.35.0 trio-0.30.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rG0vHX8sUYlQ",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rG0vHX8sUYlQ",
        "outputId": "38367a67-361e-4075-b5b2-aa427e9e1273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ –ù–∞–π–¥–µ–Ω–æ 5 –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ 2 –¥–Ω.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": "**[1] [TASS] Yonhap: –Æ–∂–Ω–∞—è –ö–æ—Ä–µ—è –∏ –í—å–µ—Ç–Ω–∞–º –ø–æ–¥–ø–∏—Å–∞–ª–∏ —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ –æ –ø–æ—Å—Ç–∞–≤–∫–∞—Ö —Å–∞–º–æ—Ö–æ–¥–æ–∫ K9**  \nüóì 2025-08-14 07:58  \nüîó [https://tass.ru/ekonomika/24776477](https://tass.ru/ekonomika/24776477)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[2] [TASS] –≠–∫—Å–ø–µ—Ä—Ç –ë–æ–≥–æ–º–æ–ª–æ–≤: —Å–∞–º–º–∏—Ç –†–§ - –°–®–ê –ø–æ–º–æ–∂–µ—Ç —Ä–∞–∑–≤–∏—Ç–∏—é –∫—Ä–æ—Å—Å–ø–æ–ª—è—Ä–Ω—ã—Ö –º–∞—Ä—à—Ä—É—Ç–æ–≤**  \nüóì 2025-08-14 07:47  \nüîó [https://tass.ru/ekonomika/24776451](https://tass.ru/ekonomika/24776451)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[3] [TASS] –ê–∫—Å–∞–∫–æ–≤: —Å 2026 –≥–æ–¥–∞ —Ä–æ—Å—Å–∏—è–Ω–∞–º –∑–∞–ø—Ä–µ—Ç—è—Ç –±—Ä–∞—Ç—å –±–æ–ª–µ–µ –æ–¥–Ω–æ–≥–æ –∑–∞–π–º–∞ –≤ –ú–§–û**  \nüóì 2025-08-14 07:34  \nüîó [https://tass.ru/ekonomika/24776439](https://tass.ru/ekonomika/24776439)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[4] [TASS] –í –†–æ—Å—Å–∏–∏ –æ—Ç–º–µ–Ω–∏–ª–∏ –ø–ª–∞–Ω–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞—É–¥–∏—Ç–æ—Ä—Å–∫–∏—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∏—Å–∫–∞**  \nüóì 2025-08-14 01:18  \nüîó [https://tass.ru/ekonomika/24775349](https://tass.ru/ekonomika/24775349)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[5] [TASS] –ë—Ä–∞–∑–∏–ª–∏—è –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Ç–∞—Ä–∏—Ñ—ã –°–®–ê –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã —á–ª–µ–Ω–æ–≤ –ë–†–ò–ö–°**  \nüóì 2025-08-14 01:03  \nüîó [https://tass.ru/ekonomika/24775379](https://tass.ru/ekonomika/24775379)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import re\n",
        "import html\n",
        "import json\n",
        "import urllib.parse\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ================= –ù–ê–°–¢–†–û–ô–ö–ò =================\n",
        "KEYWORDS = [\n",
        "    '—Ç—Ä–∞–º–ø', 'trump', '–¥–æ–Ω–∞–ª—å–¥',\n",
        "    '—Å—à–∞', '–≥–µ—Ä–º–∞–Ω', '–∫–∏—Ç–∞–π', '—Ä–æ—Å—Å–∏',\n",
        "    '—ç–∫–æ–Ω–æ–º', '–∏–Ω—Ñ–ª—è—Ü', '—Å—Ç–∞–≤–∫', '–≤–≤–ø', 'moex', '—Ñ—Ä—Å', 'ecb'\n",
        "]\n",
        "DAYS_BACK = 2\n",
        "MAX_NEWS_PER_SITE = 50           # –º–∞–∫—Å–∏–º—É–º –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Å–∞–π—Ç (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤)\n",
        "MAX_LINKS_TASS = 120             # —Å–∫–æ–ª—å–∫–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –ø—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —É TASS\n",
        "MAX_LINKS_INTERFAX = 120         # —Å–∫–æ–ª—å–∫–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –ø—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —É Interfax\n",
        "DEBUG = False                    # –≤–∫–ª—é—á–∏—Ç—å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É –æ—Ç–±—Ä–∞–∫–æ–≤–∫–∏\n",
        "\n",
        "# –ò—Å—Ç–æ—á–Ω–∏–∫–∏\n",
        "TASS_LIST_URL = \"https://tass.ru/ekonomika\"\n",
        "INTERFAX_LIST_URL = \"https://www.interfax.ru/business\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                   \"Chrome/125.0 Safari/537.36\"),\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "    \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
        "    \"Connection\": \"keep-alive\",\n",
        "    \"Referer\": \"https://google.com\"\n",
        "}\n",
        "\n",
        "# –†—É—Å—Å–∫–∏–µ –º–µ—Å—è—Ü—ã ‚Üí –Ω–æ–º–µ—Ä\n",
        "RU_MONTHS = {\n",
        "    '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4, '–º–∞—è': 5, '–∏—é–Ω—è': 6,\n",
        "    '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8, '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12\n",
        "}\n",
        "\n",
        "ISO_REGEX = re.compile(r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}(?::\\d{2})?(?:Z|[+\\-]\\d{2}:\\d{2})', re.IGNORECASE)\n",
        "\n",
        "# ================= –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–û–ï =================\n",
        "def now_utc():\n",
        "    return datetime.now(timezone.utc)\n",
        "\n",
        "def within_days(dt: datetime, days: int) -> bool:\n",
        "    \"\"\"dt –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å tz-aware; —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ç–µ–∫—É—â–∏–º UTC.\"\"\"\n",
        "    if dt.tzinfo is None:\n",
        "        dt = dt.replace(tzinfo=timezone.utc)\n",
        "    return (now_utc() - dt) <= timedelta(days=days)\n",
        "\n",
        "def title_matches(title: str) -> bool:\n",
        "    tl = (title or \"\").lower()\n",
        "    return any(kw in tl for kw in KEYWORDS)\n",
        "\n",
        "def _req(url: str, timeout=20) -> requests.Response | None:\n",
        "    try:\n",
        "        r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "        enc = r.encoding or r.apparent_encoding or 'utf-8'\n",
        "        r.encoding = enc\n",
        "        if r.status_code == 200:\n",
        "            return r\n",
        "        if DEBUG: print(f\"[WARN] HTTP {r.status_code} for {url}\")\n",
        "    except Exception as e:\n",
        "        if DEBUG: print(f\"[ERR] {e} for {url}\")\n",
        "    return None\n",
        "\n",
        "# -------- –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç --------\n",
        "def parse_iso_or_rfc2822(s: str) -> datetime | None:\n",
        "    if not s:\n",
        "        return None\n",
        "    s = s.strip()\n",
        "    # ISO 8601\n",
        "    try:\n",
        "        if s.endswith('Z'):\n",
        "            dt = datetime.fromisoformat(s.replace('Z', '+00:00'))\n",
        "        else:\n",
        "            dt = datetime.fromisoformat(s)\n",
        "        return dt.astimezone(timezone.utc)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # RFC2822\n",
        "    try:\n",
        "        from email.utils import parsedate_to_datetime\n",
        "        dt = parsedate_to_datetime(s)\n",
        "        if dt.tzinfo is None:\n",
        "            dt = dt.replace(tzinfo=timezone.utc)\n",
        "        return dt.astimezone(timezone.utc)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "RU_NUMERIC_RE = re.compile(\n",
        "    r'(?P<d>\\d{1,2})\\.(?P<m>\\d{1,2})\\.(?P<y>\\d{4})(?:\\s+(?P<h>\\d{1,2}):(?P<min>\\d{2}))?'\n",
        ")\n",
        "RU_TEXTUAL_RE = re.compile(\n",
        "    r'(?P<d>\\d{1,2})\\s+(?P<mon>[–ê-–Ø–∞-—è]+)\\s+(?P<y>\\d{4})'\n",
        "    r'(?:\\s*–≥\\.?|(?:\\s*–≥–æ–¥–∞)?)?(?:,\\s*|\\s+)?(?:(?P<h>\\d{1,2}):(?P<min>\\d{2}))?',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "def parse_russian_date(text: str, default_tz=timezone(timedelta(hours=3))) -> datetime | None:\n",
        "    \"\"\"–ü–∞—Ä—Å–∏–º '13 –∞–≤–≥—É—Å—Ç–∞ 2025, 16:49' –∏–ª–∏ '13.08.2025 16:49'. –í–æ–∑–≤—Ä–∞—â–∞–µ–º UTC.\"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "    s = html.unescape(text.strip().lower())\n",
        "\n",
        "    m = RU_NUMERIC_RE.search(s)\n",
        "    if m:\n",
        "        d, mth, y = int(m.group('d')), int(m.group('m')), int(m.group('y'))\n",
        "        hh = int(m.group('h') or 0)\n",
        "        mm = int(m.group('min') or 0)\n",
        "        try:\n",
        "            dt = datetime(y, mth, d, hh, mm, tzinfo=default_tz)\n",
        "            return dt.astimezone(timezone.utc)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    m = RU_TEXTUAL_RE.search(s)\n",
        "    if m:\n",
        "        d = int(m.group('d'))\n",
        "        mon_name = m.group('mon')\n",
        "        y = int(m.group('y'))\n",
        "        mth = RU_MONTHS.get(mon_name, None)\n",
        "        if not mth:\n",
        "            return None\n",
        "        hh = int(m.group('h') or 0)\n",
        "        mm = int(m.group('min') or 0)\n",
        "        try:\n",
        "            dt = datetime(y, mth, d, hh, mm, tzinfo=default_tz)\n",
        "            return dt.astimezone(timezone.utc)\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def extract_jsonld_datetime(soup: BeautifulSoup) -> datetime | None:\n",
        "    \"\"\"–ß–∏—Ç–∞–µ–º datePublished –∏–∑ JSON-LD (NewsArticle).\"\"\"\n",
        "    for tag in soup.find_all(\"script\", attrs={\"type\": \"application/ld+json\"}):\n",
        "        try:\n",
        "            data = json.loads(tag.string or tag.text or \"\")\n",
        "        except Exception:\n",
        "            continue\n",
        "        objs = data if isinstance(data, list) else [data]\n",
        "        for obj in objs:\n",
        "            if not isinstance(obj, dict):\n",
        "                continue\n",
        "            dp = obj.get(\"datePublished\") or obj.get(\"dateCreated\")\n",
        "            if dp:\n",
        "                dt = parse_iso_or_rfc2822(dp)\n",
        "                if dt:\n",
        "                    return dt\n",
        "    return None\n",
        "\n",
        "def extract_datetime_generic(html_text: str, soup: BeautifulSoup) -> datetime | None:\n",
        "    # 1) JSON-LD\n",
        "    dt = extract_jsonld_datetime(soup)\n",
        "    if dt:\n",
        "        return dt\n",
        "    # 2) <time datetime=\"...\"> –∏–ª–∏ <meta itemprop=\"datePublished\" content=\"...\"> –∏–ª–∏ og:published_time\n",
        "    t = soup.find('time', attrs={'datetime': True}) or soup.find('time', attrs={'content': True})\n",
        "    if t:\n",
        "        dt = parse_iso_or_rfc2822(t.get('datetime') or t.get('content'))\n",
        "        if dt:\n",
        "            return dt\n",
        "    meta = soup.find('meta', attrs={'itemprop': 'datePublished'}) or \\\n",
        "           soup.find('meta', attrs={'property': 'article:published_time'})\n",
        "    if meta and meta.get('content'):\n",
        "        dt = parse_iso_or_rfc2822(meta['content'])\n",
        "        if dt:\n",
        "            return dt\n",
        "    # 3) ISO –≤ —Å—ã—Ä–æ–º HTML\n",
        "    m = ISO_REGEX.search(html_text)\n",
        "    if m:\n",
        "        dt = parse_iso_or_rfc2822(m.group(0))\n",
        "        if dt:\n",
        "            return dt\n",
        "    # 4) –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç (–±–µ—Ä—ë–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã)\n",
        "    flat = ' '.join(soup.stripped_strings)\n",
        "    return parse_russian_date(flat)\n",
        "\n",
        "def extract_title_generic(soup: BeautifulSoup) -> str:\n",
        "    h1 = soup.find('h1')\n",
        "    if h1 and h1.get_text(strip=True):\n",
        "        return h1.get_text(strip=True)\n",
        "    og = soup.find('meta', attrs={'property': 'og:title'})\n",
        "    if og and og.get('content'):\n",
        "        return og['content'].strip()\n",
        "    if soup.title and soup.title.text:\n",
        "        return soup.title.text.strip()\n",
        "    return \"\"\n",
        "\n",
        "# ================= TASS =================\n",
        "# –õ–æ–≤–∏–º —Å—Å—ã–ª–∫–∏ —Å—Ç–∞—Ç–µ–π —Ä–∞–∑–¥–µ–ª–∞ /ekonomika/<id>...\n",
        "TASS_LINK_RE = re.compile(\n",
        "    r'href=[\"\\'](?:https?://(?:www\\.)?tass\\.ru)?(/ekonomika/\\d+[^\\s\"\\']*)[\"\\']',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "def fetch_tass():\n",
        "    out = []\n",
        "    list_resp = _req(TASS_LIST_URL)\n",
        "    if not list_resp:\n",
        "        print(\"[TASS][ERROR] –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–µ–Ω—Ç—É\")\n",
        "        return out\n",
        "\n",
        "    html_list = list_resp.text\n",
        "    links = []\n",
        "    seen = set()\n",
        "    for m in TASS_LINK_RE.finditer(html_list):\n",
        "        rel = m.group(1)\n",
        "        if not rel:\n",
        "            continue\n",
        "        url = urllib.parse.urljoin(TASS_LIST_URL, rel)\n",
        "        if url not in seen:\n",
        "            seen.add(url)\n",
        "            links.append(url)\n",
        "        if len(links) >= MAX_LINKS_TASS:\n",
        "            break\n",
        "\n",
        "    if DEBUG:\n",
        "        print(f\"[TASS] –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(links)}\")\n",
        "\n",
        "    no_title = no_kw = no_dt = old_dt = 0\n",
        "    for url in links:\n",
        "        r = _req(url, timeout=20)\n",
        "        if not r:\n",
        "            continue\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        title = extract_title_generic(soup)\n",
        "        if not title:\n",
        "            no_title += 1\n",
        "            continue\n",
        "        if not title_matches(title):\n",
        "            no_kw += 1\n",
        "            continue\n",
        "        dt = extract_datetime_generic(r.text, soup)\n",
        "        if not dt:\n",
        "            no_dt += 1\n",
        "            continue\n",
        "        if not within_days(dt, DAYS_BACK):\n",
        "            old_dt += 1\n",
        "            continue\n",
        "        out.append({'source': 'TASS', 'title': title, 'link': url, 'date': dt})\n",
        "        if len(out) >= MAX_NEWS_PER_SITE:\n",
        "            break\n",
        "\n",
        "    if DEBUG:\n",
        "        print(f\"[TASS] –ø—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä: {len(out)} | –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞: {no_title}, –±–µ–∑ KEYWORDS: {no_kw}, –±–µ–∑ –¥–∞—Ç—ã: {no_dt}, —Å—Ç–∞—Ä—à–µ {DAYS_BACK} –¥–Ω.: {old_dt}\")\n",
        "    return out\n",
        "\n",
        "# ================= Interfax =================\n",
        "# –°—Å—ã–ª–∫–∏ –≤–∏–¥–∞ /business/123456\n",
        "INTERFAX_LINK_RE = re.compile(\n",
        "    r'href=[\"\\'](?:https?://(?:www\\.)?interfax\\.ru)?(/business/\\d+[^\\s\"\\']*)[\"\\']',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "def fetch_interfax():\n",
        "    out = []\n",
        "    list_resp = _req(INTERFAX_LIST_URL)\n",
        "    if not list_resp:\n",
        "        print(\"[Interfax][ERROR] –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–µ–Ω—Ç—É\")\n",
        "        return out\n",
        "\n",
        "    html_list = list_resp.text\n",
        "    links = []\n",
        "    seen = set()\n",
        "    for m in INTERFAX_LINK_RE.finditer(html_list):\n",
        "        rel = m.group(1)\n",
        "        if not rel:\n",
        "            continue\n",
        "        url = urllib.parse.urljoin(INTERFAX_LIST_URL, rel)\n",
        "        if url not in seen:\n",
        "            seen.add(url)\n",
        "            links.append(url)\n",
        "        if len(links) >= MAX_LINKS_INTERFAX:\n",
        "            break\n",
        "\n",
        "    if DEBUG:\n",
        "        print(f\"[Interfax] –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(links)}\")\n",
        "\n",
        "    no_title = no_kw = no_dt = old_dt = 0\n",
        "    for url in links:\n",
        "        r = _req(url, timeout=20)\n",
        "        if not r:\n",
        "            continue\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        title = extract_title_generic(soup)\n",
        "        if not title:\n",
        "            no_title += 1\n",
        "            continue\n",
        "        if not title_matches(title):\n",
        "            no_kw += 1\n",
        "            continue\n",
        "        dt = extract_datetime_generic(r.text, soup)\n",
        "        if not dt:\n",
        "            no_dt += 1\n",
        "            continue\n",
        "        if not within_days(dt, DAYS_BACK):\n",
        "            old_dt += 1\n",
        "            continue\n",
        "        out.append({'source': 'Interfax', 'title': title, 'link': url, 'date': dt})\n",
        "        if len(out) >= MAX_NEWS_PER_SITE:\n",
        "            break\n",
        "\n",
        "    if DEBUG:\n",
        "        print(f\"[Interfax] –ø—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä: {len(out)} | –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞: {no_title}, –±–µ–∑ KEYWORDS: {no_kw}, –±–µ–∑ –¥–∞—Ç—ã: {no_dt}, —Å—Ç–∞—Ä—à–µ {DAYS_BACK} –¥–Ω.: {old_dt}\")\n",
        "    return out\n",
        "\n",
        "# ================= –í–´–í–û–î =================\n",
        "def print_articles(items):\n",
        "    if not items:\n",
        "        print(\"‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.\")\n",
        "        return\n",
        "    items.sort(key=lambda x: x['date'] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\n",
        "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(items)} –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ {DAYS_BACK} –¥–Ω.\\n\")\n",
        "\n",
        "    # Jupyter: –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏\n",
        "    try:\n",
        "        from IPython.display import display, Markdown\n",
        "        for i, a in enumerate(items, 1):\n",
        "            # –≤—ã–≤–æ–¥–∏–º –ø–æ Asia/Bishkek (UTC+6), —á—Ç–æ–±—ã –±—ã–ª–æ –≤ —Ç–≤–æ–µ–π TZ\n",
        "            dt_local = a['date'].astimezone(timezone(timedelta(hours=6)))\n",
        "            display(Markdown(\n",
        "                f\"**[{i}] [{a['source']}] {a['title']}**  \\n\"\n",
        "                f\"üóì {dt_local.strftime('%Y-%m-%d %H:%M')}  \\n\"\n",
        "                f\"üîó [{a['link']}]({a['link']})\\n\"\n",
        "            ))\n",
        "    except Exception:\n",
        "        for i, a in enumerate(items, 1):\n",
        "            dt_local = a['date'].astimezone(timezone(timedelta(hours=6)))\n",
        "            print(f\"[{i}] [{a['source']}] {a['title']}\\n\"\n",
        "                  f\"üóì {dt_local.strftime('%Y-%m-%d %H:%M')}\\n\"\n",
        "                  f\"üîó {a['link']}\\n\")\n",
        "\n",
        "def main():\n",
        "    tass_items = fetch_tass()\n",
        "    interfax_items = fetch_interfax()\n",
        "    all_items = tass_items + interfax_items\n",
        "    print_articles(all_items)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48dd9f42bfeead39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48dd9f42bfeead39",
        "outputId": "7b2e4ece-9aa1-47f6-a2fb-97687d612718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É...\n",
            "[INFO] –ù–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: 20\n",
            "\n",
            "‚úÖ –ù–∞–π–¥–µ–Ω–æ 4 –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ñ–∏–ª—å—Ç—Ä—É:\n",
            "\n",
            "[1] [2025-08-13] –†–æ—Å—Å—Ç–∞—Ç —Å–æ–æ–±—â–∏–ª –æ –∑–∞–º–µ–¥–ª–µ–Ω–∏–∏ —Ä–æ—Å—Ç–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —ç–∫–æ–Ω–æ–º–∏–∫–∏\n",
            "     üîó https://www.rbc.ru/economics/13/08/2025/689cbb6f9a7947396a1ea3a7\n",
            "\n",
            "[2] [2025-08-13] –í –ò–Ω–¥–∏–∏ —Ä–∞—Å—Å–∫–∞–∑–∞–ª–∏ –æ –∑–∞—â–∏—Ç–µ –æ—Ç —Ä–∏—Å–∫–æ–≤ –¥–ª—è —Ä—ã–Ω–∫–∞ –Ω–µ—Ñ—Ç–∏ –∏–∑-–∑–∞ –ø–æ—à–ª–∏–Ω –¢—Ä–∞–º–ø–∞\n",
            "     üîó https://www.rbc.ru/economics/13/08/2025/689bbb429a794773357cb68a\n",
            "\n",
            "[3] [2025-08-11] –¢—Ä–∞–º–ø –ø–æ—Å—Ç–∞–≤–∏–ª –ö–∏—Ç–∞—é –Ω–æ–≤—ã–π –¥–µ–¥–ª–∞–π–Ω –ø–æ –ø–æ—à–ª–∏–Ω–∞–º\n",
            "     üîó https://www.rbc.ru/economics/11/08/2025/689a36c79a7947ba41acd89e\n",
            "\n",
            "[4] [2025-08-11] –¢—Ä–∞–º–ø –æ—Ç–º–µ–Ω–∏–ª ¬´—É–¥–∞—Ä¬ª –ø–æ –∑–æ–ª–æ—Ç—É –®–≤–µ–π—Ü–∞—Ä–∏–∏\n",
            "     üîó https://www.rbc.ru/economics/11/08/2025/689a2c119a7947d7cfde870c\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import time\n",
        "import re\n",
        "\n",
        "# === –ù–ê–°–¢–†–û–ô–ö–ò ===\n",
        "KEYWORDS = ['—Ç—Ä–∞–º–ø', '–∫–∞–Ω–∞–¥', '–≥–µ—Ä–º–∞–Ω–∏', '—Å—à–∞', '–∞–≤—Å—Ç—Ä–∞–ª–∏', '—è–ø–æ–Ω–∏', '–≤–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏', '–∫–∏—Ç–∞–π', '—Ä–æ—Å—Å–∏']\n",
        "DAYS_BACK = 2\n",
        "URL = 'https://www.rbc.ru/economics/'\n",
        "\n",
        "# === –ó–ê–ü–£–°–ö SELENIUM ===\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "print(\"[INFO] –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É...\")\n",
        "driver.get(URL)\n",
        "time.sleep(5)  # –õ—É—á—à–µ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —è–≤–Ω—ã–µ –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞\n",
        "\n",
        "html = driver.page_source\n",
        "driver.quit()\n",
        "\n",
        "# === –ü–ê–†–°–ò–ù–ì –ò –û–î–ù–û–í–†–ï–ú–ï–ù–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø ===\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "items = soup.find_all('div', class_='item')\n",
        "print(f\"[INFO] –ù–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(items)}\")\n",
        "\n",
        "now = datetime.now()\n",
        "filtered_articles = []\n",
        "\n",
        "for item in items:\n",
        "    title_tag = item.find('span', class_='item__title')\n",
        "    link_tag = item.find('a', class_='item__link')\n",
        "\n",
        "    if not title_tag or not link_tag:\n",
        "        continue\n",
        "\n",
        "    title = ''.join(title_tag.stripped_strings)\n",
        "    link = link_tag['href']\n",
        "\n",
        "    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –∏–∑ URL\n",
        "    match = re.search(r'/(\\d{2})/(\\d{2})/(\\d{4})/', link)\n",
        "    if not match:\n",
        "        continue\n",
        "    day, month, year = map(int, match.groups())\n",
        "    pub_date = datetime(year, month, day)\n",
        "\n",
        "    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å—Ç–∞—Ä—à–µ DAYS_BACK\n",
        "    if (now - pub_date).days > DAYS_BACK:\n",
        "        continue\n",
        "\n",
        "    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞\n",
        "    title_lower = title.lower()\n",
        "    if not any(kw in title_lower for kw in KEYWORDS):\n",
        "        continue\n",
        "\n",
        "    filtered_articles.append({\n",
        "        'title': title,\n",
        "        'link': link,\n",
        "        'date': pub_date\n",
        "    })\n",
        "\n",
        "# === –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===\n",
        "if not filtered_articles:\n",
        "    print(\"\\n‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(filtered_articles)} –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ñ–∏–ª—å—Ç—Ä—É:\\n\")\n",
        "    for i, a in enumerate(filtered_articles, 1):\n",
        "        print(f\"[{i}] [{a['date'].strftime('%Y-%m-%d')}] {a['title']}\\n     üîó {a['link']}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bc429103ff66a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bc429103ff66a7",
        "outputId": "ef54b961-2c72-49d8-8dea-7adc0959ee2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ HTML\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "KEYWORDS = ['—Ç—Ä–∞–º–ø', 'trump', '–¥–æ–Ω–∞–ª—å–¥']\n",
        "DAYS_BACK = 2\n",
        "URL = 'https://iz.ru/news'\n",
        "MAX_NEWS = 50\n",
        "BASE_URL = 'https://iz.ru'\n",
        "\n",
        "def extract_json_from_html(html):\n",
        "    start_pattern = r'window\\.recommendationBlockList\\s*=\\s*{'\n",
        "    start_match = re.search(start_pattern, html)\n",
        "    if not start_match:\n",
        "        return None\n",
        "\n",
        "    start_index = start_match.end() - 1  # –ø–æ–∑–∏—Ü–∏—è –ø–µ—Ä–≤–æ–π {\n",
        "\n",
        "    braces = 0\n",
        "    end_index = start_index\n",
        "    for i, ch in enumerate(html[start_index:], start=start_index):\n",
        "        if ch == '{':\n",
        "            braces += 1\n",
        "        elif ch == '}':\n",
        "            braces -= 1\n",
        "            if braces == 0:\n",
        "                end_index = i\n",
        "                break\n",
        "\n",
        "    json_text = html[start_index:end_index+1]\n",
        "    return json_text\n",
        "\n",
        "def parse_articles(json_data):\n",
        "    articles = []\n",
        "    for key in ['even', 'odd']:\n",
        "        for item in json_data.get(key, []):\n",
        "            path = item.get('path', '')\n",
        "            if path.startswith('http://') or path.startswith('https://'):\n",
        "                full_link = path\n",
        "            else:\n",
        "                full_link = BASE_URL + path if path.startswith('/') else BASE_URL + '/' + path\n",
        "            articles.append({\n",
        "                'title': item.get('title', ''),\n",
        "                'link': full_link,\n",
        "                'date': None,\n",
        "                'reference': item.get('reference', '')\n",
        "            })\n",
        "    return articles\n",
        "\n",
        "def main():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    driver.get(URL)\n",
        "\n",
        "    html = driver.page_source\n",
        "    driver.quit()\n",
        "\n",
        "    json_text = extract_json_from_html(html)\n",
        "    if not json_text:\n",
        "        print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ HTML\")\n",
        "        return\n",
        "\n",
        "    data = json.loads(json_text)\n",
        "    articles = parse_articles(data)\n",
        "\n",
        "    filtered = []\n",
        "    now = datetime.now()\n",
        "    cutoff = now - timedelta(days=DAYS_BACK)\n",
        "\n",
        "    for a in articles:\n",
        "        title_lower = a['title'].lower()\n",
        "        if not any(kw in title_lower for kw in KEYWORDS):\n",
        "            continue\n",
        "        filtered.append(a)\n",
        "        if len(filtered) >= MAX_NEWS:\n",
        "            break\n",
        "\n",
        "    if not filtered:\n",
        "        print(\"‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(filtered)} –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ñ–∏–ª—å—Ç—Ä—É:\\n\")\n",
        "    for i, a in enumerate(filtered, 1):\n",
        "        # –í—ã–≤–æ–¥ –∫–ª–∏–∫–∞–µ–º–æ–π —Å—Å—ã–ª–∫–∏ —á–µ—Ä–µ–∑ Markdown\n",
        "        display(Markdown(f\"**[{i}] {a['title']}**  \\nüîó [{a['link']}]({a['link']})\\n\"))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20bafbd1bc48320a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "20bafbd1bc48320a",
        "outputId": "bc7af108-0f0f-4ce9-cbb9-16a096f96f1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] –ó–∞–≥—Ä—É–∂–∞–µ–º RBC...\n",
            "[INFO] –ó–∞–≥—Ä—É–∂–∞–µ–º Iz.ru...\n",
            "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ Iz.ru\n",
            "‚úÖ –ù–∞–π–¥–µ–Ω–æ 4 –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ñ–∏–ª—å—Ç—Ä—É:\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": "**[1] –†–æ—Å—Å—Ç–∞—Ç —Å–æ–æ–±—â–∏–ª –æ –∑–∞–º–µ–¥–ª–µ–Ω–∏–∏ —Ä–æ—Å—Ç–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —ç–∫–æ–Ω–æ–º–∏–∫–∏**  \nüîó [https://www.rbc.ru/economics/13/08/2025/689cbb6f9a7947396a1ea3a7](https://www.rbc.ru/economics/13/08/2025/689cbb6f9a7947396a1ea3a7)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[2] –í –ò–Ω–¥–∏–∏ —Ä–∞—Å—Å–∫–∞–∑–∞–ª–∏ –æ –∑–∞—â–∏—Ç–µ –æ—Ç —Ä–∏—Å–∫–æ–≤ –¥–ª—è —Ä—ã–Ω–∫–∞ –Ω–µ—Ñ—Ç–∏ –∏–∑-–∑–∞ –ø–æ—à–ª–∏–Ω –¢—Ä–∞–º–ø–∞**  \nüîó [https://www.rbc.ru/economics/13/08/2025/689bbb429a794773357cb68a](https://www.rbc.ru/economics/13/08/2025/689bbb429a794773357cb68a)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[3] –¢—Ä–∞–º–ø –ø–æ—Å—Ç–∞–≤–∏–ª –ö–∏—Ç–∞—é –Ω–æ–≤—ã–π –¥–µ–¥–ª–∞–π–Ω –ø–æ –ø–æ—à–ª–∏–Ω–∞–º**  \nüîó [https://www.rbc.ru/economics/11/08/2025/689a36c79a7947ba41acd89e](https://www.rbc.ru/economics/11/08/2025/689a36c79a7947ba41acd89e)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[4] –¢—Ä–∞–º–ø –æ—Ç–º–µ–Ω–∏–ª ¬´—É–¥–∞—Ä¬ª –ø–æ –∑–æ–ª–æ—Ç—É –®–≤–µ–π—Ü–∞—Ä–∏–∏**  \nüîó [https://www.rbc.ru/economics/11/08/2025/689a2c119a7947d7cfde870c](https://www.rbc.ru/economics/11/08/2025/689a2c119a7947d7cfde870c)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import display, Markdown\n",
        "import time\n",
        "\n",
        "KEYWORDS = ['—Ç—Ä–∞–º–ø', 'trump', '–¥–æ–Ω–∞–ª—å–¥', '–∫–∞–Ω–∞–¥', '–≥–µ—Ä–º–∞–Ω–∏', '—Å—à–∞', '–∞–≤—Å—Ç—Ä–∞–ª–∏', '—è–ø–æ–Ω–∏', '–≤–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏', '–∫–∏—Ç–∞–π', '—Ä–æ—Å—Å–∏']\n",
        "DAYS_BACK = 2\n",
        "MAX_NEWS = 50\n",
        "\n",
        "def parse_rbc(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    items = soup.find_all('div', class_='item')\n",
        "    now = datetime.now()\n",
        "    filtered_articles = []\n",
        "\n",
        "    for item in items:\n",
        "        title_tag = item.find('span', class_='item__title')\n",
        "        link_tag = item.find('a', class_='item__link')\n",
        "\n",
        "        if not title_tag or not link_tag:\n",
        "            continue\n",
        "\n",
        "        title = ''.join(title_tag.stripped_strings)\n",
        "        link = link_tag['href']\n",
        "\n",
        "        match = re.search(r'/(\\d{2})/(\\d{2})/(\\d{4})/', link)\n",
        "        if not match:\n",
        "            continue\n",
        "        day, month, year = map(int, match.groups())\n",
        "        pub_date = datetime(year, month, day)\n",
        "\n",
        "        if (now - pub_date).days > DAYS_BACK:\n",
        "            continue\n",
        "\n",
        "        title_lower = title.lower()\n",
        "        if not any(kw in title_lower for kw in KEYWORDS):\n",
        "            continue\n",
        "\n",
        "        filtered_articles.append({\n",
        "            'title': title,\n",
        "            'link': link,\n",
        "            'date': pub_date\n",
        "        })\n",
        "\n",
        "    return filtered_articles\n",
        "\n",
        "def extract_json_from_html(html):\n",
        "    start_pattern = r'window\\.recommendationBlockList\\s*=\\s*{'\n",
        "    start_match = re.search(start_pattern, html)\n",
        "    if not start_match:\n",
        "        return None\n",
        "\n",
        "    start_index = start_match.end() - 1\n",
        "    braces = 0\n",
        "    end_index = start_index\n",
        "    for i, ch in enumerate(html[start_index:], start=start_index):\n",
        "        if ch == '{':\n",
        "            braces += 1\n",
        "        elif ch == '}':\n",
        "            braces -= 1\n",
        "            if braces == 0:\n",
        "                end_index = i\n",
        "                break\n",
        "\n",
        "    return html[start_index:end_index+1]\n",
        "\n",
        "def parse_iz(json_data):\n",
        "    BASE_URL = 'https://iz.ru'\n",
        "    articles = []\n",
        "    for key in ['even', 'odd']:\n",
        "        for item in json_data.get(key, []):\n",
        "            path = item.get('path', '')\n",
        "            if path.startswith('http://') or path.startswith('https://'):\n",
        "                full_link = path\n",
        "            else:\n",
        "                full_link = BASE_URL + path if path.startswith('/') else BASE_URL + '/' + path\n",
        "            articles.append({\n",
        "                'title': item.get('title', ''),\n",
        "                'link': full_link,\n",
        "                'date': None,\n",
        "            })\n",
        "    return articles\n",
        "\n",
        "def fetch_with_selenium(url):\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    driver.get(url)\n",
        "    time.sleep(5)  # –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —è–≤–Ω—ã–µ –æ–∂–∏–¥–∞–Ω–∏—è\n",
        "    html = driver.page_source\n",
        "    driver.quit()\n",
        "    return html\n",
        "\n",
        "def main():\n",
        "    # --- RBC ---\n",
        "    print(\"[INFO] –ó–∞–≥—Ä—É–∂–∞–µ–º RBC...\")\n",
        "    rbc_html = fetch_with_selenium('https://www.rbc.ru/economics/')\n",
        "    rbc_articles = parse_rbc(rbc_html)\n",
        "\n",
        "    # --- Iz.ru ---\n",
        "    print(\"[INFO] –ó–∞–≥—Ä—É–∂–∞–µ–º Iz.ru...\")\n",
        "    iz_html = fetch_with_selenium('https://iz.ru/news')\n",
        "    json_text = extract_json_from_html(iz_html)\n",
        "    if not json_text:\n",
        "        print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ Iz.ru\")\n",
        "        iz_articles = []\n",
        "    else:\n",
        "        data = json.loads(json_text)\n",
        "        iz_articles = parse_iz(data)\n",
        "\n",
        "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–∞—Ç—å–∏\n",
        "    all_articles = rbc_articles + iz_articles\n",
        "\n",
        "    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏ –¥–∞—Ç–µ –¥–ª—è Iz.ru (–¥–∞—Ç—ã –Ω–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ –≤—Ä–µ–º–µ–Ω–∏)\n",
        "    now = datetime.now()\n",
        "    cutoff = now - timedelta(days=DAYS_BACK)\n",
        "\n",
        "    filtered = []\n",
        "    for a in all_articles:\n",
        "        title_lower = a['title'].lower()\n",
        "        if not any(kw in title_lower for kw in KEYWORDS):\n",
        "            continue\n",
        "        # –î–ª—è RBC –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç—ã —É–∂–µ —Å–¥–µ–ª–∞–Ω–∞\n",
        "        # –î–ª—è Iz.ru –¥–∞—Ç—ã –Ω–µ—Ç, —Å—á–∏—Ç–∞–µ–º –¥–æ–ø—É—Å—Ç–∏–º—ã–º–∏ –≤—Å–µ\n",
        "        filtered.append(a)\n",
        "        if len(filtered) >= MAX_NEWS:\n",
        "            break\n",
        "\n",
        "    if not filtered:\n",
        "        print(\"‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(filtered)} –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ñ–∏–ª—å—Ç—Ä—É:\\n\")\n",
        "    for i, a in enumerate(filtered, 1):\n",
        "        # –í Jupyter –¥–µ–ª–∞–µ–º –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥\n",
        "        display(Markdown(f\"**[{i}] {a['title']}**  \\nüîó [{a['link']}]({a['link']})\\n\"))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a34eb9f6a08b45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "37a34eb9f6a08b45",
        "outputId": "bd803880-b12a-48f7-fee3-86eabad3ba25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–µ–π iz.ru –≤ HTML\n",
            "‚úÖ –ù–∞–π–¥–µ–Ω–æ 4 –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ñ–∏–ª—å—Ç—Ä—É:\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": "**[1] –†–æ—Å—Å—Ç–∞—Ç —Å–æ–æ–±—â–∏–ª –æ –∑–∞–º–µ–¥–ª–µ–Ω–∏–∏ —Ä–æ—Å—Ç–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —ç–∫–æ–Ω–æ–º–∏–∫–∏**  \nüîó [https://www.rbc.ru/economics/13/08/2025/689cbb6f9a7947396a1ea3a7](https://www.rbc.ru/economics/13/08/2025/689cbb6f9a7947396a1ea3a7)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[2] –í –ò–Ω–¥–∏–∏ —Ä–∞—Å—Å–∫–∞–∑–∞–ª–∏ –æ –∑–∞—â–∏—Ç–µ –æ—Ç —Ä–∏—Å–∫–æ–≤ –¥–ª—è —Ä—ã–Ω–∫–∞ –Ω–µ—Ñ—Ç–∏ –∏–∑-–∑–∞ –ø–æ—à–ª–∏–Ω –¢—Ä–∞–º–ø–∞**  \nüîó [https://www.rbc.ru/economics/13/08/2025/689bbb429a794773357cb68a](https://www.rbc.ru/economics/13/08/2025/689bbb429a794773357cb68a)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[3] –¢—Ä–∞–º–ø –ø–æ—Å—Ç–∞–≤–∏–ª –ö–∏—Ç–∞—é –Ω–æ–≤—ã–π –¥–µ–¥–ª–∞–π–Ω –ø–æ –ø–æ—à–ª–∏–Ω–∞–º**  \nüîó [https://www.rbc.ru/economics/11/08/2025/689a36c79a7947ba41acd89e](https://www.rbc.ru/economics/11/08/2025/689a36c79a7947ba41acd89e)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[4] –¢—Ä–∞–º–ø –æ—Ç–º–µ–Ω–∏–ª ¬´—É–¥–∞—Ä¬ª –ø–æ –∑–æ–ª–æ—Ç—É –®–≤–µ–π—Ü–∞—Ä–∏–∏**  \nüîó [https://www.rbc.ru/economics/11/08/2025/689a2c119a7947d7cfde870c](https://www.rbc.ru/economics/11/08/2025/689a2c119a7947d7cfde870c)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import display, Markdown  # –¥–ª—è –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –≤ Jupyter, –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å ‚Äî –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ print\n",
        "\n",
        "# === –ù–ê–°–¢–†–û–ô–ö–ò ===\n",
        "KEYWORDS = ['—Ç—Ä–∞–º–ø', 'trump', '–¥–æ–Ω–∞–ª—å–¥', '–∫–∞–Ω–∞–¥', '–≥–µ—Ä–º–∞–Ω–∏', '—Å—à–∞', '–∞–≤—Å—Ç—Ä–∞–ª–∏', '—è–ø–æ–Ω–∏', '–≤–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏', '–∫–∏—Ç–∞–π', '—Ä–æ—Å—Å–∏']\n",
        "DAYS_BACK = 2\n",
        "MAX_NEWS_IZ = 50\n",
        "\n",
        "# --- URLs ---\n",
        "RBC_URL = 'https://www.rbc.ru/economics/'\n",
        "IZ_URL = 'https://iz.ru/news'\n",
        "IZ_BASE_URL = 'https://iz.ru'\n",
        "\n",
        "def fetch_rbc_articles(driver, now):\n",
        "    driver.get(RBC_URL)\n",
        "    driver.implicitly_wait(5)  # –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —è–≤–Ω—ã–µ –æ–∂–∏–¥–∞–Ω–∏—è, –Ω–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    items = soup.find_all('div', class_='item')\n",
        "    articles = []\n",
        "\n",
        "    for item in items:\n",
        "        title_tag = item.find('span', class_='item__title')\n",
        "        link_tag = item.find('a', class_='item__link')\n",
        "\n",
        "        if not title_tag or not link_tag:\n",
        "            continue\n",
        "\n",
        "        title = ''.join(title_tag.stripped_strings)\n",
        "        link = link_tag['href']\n",
        "\n",
        "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –∏–∑ URL\n",
        "        match = re.search(r'/(\\d{2})/(\\d{2})/(\\d{4})/', link)\n",
        "        if not match:\n",
        "            continue\n",
        "        day, month, year = map(int, match.groups())\n",
        "        pub_date = datetime(year, month, day)\n",
        "\n",
        "        # –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ\n",
        "        if (now - pub_date).days > DAYS_BACK:\n",
        "            continue\n",
        "\n",
        "        # –§–∏–ª—å—Ç—Ä –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\n",
        "        if not any(kw in title.lower() for kw in KEYWORDS):\n",
        "            continue\n",
        "\n",
        "        articles.append({\n",
        "            'title': title,\n",
        "            'link': link,\n",
        "            'date': pub_date,\n",
        "        })\n",
        "\n",
        "    return articles\n",
        "\n",
        "def extract_json_from_html(html):\n",
        "    start_pattern = r'window\\.recommendationBlockList\\s*=\\s*{'\n",
        "    start_match = re.search(start_pattern, html)\n",
        "    if not start_match:\n",
        "        return None\n",
        "\n",
        "    start_index = start_match.end() - 1\n",
        "    braces = 0\n",
        "    end_index = start_index\n",
        "    for i, ch in enumerate(html[start_index:], start=start_index):\n",
        "        if ch == '{':\n",
        "            braces += 1\n",
        "        elif ch == '}':\n",
        "            braces -= 1\n",
        "            if braces == 0:\n",
        "                end_index = i\n",
        "                break\n",
        "    json_text = html[start_index:end_index+1]\n",
        "    return json_text\n",
        "\n",
        "def parse_iz(json_data):\n",
        "    articles_dict = {}\n",
        "    for key in ['even', 'odd']:\n",
        "        for item in json_data.get(key, []):\n",
        "            article_id = item.get('id') or item.get('reference') or item.get('path')\n",
        "            path = item.get('path', '').strip()\n",
        "            if not path:\n",
        "                path = item.get('reference', '').strip()\n",
        "\n",
        "            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Å—Å—ã–ª–∫–∏\n",
        "            if not path or path in ['/', IZ_BASE_URL, IZ_BASE_URL + '/']:\n",
        "                print(f\"[WARN] –ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å –±–µ–∑ –≤–∞–ª–∏–¥–Ω–æ–π —Å—Å—ã–ª–∫–∏: –ó–∞–≥–æ–ª–æ–≤–æ–∫: \\\"{item.get('title', '')}\\\"\")\n",
        "                continue\n",
        "\n",
        "            if path.startswith('http://') or path.startswith('https://'):\n",
        "                full_link = path\n",
        "            else:\n",
        "                full_link = IZ_BASE_URL + path if path.startswith('/') else IZ_BASE_URL + '/' + path\n",
        "\n",
        "            unique_key = article_id if article_id else full_link\n",
        "\n",
        "            if unique_key in articles_dict:\n",
        "                continue\n",
        "\n",
        "            articles_dict[unique_key] = {\n",
        "                'title': item.get('title', ''),\n",
        "                'link': full_link,\n",
        "                'date': None,  # –≤ –¥–∞–Ω–Ω—ã—Ö iz.ru –¥–∞—Ç—ã –Ω–µ—Ç, –º–æ–∂–Ω–æ –ø–æ—Ç–æ–º –ø—ã—Ç–∞—Ç—å—Å—è –ø–∞—Ä—Å–∏—Ç—å —Å —Å–∞–π—Ç–∞, –Ω–æ —ç—Ç–æ —Å–ª–æ–∂–Ω–µ–µ\n",
        "            }\n",
        "\n",
        "    return list(articles_dict.values())\n",
        "\n",
        "def fetch_iz_articles(driver, now):\n",
        "    driver.get(IZ_URL)\n",
        "    driver.implicitly_wait(5)\n",
        "    html = driver.page_source\n",
        "\n",
        "    json_text = extract_json_from_html(html)\n",
        "    if not json_text:\n",
        "        print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–µ–π iz.ru –≤ HTML\")\n",
        "        return []\n",
        "\n",
        "    data = json.loads(json_text)\n",
        "    articles = parse_iz(data)\n",
        "\n",
        "    filtered = []\n",
        "    for a in articles:\n",
        "        if not any(kw in a['title'].lower() for kw in KEYWORDS):\n",
        "            continue\n",
        "        filtered.append(a)\n",
        "        if len(filtered) >= MAX_NEWS_IZ:\n",
        "            break\n",
        "\n",
        "    return filtered\n",
        "\n",
        "def print_articles(articles):\n",
        "    if not articles:\n",
        "        print(\"‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.\")\n",
        "        return\n",
        "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ñ–∏–ª—å—Ç—Ä—É:\\n\")\n",
        "    for i, a in enumerate(articles, 1):\n",
        "        # –ö–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏, –µ—Å–ª–∏ –∑–∞–ø—É—Å–∫–∞–µ—à—å –≤ Jupyter/IPython\n",
        "        try:\n",
        "            display(Markdown(f\"**[{i}] {a['title']}**  \\nüîó [{a['link']}]({a['link']})\\n\"))\n",
        "        except NameError:\n",
        "            # –ï—Å–ª–∏ display –∏–ª–∏ Markdown –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã ‚Äî –ø—Ä–æ—Å—Ç–æ–π –≤—ã–≤–æ–¥\n",
        "            print(f\"[{i}] {a['title']}\\nüîó {a['link']}\\n\")\n",
        "\n",
        "def main():\n",
        "    now = datetime.now()\n",
        "\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    # –ü–∞—Ä—Å–∏–º RBC\n",
        "    rbc_articles = fetch_rbc_articles(driver, now)\n",
        "\n",
        "    # –ü–∞—Ä—Å–∏–º Iz.ru\n",
        "    iz_articles = fetch_iz_articles(driver, now)\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "    all_articles = rbc_articles + iz_articles\n",
        "\n",
        "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ ‚Äî —É iz.ru –¥–∞—Ç—ã –Ω–µ—Ç, –ø–æ—ç—Ç–æ–º—É –æ–Ω–∏ –∏–¥—É—Ç –ø–æ—Å–ª–µ RBC\n",
        "    all_articles.sort(key=lambda x: x['date'] if x['date'] else datetime.min, reverse=True)\n",
        "\n",
        "    print_articles(all_articles)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c60ee2289567095f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c60ee2289567095f",
        "outputId": "833dd912-f4bd-45c6-bca3-43fe53c830a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É TASS...\n",
            "[ERROR] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: Message: \n",
            "Stacktrace:\n",
            "#0 0x5a881382601a <unknown>\n",
            "#1 0x5a88132c5a70 <unknown>\n",
            "#2 0x5a8813317907 <unknown>\n",
            "#3 0x5a8813317b01 <unknown>\n",
            "#4 0x5a8813365d54 <unknown>\n",
            "#5 0x5a881333d40d <unknown>\n",
            "#6 0x5a881336314f <unknown>\n",
            "#7 0x5a881333d1b3 <unknown>\n",
            "#8 0x5a881330959b <unknown>\n",
            "#9 0x5a881330a971 <unknown>\n",
            "#10 0x5a88137eb1eb <unknown>\n",
            "#11 0x5a88137eef39 <unknown>\n",
            "#12 0x5a88137d22c9 <unknown>\n",
            "#13 0x5a88137efae8 <unknown>\n",
            "#14 0x5a88137b6baf <unknown>\n",
            "#15 0x5a88138130a8 <unknown>\n",
            "#16 0x5a8813813286 <unknown>\n",
            "#17 0x5a8813824ff6 <unknown>\n",
            "#18 0x7d6c49916ac3 <unknown>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import re\n",
        "\n",
        "KEYWORDS = ['—ç–∫–æ–Ω–æ–º–∏–∫–∞', '—Ä—ã–Ω–æ–∫', '–∏–Ω—Ñ–ª—è—Ü–∏—è', '–∫—É—Ä—Å', '–±–∞–Ω–∫', '—Ü–µ–Ω–∞', '—Å–Ω–∏–∂–µ–Ω–∏–µ', '—Ä–æ—Å—Ç', '—Ç—Ä–∞–º–ø', 'trump', '–¥–æ–Ω–∞–ª—å–¥']\n",
        "DAYS_BACK = 2\n",
        "URL = 'https://tass.ru/ekonomika'\n",
        "TARGET_NEWS_COUNT = 100\n",
        "SCROLL_PAUSE_SEC = 2  # –ø–∞—É–∑–∞ –ø–æ—Å–ª–µ —Å–∫—Ä–æ–ª–ª–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏\n",
        "\n",
        "def scroll_to_load(driver, target_count):\n",
        "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "    news_count = 0\n",
        "    tries = 0\n",
        "    max_tries = 20  # —á—Ç–æ–±—ã –Ω–µ –∑–∞—Ü–∏–∫–ª–∏—Ç—å—Å—è\n",
        "\n",
        "    while news_count < target_count and tries < max_tries:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(SCROLL_PAUSE_SEC)\n",
        "\n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∫–æ–ª—å–∫–æ –∫–∞—Ä—Ç–æ—á–µ–∫ —Å–µ–π—á–∞—Å –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        articles = soup.find_all('article', class_=re.compile(r'list-item|card-mini'))\n",
        "        news_count = len(articles)\n",
        "        if news_count >= target_count:\n",
        "            break\n",
        "\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            tries += 1\n",
        "        else:\n",
        "            tries = 0\n",
        "            last_height = new_height\n",
        "\n",
        "def parse_tass_news():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    driver.set_page_load_timeout(180)\n",
        "\n",
        "    print(\"[INFO] –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É TASS...\")\n",
        "    try:\n",
        "        driver.get(URL)\n",
        "        WebDriverWait(driver, 20).until(\n",
        "            EC.presence_of_element_located((By.TAG_NAME, 'article'))\n",
        "        )\n",
        "        scroll_to_load(driver, TARGET_NEWS_COUNT)\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}\")\n",
        "        driver.quit()\n",
        "        return\n",
        "\n",
        "    html = driver.page_source\n",
        "    driver.quit()\n",
        "\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    articles = soup.find_all('article', class_=re.compile(r'list-item|card-mini'))\n",
        "    print(f\"[INFO] –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ—Å–ª–µ —Å–∫—Ä–æ–ª–ª–∞: {len(articles)}\")\n",
        "\n",
        "    now = datetime.now()\n",
        "    cutoff_date = now - timedelta(days=DAYS_BACK)\n",
        "\n",
        "    filtered_articles = []\n",
        "\n",
        "    for article in articles:\n",
        "        title_tag = article.find('a', class_='list-item__title') or article.find('a', class_='card-mini__title')\n",
        "        if not title_tag:\n",
        "            continue\n",
        "        title = title_tag.get_text(strip=True)\n",
        "        link = title_tag['href']\n",
        "\n",
        "        time_tag = article.find('time')\n",
        "        if time_tag and time_tag.has_attr('datetime'):\n",
        "            pub_date_str = time_tag['datetime']\n",
        "            try:\n",
        "                pub_date = datetime.fromisoformat(pub_date_str.replace('Z', '+00:00')).replace(tzinfo=None)\n",
        "            except Exception:\n",
        "                continue\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        if pub_date < cutoff_date:\n",
        "            continue\n",
        "\n",
        "        title_lower = title.lower()\n",
        "        if not any(kw in title_lower for kw in KEYWORDS):\n",
        "            continue\n",
        "\n",
        "        if link.startswith('/'):\n",
        "            link = 'https://tass.ru' + link\n",
        "\n",
        "        filtered_articles.append({\n",
        "            'title': title,\n",
        "            'link': link,\n",
        "            'date': pub_date\n",
        "        })\n",
        "\n",
        "        if len(filtered_articles) >= TARGET_NEWS_COUNT:\n",
        "            break\n",
        "\n",
        "    if not filtered_articles:\n",
        "        print(\"‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(filtered_articles)} –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ñ–∏–ª—å—Ç—Ä—É –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {DAYS_BACK} –¥–Ω—è:\\n\")\n",
        "    for i, a in enumerate(filtered_articles, 1):\n",
        "        print(f\"[{i}] [{a['date'].strftime('%Y-%m-%d %H:%M')}] {a['title']}\\n     üîó {a['link']}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parse_tass_news()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1eda527f453a5c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "b1eda527f453a5c7",
        "outputId": "15434aac-fc31-410c-c409-1f5cc9807006"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Iz: –Ω–µ –Ω–∞–π–¥–µ–Ω window.recommendationBlockList\n",
            "‚úÖ –ù–∞–π–¥–µ–Ω–æ 2 –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ñ–∏–ª—å—Ç—Ä—É (RBC + Iz):\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": "**[1] [2025-08-13] –†–æ—Å—Å—Ç–∞—Ç —Å–æ–æ–±—â–∏–ª –æ –∑–∞–º–µ–¥–ª–µ–Ω–∏–∏ —Ä–æ—Å—Ç–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —ç–∫–æ–Ω–æ–º–∏–∫–∏**  \nüîó [https://www.rbc.ru/economics/13/08/2025/689cbb6f9a7947396a1ea3a7](https://www.rbc.ru/economics/13/08/2025/689cbb6f9a7947396a1ea3a7)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[2] [2025-08-13] –í –ò–Ω–¥–∏–∏ —Ä–∞—Å—Å–∫–∞–∑–∞–ª–∏ –æ –∑–∞—â–∏—Ç–µ –æ—Ç —Ä–∏—Å–∫–æ–≤ –¥–ª—è —Ä—ã–Ω–∫–∞ –Ω–µ—Ñ—Ç–∏ –∏–∑-–∑–∞ –ø–æ—à–ª–∏–Ω –¢—Ä–∞–º–ø–∞**  \nüîó [https://www.rbc.ru/economics/13/08/2025/689bbb429a794773357cb68a](https://www.rbc.ru/economics/13/08/2025/689bbb429a794773357cb68a)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# === –ù–ê–°–¢–†–û–ô–ö–ò ===\n",
        "KEYWORDS = ['—Ç—Ä–∞–º–ø', 'trump', '–¥–æ–Ω–∞–ª—å–¥', '–∫–∞–Ω–∞–¥', '–≥–µ—Ä–º–∞–Ω–∏', '—Å—à–∞', '–∞–≤—Å—Ç—Ä–∞–ª–∏', '—è–ø–æ–Ω–∏', '–≤–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏', '–∫–∏—Ç–∞–π', '—Ä–æ—Å—Å–∏']\n",
        "DAYS_BACK = 2\n",
        "\n",
        "# –ª–∏–º–∏—Ç—ã (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–±–∏—Ä–∞—Ç—å –ª–∏—à–Ω–µ–≥–æ)\n",
        "MAX_NEWS_RBC = 50\n",
        "MAX_NEWS_IZ  = 50\n",
        "\n",
        "# --- URLs ---\n",
        "RBC_URL = 'https://www.rbc.ru/economics/'\n",
        "IZ_URL  = 'https://iz.ru/news'\n",
        "IZ_BASE_URL = 'https://iz.ru'\n",
        "\n",
        "# Markdown-–≤—ã–≤–æ–¥ (–∫–ª–∏–∫–∞–±–µ–ª—å–Ω–æ –≤ Jupyter), –∏–Ω–∞—á–µ fallback –Ω–∞ print\n",
        "def emit_item(i, title, link, dt=None):\n",
        "    try:\n",
        "        from IPython.display import display, Markdown\n",
        "        date_str = f\"[{dt.strftime('%Y-%m-%d')}]\" if dt else \"\"\n",
        "        display(Markdown(f\"**[{i}] {date_str} {title}**  \\nüîó [{link}]({link})\\n\"))\n",
        "    except Exception:\n",
        "        date_str = f\"[{dt.strftime('%Y-%m-%d')}]\" if dt else \"\"\n",
        "        print(f\"[{i}] {date_str} {title}\\nüîó {link}\\n\")\n",
        "\n",
        "def build_driver() -> webdriver.Chrome:\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless=new\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    # —á—É—Ç—å —É—Å–∫–æ—Ä—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É\n",
        "    chrome_prefs = {\n",
        "        \"profile.managed_default_content_settings.images\": 2,  # –±–µ–∑ –∫–∞—Ä—Ç–∏–Ω–æ–∫\n",
        "        \"profile.default_content_setting_values.cookies\": 2,    # –±–µ–∑ –∫—É–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
        "    }\n",
        "    chrome_options.add_experimental_option(\"prefs\", chrome_prefs)\n",
        "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                                \"Chrome/125.0 Safari/537.36\")\n",
        "\n",
        "    # –µ—Å–ª–∏ –Ω—É–∂–µ–Ω —è–≤–Ω—ã–π –ø—É—Ç—å –∫ chromedriver:\n",
        "    # service = Service(\"/path/to/chromedriver\")\n",
        "    # return webdriver.Chrome(service=service, options=chrome_options)\n",
        "    return webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# ---------- RBC ----------\n",
        "def fetch_rbc_articles(driver, now: datetime):\n",
        "    driver.set_page_load_timeout(60)\n",
        "    driver.get(RBC_URL)\n",
        "    # –∂–¥—ë–º –ø–æ—è–≤–ª–µ–Ω–∏—è –∫–∞—Ä—Ç–æ—á–µ–∫\n",
        "    WebDriverWait(driver, 20).until(\n",
        "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.item\"))\n",
        "    )\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    items = soup.find_all('div', class_='item')\n",
        "    articles = []\n",
        "    seen = set()  # –¥–µ–¥—É–ø –ø–æ —Å—Å—ã–ª–∫–µ\n",
        "\n",
        "    date_rx = re.compile(r'/(\\d{2})/(\\d{2})/(\\d{4})/')\n",
        "    cutoff = now - timedelta(days=DAYS_BACK)\n",
        "\n",
        "    for item in items:\n",
        "        if len(articles) >= MAX_NEWS_RBC:\n",
        "            break\n",
        "\n",
        "        title_tag = item.find('span', class_='item__title')\n",
        "        link_tag  = item.find('a',   class_='item__link')\n",
        "        if not title_tag or not link_tag:\n",
        "            continue\n",
        "\n",
        "        title = ''.join(title_tag.stripped_strings)\n",
        "        link  = link_tag.get('href', '').strip()\n",
        "        if not link or link in seen:\n",
        "            continue\n",
        "\n",
        "        m = date_rx.search(link)\n",
        "        if not m:\n",
        "            continue\n",
        "        day, month, year = map(int, m.groups())\n",
        "        pub_date = datetime(year, month, day)\n",
        "\n",
        "        if pub_date < cutoff:\n",
        "            continue\n",
        "        if not any(kw in title.lower() for kw in KEYWORDS):\n",
        "            continue\n",
        "\n",
        "        seen.add(link)\n",
        "        articles.append({\n",
        "            'title': title,\n",
        "            'link':  link,\n",
        "            'date':  pub_date,\n",
        "        })\n",
        "\n",
        "    return articles\n",
        "\n",
        "# ---------- IZ ----------\n",
        "def extract_reco_json(html: str) -> str | None:\n",
        "    \"\"\"\n",
        "    –ê–∫–∫—É—Ä–∞—Ç–Ω–æ –¥–æ—Å—Ç–∞—ë–º —Ç–µ–ª–æ JSON –∏–∑ window.recommendationBlockList = {...};\n",
        "    –±–∞–ª–∞–Ω—Å–∏—Ä—É—è —Ñ–∏–≥—É—Ä–Ω—ã–µ —Å–∫–æ–±–∫–∏.\n",
        "    \"\"\"\n",
        "    start_match = re.search(r'window\\.recommendationBlockList\\s*=\\s*{', html)\n",
        "    if not start_match:\n",
        "        return None\n",
        "    start = start_match.end() - 1\n",
        "    braces = 0\n",
        "    end = start\n",
        "    for i, ch in enumerate(html[start:], start=start):\n",
        "        if ch == '{':\n",
        "            braces += 1\n",
        "        elif ch == '}':\n",
        "            braces -= 1\n",
        "            if braces == 0:\n",
        "                end = i\n",
        "                break\n",
        "    return html[start:end+1]\n",
        "\n",
        "def parse_iz_json(data: dict, allow_only_iz_domain: bool = True):\n",
        "    \"\"\"\n",
        "    –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç—å–∏ –∏–∑ –±–ª–æ–∫–æ–≤ even/odd.\n",
        "    - –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ (–ø–æ id/reference/path –∏–ª–∏ —Å—Å—ã–ª–∫–µ)\n",
        "    - –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –≤–Ω–µ—à–Ω–∏–µ –¥–æ–º–µ–Ω—ã (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ iz.ru)\n",
        "    \"\"\"\n",
        "    articles_dict = {}\n",
        "    for key in ('even', 'odd'):\n",
        "        for item in data.get(key, []) or []:\n",
        "            article_id = item.get('id') or item.get('reference') or item.get('path')\n",
        "            raw_path = (item.get('path') or item.get('reference') or '').strip()\n",
        "            if not raw_path:\n",
        "                print(f\"[WARN] –ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å –±–µ–∑ —Å—Å—ã–ª–∫–∏: ¬´{item.get('title', '')}¬ª\")\n",
        "                continue\n",
        "\n",
        "            if raw_path.startswith('http://') or raw_path.startswith('https://'):\n",
        "                full_link = raw_path\n",
        "            else:\n",
        "                full_link = IZ_BASE_URL + raw_path if raw_path.startswith('/') else IZ_BASE_URL + '/' + raw_path\n",
        "\n",
        "            if allow_only_iz_domain:\n",
        "                dom = urlparse(full_link).netloc\n",
        "                if 'iz.ru' not in dom:\n",
        "                    # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —Ä–µ–ø–æ—Å—Ç—ã, —á—Ç–æ–±—ã –Ω–µ —Ç–∞—â–∏—Ç—å sport-express/regnum –∏ —Ç.–ø.\n",
        "                    continue\n",
        "\n",
        "            unique_key = str(article_id) if article_id else full_link\n",
        "            if unique_key in articles_dict:\n",
        "                continue\n",
        "\n",
        "            articles_dict[unique_key] = {\n",
        "                'title': item.get('title', '').strip(),\n",
        "                'link':  full_link,\n",
        "                'date':  None,  # –¥–∞—Ç—ã –≤ —ç—Ç–æ–º JSON –Ω–µ—Ç\n",
        "            }\n",
        "    return list(articles_dict.values())\n",
        "\n",
        "def fetch_iz_articles(driver):\n",
        "    driver.set_page_load_timeout(60)\n",
        "    driver.get(IZ_URL)\n",
        "    WebDriverWait(driver, 20).until(\n",
        "        EC.presence_of_element_located((By.TAG_NAME, \"script\"))\n",
        "    )\n",
        "    html = driver.page_source\n",
        "    json_text = extract_reco_json(html)\n",
        "    if not json_text:\n",
        "        print(\"‚ùå Iz: –Ω–µ –Ω–∞–π–¥–µ–Ω window.recommendationBlockList\")\n",
        "        return []\n",
        "\n",
        "    data = json.loads(json_text)\n",
        "    articles = parse_iz_json(data, allow_only_iz_domain=True)\n",
        "\n",
        "    # –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\n",
        "    out = []\n",
        "    for a in articles:\n",
        "        if any(kw in a['title'].lower() for kw in KEYWORDS):\n",
        "            out.append(a)\n",
        "            if len(out) >= MAX_NEWS_IZ:\n",
        "                break\n",
        "    return out\n",
        "\n",
        "# ---------- MAIN ----------\n",
        "def main():\n",
        "    now = datetime.now()\n",
        "    driver = build_driver()\n",
        "\n",
        "    try:\n",
        "        rbc_articles = fetch_rbc_articles(driver, now)\n",
        "        iz_articles  = fetch_iz_articles(driver)\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "    # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º (—É IZ –¥–∞—Ç—ã –Ω–µ—Ç ‚Äî –æ–Ω–∏ –ø–æ–π–¥—É—Ç –ø–æ—Å–ª–µ RBC)\n",
        "    all_articles = rbc_articles + iz_articles\n",
        "    all_articles.sort(key=lambda x: x['date'] if x['date'] else datetime.min, reverse=True)\n",
        "\n",
        "    if not all_articles:\n",
        "        print(\"‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(all_articles)} –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ñ–∏–ª—å—Ç—Ä—É (RBC + Iz):\\n\")\n",
        "    for i, a in enumerate(all_articles, 1):\n",
        "        emit_item(i, a['title'], a['link'], a.get('date'))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RHRxjvdVNf3n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "RHRxjvdVNf3n",
        "outputId": "f0cab30a-ad84-4a08-b0a9-b7dd1f21b3e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ –ù–∞–π–¥–µ–Ω–æ 15 –Ω–æ–≤–æ—Å—Ç–µ–π (–∑–∞ 2 –¥–Ω.)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": "**[1] [DW] –ì–µ—Ä–º–∞–Ω–∏—è –ø—Ä–æ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä—É–µ—Ç –ø–∞–∫–µ—Ç –ø–æ–º–æ—â–∏ –£–∫—Ä–∞–∏–Ω–µ –Ω–∞ 500 –º–ª–Ω –¥–æ–ª–ª–∞—Ä–æ–≤**  \nüóì 2025-08-14 05:19  \nüîó [https://www.dw.com/ru/–≥–µ—Ä–º–∞–Ω–∏—è-–ø—Ä–æ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä—É–µ—Ç-–ø–∞–∫–µ—Ç-–ø–æ–º–æ—â–∏-—É–∫—Ä–∞–∏–Ω–µ-–Ω–∞-500-–º–ª–Ω-–¥–æ–ª–ª–∞—Ä–æ–≤/a-73630964?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–≥–µ—Ä–º–∞–Ω–∏—è-–ø—Ä–æ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä—É–µ—Ç-–ø–∞–∫–µ—Ç-–ø–æ–º–æ—â–∏-—É–∫—Ä–∞–∏–Ω–µ-–Ω–∞-500-–º–ª–Ω-–¥–æ–ª–ª–∞—Ä–æ–≤/a-73630964?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[2] [DW] –í –ë–µ—Ä–ª–∏–Ω–µ –æ—Ç—Ä–µ–∞–≥–∏—Ä–æ–≤–∞–ª–∏ –Ω–∞ –æ—Ç—á–µ—Ç –ì–æ—Å–¥–µ–ø–∞ –°–®–ê: –í –ì–µ—Ä–º–∞–Ω–∏–∏ –Ω–µ—Ç —Ü–µ–Ω–∑—É—Ä—ã**  \nüóì 2025-08-14 03:43  \nüîó [https://www.dw.com/ru/–≤-–±–µ—Ä–ª–∏–Ω–µ-–æ—Ç—Ä–µ–∞–≥–∏—Ä–æ–≤–∞–ª–∏-–Ω–∞-–æ—Ç—á–µ—Ç-–≥–æ—Å–¥–µ–ø–∞-—Å—à–∞-–≤-–≥–µ—Ä–º–∞–Ω–∏–∏-–Ω–µ—Ç-—Ü–µ–Ω–∑—É—Ä—ã/a-73630463?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–≤-–±–µ—Ä–ª–∏–Ω–µ-–æ—Ç—Ä–µ–∞–≥–∏—Ä–æ–≤–∞–ª–∏-–Ω–∞-–æ—Ç—á–µ—Ç-–≥–æ—Å–¥–µ–ø–∞-—Å—à–∞-–≤-–≥–µ—Ä–º–∞–Ω–∏–∏-–Ω–µ—Ç-—Ü–µ–Ω–∑—É—Ä—ã/a-73630463?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[3] [DW] –¢—Ä–∞–º–ø –æ–±—ä—è–≤–∏–ª –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–≤–æ–µ–π –≤—Å—Ç—Ä–µ—á–∏ —Å –ü—É—Ç–∏–Ω—ã–º –∏ –ó–µ–ª–µ–Ω—Å–∫–∏–º**  \nüóì 2025-08-14 00:44  \nüîó [https://www.dw.com/ru/—Ç—Ä–∞–º–ø-–æ–±—ä—è–≤–∏–ª-–æ-–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏-—Å–≤–æ–µ–π-–≤—Å—Ç—Ä–µ—á–∏-—Å-–ø—É—Ç–∏–Ω—ã–º-–∏-–∑–µ–ª–µ–Ω—Å–∫–∏–º/a-73629967?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/—Ç—Ä–∞–º–ø-–æ–±—ä—è–≤–∏–ª-–æ-–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏-—Å–≤–æ–µ–π-–≤—Å—Ç—Ä–µ—á–∏-—Å-–ø—É—Ç–∏–Ω—ã–º-–∏-–∑–µ–ª–µ–Ω—Å–∫–∏–º/a-73629967?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[4] [DW] \"–ú–∞—Ä—à—Ä—É—Ç –¢—Ä–∞–º–ø–∞\" –¥–ª—è –ï—Ä–µ–≤–∞–Ω–∞ –∏ –ë–∞–∫—É: –≤ –¢–µ–≥–µ—Ä–∞–Ω–µ –µ–º—É –Ω–µ –æ—á–µ–Ω—å —Ä–∞–¥—ã?**  \nüóì 2025-08-13 22:10  \nüîó [https://www.dw.com/ru/–º–∞—Ä—à—Ä—É—Ç-—Ç—Ä–∞–º–ø–∞-–¥–ª—è-–µ—Ä–µ–≤–∞–Ω–∞-–∏-–±–∞–∫—É-–≤-—Ç–µ–≥–µ—Ä–∞–Ω–µ-–µ–º—É-–Ω–µ-–æ—á–µ–Ω—å-—Ä–∞–¥—ã/a-73623655?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–º–∞—Ä—à—Ä—É—Ç-—Ç—Ä–∞–º–ø–∞-–¥–ª—è-–µ—Ä–µ–≤–∞–Ω–∞-–∏-–±–∞–∫—É-–≤-—Ç–µ–≥–µ—Ä–∞–Ω–µ-–µ–º—É-–Ω–µ-–æ—á–µ–Ω—å-—Ä–∞–¥—ã/a-73623655?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[5] [DW] –ú–∞–∫—Ä–æ–Ω: –¢—Ä–∞–º–ø –Ω–∞ –≤—Å—Ç—Ä–µ—á–µ —Å –ü—É—Ç–∏–Ω—ã–º –Ω–∞–º–µ—Ä–µ–Ω –¥–æ–±–∏—Ç—å—Å—è –ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏—è –æ–≥–Ω—è**  \nüóì 2025-08-13 21:35  \nüîó [https://www.dw.com/ru/–º–∞–∫—Ä–æ–Ω-—Ç—Ä–∞–º–ø-–Ω–∞-–≤—Å—Ç—Ä–µ—á–µ-—Å-–ø—É—Ç–∏–Ω—ã–º-–Ω–∞–º–µ—Ä–µ–Ω-–¥–æ–±–∏—Ç—å—Å—è-–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏—è-–æ–≥–Ω—è/a-73628326?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–º–∞–∫—Ä–æ–Ω-—Ç—Ä–∞–º–ø-–Ω–∞-–≤—Å—Ç—Ä–µ—á–µ-—Å-–ø—É—Ç–∏–Ω—ã–º-–Ω–∞–º–µ—Ä–µ–Ω-–¥–æ–±–∏—Ç—å—Å—è-–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏—è-–æ–≥–Ω—è/a-73628326?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[6] [DW] –ú–µ—Ä—Ü: –ù–∞ –≤—Å—Ç—Ä–µ—á–µ –¢—Ä–∞–º–ø–∞ –∏ –ü—É—Ç–∏–Ω–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–±–µ—Å–ø–µ—á–µ–Ω—ã –∏–Ω—Ç–µ—Ä–µ—Å—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ï–≤—Ä–æ–ø—ã –∏ –£–∫—Ä–∞–∏–Ω—ã**  \nüóì 2025-08-13 21:24  \nüîó [https://www.dw.com/ru/–º–µ—Ä—Ü-–Ω–∞-–≤—Å—Ç—Ä–µ—á–µ-—Ç—Ä–∞–º–ø–∞-–∏-–ø—É—Ç–∏–Ω–∞-–¥–æ–ª–∂–Ω—ã-–±—ã—Ç—å-–æ–±–µ—Å–ø–µ—á–µ–Ω—ã-–∏–Ω—Ç–µ—Ä–µ—Å—ã-–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏-–µ–≤—Ä–æ–ø—ã-–∏-—É–∫—Ä–∞–∏–Ω—ã/a-73627775?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–º–µ—Ä—Ü-–Ω–∞-–≤—Å—Ç—Ä–µ—á–µ-—Ç—Ä–∞–º–ø–∞-–∏-–ø—É—Ç–∏–Ω–∞-–¥–æ–ª–∂–Ω—ã-–±—ã—Ç—å-–æ–±–µ—Å–ø–µ—á–µ–Ω—ã-–∏–Ω—Ç–µ—Ä–µ—Å—ã-–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏-–µ–≤—Ä–æ–ø—ã-–∏-—É–∫—Ä–∞–∏–Ω—ã/a-73627775?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[7] [DW] –¢—Ä–∞–º–ø —Ä–∞—Å–∫—Ä–∏—Ç–∏–∫–æ–≤–∞–ª —Å–æ–æ–±—â–µ–Ω–∏—è –°–ú–ò –æ –µ–≥–æ –≤—Å—Ç—Ä–µ—á–µ —Å –ü—É—Ç–∏–Ω—ã–º**  \nüóì 2025-08-13 20:50  \nüîó [https://www.dw.com/ru/—Ç—Ä–∞–º–ø-—Ä–∞—Å–∫—Ä–∏—Ç–∏–∫–æ–≤–∞–ª-—Å–æ–æ–±—â–µ–Ω–∏—è-—Å–º–∏-–æ-–µ–≥–æ-–≤—Å—Ç—Ä–µ—á–µ-—Å-–ø—É—Ç–∏–Ω—ã–º/a-73627414?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/—Ç—Ä–∞–º–ø-—Ä–∞—Å–∫—Ä–∏—Ç–∏–∫–æ–≤–∞–ª-—Å–æ–æ–±—â–µ–Ω–∏—è-—Å–º–∏-–æ-–µ–≥–æ-–≤—Å—Ç—Ä–µ—á–µ-—Å-–ø—É—Ç–∏–Ω—ã–º/a-73627414?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[8] [DW] –í–æ–µ–Ω–Ω–∞—è –ø–æ–º–æ—â—å –£–∫—Ä–∞–∏–Ω–µ: –ï–≤—Ä–æ–ø–∞ –ø–µ—Ä–µ–Ω–∏–º–∞–µ—Ç –ª–∏–¥–µ—Ä—Å—Ç–≤–æ —É –°–®–ê**  \nüóì 2025-08-13 20:20  \nüîó [https://www.dw.com/ru/–≤–æ–µ–Ω–Ω–∞—è-–ø–æ–º–æ—â—å-—É–∫—Ä–∞–∏–Ω–µ-–µ–≤—Ä–æ–ø–∞-–ø–µ—Ä–µ–Ω–∏–º–∞–µ—Ç-–ª–∏–¥–µ—Ä—Å—Ç–≤–æ-—É-—Å—à–∞/a-73626115?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–≤–æ–µ–Ω–Ω–∞—è-–ø–æ–º–æ—â—å-—É–∫—Ä–∞–∏–Ω–µ-–µ–≤—Ä–æ–ø–∞-–ø–µ—Ä–µ–Ω–∏–º–∞–µ—Ç-–ª–∏–¥–µ—Ä—Å—Ç–≤–æ-—É-—Å—à–∞/a-73626115?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[9] [DW] –≠—Å—Ç–æ–Ω–∏—è –æ–±—ä—è–≤–∏–ª–∞ –ø–µ—Ä—Å–æ–Ω–æ–π –Ω–æ–Ω –≥—Ä–∞—Ç–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –¥–∏–ø–ª–æ–º–∞—Ç–∞ –∏ –≤—ã—Å–ª–∞–ª–∞ –µ–≥–æ –∏–∑ —Å—Ç—Ä–∞–Ω—ã**  \nüóì 2025-08-13 18:14  \nüîó [https://www.dw.com/ru/—ç—Å—Ç–æ–Ω–∏—è-–æ–±—ä—è–≤–∏–ª–∞-–ø–µ—Ä—Å–æ–Ω–æ–π-–Ω–æ–Ω-–≥—Ä–∞—Ç–∞-—Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ-–¥–∏–ø–ª–æ–º–∞—Ç–∞-–∏-–≤—ã—Å–ª–∞–ª–∞-–µ–≥–æ-–∏–∑-—Å—Ç—Ä–∞–Ω—ã/a-73624234?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/—ç—Å—Ç–æ–Ω–∏—è-–æ–±—ä—è–≤–∏–ª–∞-–ø–µ—Ä—Å–æ–Ω–æ–π-–Ω–æ–Ω-–≥—Ä–∞—Ç–∞-—Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ-–¥–∏–ø–ª–æ–º–∞—Ç–∞-–∏-–≤—ã—Å–ª–∞–ª–∞-–µ–≥–æ-–∏–∑-—Å—Ç—Ä–∞–Ω—ã/a-73624234?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[10] [DW] –¢—Ä–∞–º–ø –ø–æ—à–µ–ª –Ω–∞ —É—Å—Ç—É–ø–∫–∏? –ö–∞–∫–æ–π —Ç–µ–ø–µ—Ä—å –±—É–¥–µ—Ç —Ç–æ—Ä–≥–æ–≤–ª—è –°–®–ê –∏ –ö–ù–†**  \nüóì 2025-08-13 18:05  \nüîó [https://www.dw.com/ru/—Ç—Ä–∞–º–ø-–ø–æ—à–µ–ª-–Ω–∞-—É—Å—Ç—É–ø–∫–∏-–∫–∞–∫–æ–π-—Ç–µ–ø–µ—Ä—å-–±—É–¥–µ—Ç-—Ç–æ—Ä–≥–æ–≤–ª—è-—Å—à–∞-–∏-–∫–Ω—Ä/a-73622792?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/—Ç—Ä–∞–º–ø-–ø–æ—à–µ–ª-–Ω–∞-—É—Å—Ç—É–ø–∫–∏-–∫–∞–∫–æ–π-—Ç–µ–ø–µ—Ä—å-–±—É–¥–µ—Ç-—Ç–æ—Ä–≥–æ–≤–ª—è-—Å—à–∞-–∏-–∫–Ω—Ä/a-73622792?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[11] [DW] –ó–µ–ª–µ–Ω—Å–∫–∏–π –ø—Ä–∏–±—ã–ª –≤ –ë–µ—Ä–ª–∏–Ω –Ω–∞ –≤–∏–¥–µ–æ–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—é —Å –¢—Ä–∞–º–ø–æ–º**  \nüóì 2025-08-13 17:57  \nüîó [https://www.dw.com/ru/–∑–µ–ª–µ–Ω—Å–∫–∏–π-–ø—Ä–∏–±—ã–ª-–≤-–±–µ—Ä–ª–∏–Ω-–Ω–∞-–≤–∏–¥–µ–æ–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—é-—Å-—Ç—Ä–∞–º–ø–æ–º/a-73618767?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–∑–µ–ª–µ–Ω—Å–∫–∏–π-–ø—Ä–∏–±—ã–ª-–≤-–±–µ—Ä–ª–∏–Ω-–Ω–∞-–≤–∏–¥–µ–æ–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—é-—Å-—Ç—Ä–∞–º–ø–æ–º/a-73618767?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[12] [DW] –î–µ–≤–æ—á–∫–∏ –≤ –ì–µ—Ä–º–∞–Ω–∏–∏ –≤ –¥–≤–∞ —Ä–∞–∑–∞ —á–∞—â–µ —Å—Ç–∞–ª–∏ –±–æ–ª–µ—Ç—å –∞–Ω–æ—Ä–µ–∫—Å–∏–µ–π**  \nüóì 2025-08-12 19:07  \nüîó [https://www.dw.com/ru/–¥–µ–≤–æ—á–∫–∏-–≤-–≥–µ—Ä–º–∞–Ω–∏–∏-–≤-–¥–≤–∞-—Ä–∞–∑–∞-—á–∞—â–µ-—Å—Ç–∞–ª–∏-–±–æ–ª–µ—Ç—å-–∞–Ω–æ—Ä–µ–∫—Å–∏–µ–π/a-73606787?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–¥–µ–≤–æ—á–∫–∏-–≤-–≥–µ—Ä–º–∞–Ω–∏–∏-–≤-–¥–≤–∞-—Ä–∞–∑–∞-—á–∞—â–µ-—Å—Ç–∞–ª–∏-–±–æ–ª–µ—Ç—å-–∞–Ω–æ—Ä–µ–∫—Å–∏–µ–π/a-73606787?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[13] [DW] \"–ì–æ–ª–∞—è –≤—ã—Å—Ç–∞–≤–∫–∞\" –≤ –ì–µ—Ä–º–∞–Ω–∏–∏: –º—É–∑–µ–π –æ—Ç–∫—Ä–æ–µ—Ç –¥–≤–µ—Ä–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±–Ω–∞–∂–µ–Ω–Ω—ã—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π**  \nüóì 2025-08-12 14:29  \nüîó [https://www.dw.com/ru/–≥–æ–ª–∞—è-–≤—ã—Å—Ç–∞–≤–∫–∞-–≤-–≥–µ—Ä–º–∞–Ω–∏–∏-–º—É–∑–µ–π-–æ—Ç–∫—Ä–æ–µ—Ç-–¥–≤–µ—Ä–∏-—Ç–æ–ª—å–∫–æ-–¥–ª—è-–æ–±–Ω–∞–∂–µ–Ω–Ω—ã—Ö-–ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π/a-73603507?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–≥–æ–ª–∞—è-–≤—ã—Å—Ç–∞–≤–∫–∞-–≤-–≥–µ—Ä–º–∞–Ω–∏–∏-–º—É–∑–µ–π-–æ—Ç–∫—Ä–æ–µ—Ç-–¥–≤–µ—Ä–∏-—Ç–æ–ª—å–∫–æ-–¥–ª—è-–æ–±–Ω–∞–∂–µ–Ω–Ω—ã—Ö-–ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π/a-73603507?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[14] [DW] –í—ã —É–≤–æ–ª–µ–Ω—ã: –∫–∞–∫ –¢—Ä–∞–º–ø –±–æ—Ä–µ—Ç—Å—è —Å —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –°–®–ê**  \nüóì 2025-08-12 12:56  \nüîó [https://www.dw.com/ru/–≤—ã-—É–≤–æ–ª–µ–Ω—ã-–∫–∞–∫-—Ç—Ä–∞–º–ø-–±–æ—Ä–µ—Ç—Å—è-—Å-—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π-—Å—à–∞/a-73604373?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–≤—ã-—É–≤–æ–ª–µ–Ω—ã-–∫–∞–∫-—Ç—Ä–∞–º–ø-–±–æ—Ä–µ—Ç—Å—è-—Å-—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π-—Å—à–∞/a-73604373?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[15] [DW] –ê–≤—Ç–æ–ø—Ä–æ–º –≤ –†–æ—Å—Å–∏–∏ –∑–∞–≥–ª–æ—Ö? –ü—Ä–æ–¥–∞–∂–∏ —Ä—É—Ö–Ω—É–ª–∏, –∑–∞–≤–æ–¥—ã –≤ –ø—Ä–æ—Å—Ç–æ–µ**  \nüóì 2025-08-12 12:56  \nüîó [https://www.dw.com/ru/–∞–≤—Ç–æ–ø—Ä–æ–º-–≤-—Ä–æ—Å—Å–∏–∏-–∑–∞–≥–ª–æ—Ö-–ø—Ä–æ–¥–∞–∂–∏-—Ä—É—Ö–Ω—É–ª–∏-–∑–∞–≤–æ–¥—ã-–≤-–ø—Ä–æ—Å—Ç–æ–µ/a-73604599?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–∞–≤—Ç–æ–ø—Ä–æ–º-–≤-—Ä–æ—Å—Å–∏–∏-–∑–∞–≥–ª–æ—Ö-–ø—Ä–æ–¥–∞–∂–∏-—Ä—É—Ö–Ω—É–ª–∏-–∑–∞–≤–æ–¥—ã-–≤-–ø—Ä–æ—Å—Ç–æ–µ/a-73604599?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import re\n",
        "import sys\n",
        "import time\n",
        "import html\n",
        "import json\n",
        "import urllib.parse\n",
        "import xml.etree.ElementTree as ET\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ========= –ù–ê–°–¢–†–û–ô–ö–ò =========\n",
        "KEYWORDS = ['—Ç—Ä–∞–º–ø', 'trump', '–¥–æ–Ω–∞–ª—å–¥', '—Å—à–∞', '–≥–µ—Ä–º–∞–Ω–∏', '–∫–∏—Ç–∞–π', '—Ä–æ—Å—Å–∏', '—ç–∫–æ–Ω–æ–º', '–∏–Ω—Ñ–ª—è—Ü', '—Å—Ç–∞–≤–∫']\n",
        "DAYS_BACK = 2\n",
        "MAX_NEWS_PER_SITE = 50\n",
        "MAX_LINKS_FINMARKET = 100   # –æ–≥—Ä–∞–Ω–∏—á–∏–º —á–∏—Å–ª–æ –∫–∞—Ä—Ç–æ—á–µ–∫, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–∫—Ä—ã–≤–∞–µ–º —É Finmarket\n",
        "\n",
        "# DW RSS ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±—ã—Å—Ç—Ä–æ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ\n",
        "DW_RSS_URL = \"https://rss.dw.com/rdf/rss-ru-all\"\n",
        "\n",
        "# Finmarket –ª–µ–Ω—Ç–∞\n",
        "FINMARKET_LIST_URL = \"https://www.finmarket.ru/news/\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                  \"Chrome/124.0.0.0 Safari/537.36\",\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "    \"Accept-Language\": \"ru,en;q=0.9\",\n",
        "    \"Connection\": \"keep-alive\",\n",
        "}\n",
        "\n",
        "# –†—É—Å—Å–∫–∏–µ –º–µ—Å—è—Ü—ã ‚Üí –Ω–æ–º–µ—Ä\n",
        "RU_MONTHS = {\n",
        "    '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4, '–º–∞—è': 5, '–∏—é–Ω—è': 6,\n",
        "    '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8, '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12\n",
        "}\n",
        "\n",
        "def now_utc():\n",
        "    return datetime.now(timezone.utc)\n",
        "\n",
        "def within_days(dt: datetime, days: int) -> bool:\n",
        "    \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ dt –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ø–æ—Å–ª–µ–¥–Ω–∏—Ö days —Å—É—Ç–æ–∫ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–µ–≥–æ UTC.\"\"\"\n",
        "    if dt.tzinfo is None:\n",
        "        dt = dt.replace(tzinfo=timezone.utc)\n",
        "    return (now_utc() - dt) <= timedelta(days=days)\n",
        "\n",
        "def title_matches(title: str) -> bool:\n",
        "    tl = title.lower()\n",
        "    return any(kw in tl for kw in KEYWORDS)\n",
        "\n",
        "# ---------- –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç ----------\n",
        "\n",
        "def parse_iso_or_rfc2822(s: str) -> datetime | None:\n",
        "    \"\"\"–ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å ISO8601 –∏–ª–∏ RFC2822 (–∏–∑ RSS). –í–æ–∑–≤—Ä–∞—â–∞–µ–º tz-aware UTC.\"\"\"\n",
        "    s = s.strip()\n",
        "    # ISO 8601\n",
        "    try:\n",
        "        # –ü—Ä–∏–≤–æ–¥–∏–º 'Z' –∫ +00:00\n",
        "        if s.endswith('Z'):\n",
        "            dt = datetime.fromisoformat(s.replace('Z', '+00:00'))\n",
        "        else:\n",
        "            dt = datetime.fromisoformat(s)\n",
        "        return dt.astimezone(timezone.utc)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # RFC 2822 (Wed, 07 Aug 2025 18:04:00 +0000)\n",
        "    try:\n",
        "        from email.utils import parsedate_to_datetime\n",
        "        dt = parsedate_to_datetime(s)\n",
        "        if dt.tzinfo is None:\n",
        "            dt = dt.replace(tzinfo=timezone.utc)\n",
        "        return dt.astimezone(timezone.utc)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "_RU_NUMERIC_RE = re.compile(r'(?P<d>\\d{1,2})\\.(?P<m>\\d{1,2})\\.(?P<y>\\d{4})(?:\\s+(?P<h>\\d{1,2}):(?P<min>\\d{2}))?')\n",
        "_RU_TEXTUAL_RE = re.compile(\n",
        "    r'(?P<d>\\d{1,2})\\s+(?P<mon>[–ê-–Ø–∞-—è]+)\\s+(?P<y>\\d{4})'\n",
        "    r'(?:\\s*–≥\\.?|(?:\\s*–≥–æ–¥–∞)?)?(?:\\s+(?P<h>\\d{1,2}):(?P<min>\\d{2}))?',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "def parse_russian_date(text: str) -> datetime | None:\n",
        "    \"\"\"–ü–∞—Ä—Å–∏–º —Ä—É—Å—Å–∫–∏–µ –¥–∞—Ç—ã –≤–∏–¥–∞ '8 –∞–≤–≥—É—Å—Ç–∞ 2025 19:04' –∏–ª–∏ '08.08.2025 19:04'. –í–æ–∑–≤—Ä–∞—â–∞–µ–º UTC.\"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "    s = html.unescape(text.strip().lower())\n",
        "    # 1) dd.mm.yyyy [hh:mm]\n",
        "    m = _RU_NUMERIC_RE.search(s)\n",
        "    if m:\n",
        "        d, mth, y = int(m.group('d')), int(m.group('m')), int(m.group('y'))\n",
        "        hh = int(m.group('h')) if m.group('h') else 0\n",
        "        mm = int(m.group('min')) if m.group('min') else 0\n",
        "        try:\n",
        "            # —Å—á–∏—Ç–∞–µ–º –≤—Ä–µ–º—è –º–æ—Å–∫–æ–≤—Å–∫–∏–º (MSK, UTC+3) –∫–∞–∫ sane default, –∑–∞—Ç–µ–º –≤ UTC\n",
        "            dt = datetime(y, mth, d, hh, mm, tzinfo=timezone(timedelta(hours=3)))\n",
        "            return dt.astimezone(timezone.utc)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    # 2) 8 –∞–≤–≥—É—Å—Ç–∞ 2025 [19:04]\n",
        "    m = _RU_TEXTUAL_RE.search(s)\n",
        "    if m:\n",
        "        d = int(m.group('d'))\n",
        "        mon_name = m.group('mon')\n",
        "        y = int(m.group('y'))\n",
        "        mth = RU_MONTHS.get(mon_name, None)\n",
        "        if not mth:\n",
        "            return None\n",
        "        hh = int(m.group('h')) if m.group('h') else 0\n",
        "        mm = int(m.group('min')) if m.group('min') else 0\n",
        "        try:\n",
        "            dt = datetime(y, mth, d, hh, mm, tzinfo=timezone(timedelta(hours=3)))\n",
        "            return dt.astimezone(timezone.utc)\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "# ---------- DW: RSS ----------\n",
        "\n",
        "def fetch_dw_articles() -> list[dict]:\n",
        "    out = []\n",
        "    try:\n",
        "        resp = requests.get(DW_RSS_URL, headers=HEADERS, timeout=20)\n",
        "        resp.raise_for_status()\n",
        "        # RSS RDF: –ø–∞—Ä—Å–∏–º —á–µ—Ä–µ–∑ xml.etree\n",
        "        root = ET.fromstring(resp.content)\n",
        "        # –ò—â–µ–º items –±–µ–∑ –∑–∞–º–æ—Ä–æ—á–µ–∫ —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º–∏ –∏–º—ë–Ω: –ø–µ—Ä–µ–±–æ—Ä–æ–º\n",
        "        items = [el for el in root.iter() if el.tag.lower().endswith('item')]\n",
        "        for it in items:\n",
        "            title = None\n",
        "            link = None\n",
        "            date_text = None\n",
        "            for child in it:\n",
        "                tag = child.tag.lower()\n",
        "                if tag.endswith('title'):\n",
        "                    title = (child.text or '').strip()\n",
        "                elif tag.endswith('link'):\n",
        "                    link = (child.text or '').strip()\n",
        "                elif tag.endswith('date') or tag.endswith('pubdate'):\n",
        "                    date_text = (child.text or '').strip()\n",
        "            if not title or not link:\n",
        "                continue\n",
        "            if not title_matches(title):\n",
        "                continue\n",
        "            pub_dt = parse_iso_or_rfc2822(date_text) if date_text else None\n",
        "            # –ï—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ—Ç ‚Äî –≤–æ–∑—å–º—ë–º –∫–∞–∫ —Å—Ç–∞—Ä—É—é (–≤—ã–ø–∞–¥–µ—Ç —Ñ–∏–ª—å—Ç—Ä–æ–º)\n",
        "            if not pub_dt or not within_days(pub_dt, DAYS_BACK):\n",
        "                continue\n",
        "            out.append({\n",
        "                'source': 'DW',\n",
        "                'title': title,\n",
        "                'link': link,\n",
        "                'date': pub_dt\n",
        "            })\n",
        "            if len(out) >= MAX_NEWS_PER_SITE:\n",
        "                break\n",
        "    except Exception as e:\n",
        "        print(f\"[DW][ERROR] {e}\")\n",
        "    return out\n",
        "\n",
        "# ---------- Finmarket: HTML + –∫–∞—Ä—Ç–æ—á–∫–∏ ----------\n",
        "\n",
        "_FINMARKET_LINK_RE = re.compile(r'href=[\"\\'](/news/\\d{6,})[\"\\']', re.IGNORECASE)\n",
        "\n",
        "def _request(url: str, timeout: int = 20) -> requests.Response | None:\n",
        "    try:\n",
        "        r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "        # –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—Ç–¥–∞—é—Ç cp1251 ‚Äî —É–≤–∞–∂–∏–º —è–≤–Ω—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É/–∞–ø–ø–∞—Ä–µ–Ω—Ç–Ω—É—é\n",
        "        enc = r.encoding or r.apparent_encoding or 'utf-8'\n",
        "        r.encoding = enc\n",
        "        if r.status_code == 200:\n",
        "            return r\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _extract_finmarket_article_date(html_text: str) -> datetime | None:\n",
        "    \"\"\"\n",
        "    –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–æ–≤–æ—Å—Ç–∏ Finmarket –∏—â–µ–º –¥–∞—Ç—É –≤ —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–º –≤–∏–¥–µ.\n",
        "    –ü—Ä–∏–º–µ—Ä—ã: '08 –∞–≤–≥—É—Å—Ç–∞ 2025 –≥–æ–¥–∞ 19:04', '08.08.2025 19:04'\n",
        "    \"\"\"\n",
        "    # —Å—Ä–µ–∂–µ–º –ª–∏—à–Ω–µ–µ –∏ –∏—â–µ–º –ø–æ –≤—Å–µ–º—É —Ç–µ–∫—Å—Ç—É\n",
        "    txt = ' '.join(BeautifulSoup(html_text, 'html.parser').stripped_strings)\n",
        "    # —Å–Ω–∞—á–∞–ª–∞ numeric\n",
        "    dt = parse_russian_date(txt)\n",
        "    return dt\n",
        "\n",
        "def _extract_finmarket_article_title(html_text: str) -> str:\n",
        "    soup = BeautifulSoup(html_text, 'html.parser')\n",
        "    # –ø—Ä–æ–±—É–µ–º h1\n",
        "    h1 = soup.find('h1')\n",
        "    if h1 and h1.get_text(strip=True):\n",
        "        return h1.get_text(strip=True)\n",
        "    # fallback: title\n",
        "    if soup.title and soup.title.text:\n",
        "        return soup.title.text.strip()\n",
        "    return \"\"\n",
        "\n",
        "def fetch_finmarket_articles() -> list[dict]:\n",
        "    out = []\n",
        "    seen = set()\n",
        "    resp = _request(FINMARKET_LIST_URL)\n",
        "    if not resp:\n",
        "        print(\"[Finmarket][ERROR] –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–µ–Ω—Ç—É.\")\n",
        "        return out\n",
        "\n",
        "    html_text = resp.text\n",
        "    # —Å–æ–±–∏—Ä–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ –ª–µ–Ω—Ç—ã (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –ø–æ—è–≤–ª–µ–Ω–∏—è)\n",
        "    candidates = []\n",
        "    for m in _FINMARKET_LINK_RE.finditer(html_text):\n",
        "        path = m.group(1)\n",
        "        if path not in seen:\n",
        "            seen.add(path)\n",
        "            candidates.append(urllib.parse.urljoin(FINMARKET_LIST_URL, path))\n",
        "        if len(candidates) >= MAX_LINKS_FINMARKET:\n",
        "            break\n",
        "\n",
        "    for url in candidates:\n",
        "        art_resp = _request(url, timeout=20)\n",
        "        if not art_resp:\n",
        "            continue\n",
        "        art_html = art_resp.text\n",
        "        title = _extract_finmarket_article_title(art_html)\n",
        "        if not title or not title_matches(title):\n",
        "            continue\n",
        "        pub_dt = _extract_finmarket_article_date(art_html)\n",
        "        if not pub_dt or not within_days(pub_dt, DAYS_BACK):\n",
        "            continue\n",
        "        out.append({\n",
        "            'source': 'Finmarket',\n",
        "            'title': title,\n",
        "            'link': url,\n",
        "            'date': pub_dt\n",
        "        })\n",
        "        if len(out) >= MAX_NEWS_PER_SITE:\n",
        "            break\n",
        "    return out\n",
        "\n",
        "# ---------- –í—ã–≤–æ–¥ ----------\n",
        "\n",
        "def print_articles(articles: list[dict]):\n",
        "    if not articles:\n",
        "        print(\"‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.\")\n",
        "        return\n",
        "\n",
        "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –ø–æ —É–±—ã–≤–∞–Ω–∏—é\n",
        "    articles.sort(key=lambda x: x['date'] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\n",
        "\n",
        "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π (–∑–∞ {DAYS_BACK} –¥–Ω.)\\n\")\n",
        "    # –ö–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ Jupyter\n",
        "    try:\n",
        "        from IPython.display import display, Markdown\n",
        "        for i, a in enumerate(articles, 1):\n",
        "            dt_local = a['date'].astimezone(timezone(timedelta(hours=6)))  # Asia/Bishkek UTC+6\n",
        "            display(Markdown(\n",
        "                f\"**[{i}] [{a['source']}] {a['title']}**  \\n\"\n",
        "                f\"üóì {dt_local.strftime('%Y-%m-%d %H:%M')}  \\n\"\n",
        "                f\"üîó [{a['link']}]({a['link']})\\n\"\n",
        "            ))\n",
        "    except Exception:\n",
        "        for i, a in enumerate(articles, 1):\n",
        "            dt_local = a['date'].astimezone(timezone(timedelta(hours=6)))\n",
        "            print(f\"[{i}] [{a['source']}] {a['title']}\\n\"\n",
        "                  f\"üóì {dt_local.strftime('%Y-%m-%d %H:%M')}\\n\"\n",
        "                  f\"üîó {a['link']}\\n\")\n",
        "\n",
        "def main():\n",
        "    # 1) DW —á–µ—Ä–µ–∑ RSS (–æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ)\n",
        "    dw = fetch_dw_articles()\n",
        "    # 2) Finmarket —á–µ—Ä–µ–∑ HTML + –∫–∞—Ä—Ç–æ—á–∫–∏ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ MAX_LINKS_FINMARKET)\n",
        "    fm = fetch_finmarket_articles()\n",
        "    all_items = dw + fm\n",
        "    print_articles(all_items)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7OuDeannORvp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7OuDeannORvp",
        "outputId": "86a53c88-fd80-438f-ea16-bcf8446cf0bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DW] items in RSS: 75\n",
            "[Finmarket] –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: 10\n",
            "[Finmarket] –∏—Ç–æ–≥: 0 –ø—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä | –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞: 0, –±–µ–∑ KEYWORDS: 10, –±–µ–∑ –¥–∞—Ç—ã: 0, —Å—Ç–∞—Ä—à–µ 2 –¥–Ω.: 0\n",
            "‚úÖ –ù–∞–π–¥–µ–Ω–æ 15 –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ 2 –¥–Ω.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": "**[1] [DW] –ì–µ—Ä–º–∞–Ω–∏—è –ø—Ä–æ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä—É–µ—Ç –ø–∞–∫–µ—Ç –ø–æ–º–æ—â–∏ –£–∫—Ä–∞–∏–Ω–µ –Ω–∞ 500 –º–ª–Ω –¥–æ–ª–ª–∞—Ä–æ–≤**  \nüóì 2025-08-14 05:19  \nüîó [https://www.dw.com/ru/–≥–µ—Ä–º–∞–Ω–∏—è-–ø—Ä–æ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä—É–µ—Ç-–ø–∞–∫–µ—Ç-–ø–æ–º–æ—â–∏-—É–∫—Ä–∞–∏–Ω–µ-–Ω–∞-500-–º–ª–Ω-–¥–æ–ª–ª–∞—Ä–æ–≤/a-73630964?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–≥–µ—Ä–º–∞–Ω–∏—è-–ø—Ä–æ—Ñ–∏–Ω–∞–Ω—Å–∏—Ä—É–µ—Ç-–ø–∞–∫–µ—Ç-–ø–æ–º–æ—â–∏-—É–∫—Ä–∞–∏–Ω–µ-–Ω–∞-500-–º–ª–Ω-–¥–æ–ª–ª–∞—Ä–æ–≤/a-73630964?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[2] [DW] –í –ë–µ—Ä–ª–∏–Ω–µ –æ—Ç—Ä–µ–∞–≥–∏—Ä–æ–≤–∞–ª–∏ –Ω–∞ –æ—Ç—á–µ—Ç –ì–æ—Å–¥–µ–ø–∞ –°–®–ê: –í –ì–µ—Ä–º–∞–Ω–∏–∏ –Ω–µ—Ç —Ü–µ–Ω–∑—É—Ä—ã**  \nüóì 2025-08-14 03:43  \nüîó [https://www.dw.com/ru/–≤-–±–µ—Ä–ª–∏–Ω–µ-–æ—Ç—Ä–µ–∞–≥–∏—Ä–æ–≤–∞–ª–∏-–Ω–∞-–æ—Ç—á–µ—Ç-–≥–æ—Å–¥–µ–ø–∞-—Å—à–∞-–≤-–≥–µ—Ä–º–∞–Ω–∏–∏-–Ω–µ—Ç-—Ü–µ–Ω–∑—É—Ä—ã/a-73630463?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–≤-–±–µ—Ä–ª–∏–Ω–µ-–æ—Ç—Ä–µ–∞–≥–∏—Ä–æ–≤–∞–ª–∏-–Ω–∞-–æ—Ç—á–µ—Ç-–≥–æ—Å–¥–µ–ø–∞-—Å—à–∞-–≤-–≥–µ—Ä–º–∞–Ω–∏–∏-–Ω–µ—Ç-—Ü–µ–Ω–∑—É—Ä—ã/a-73630463?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[3] [DW] –¢—Ä–∞–º–ø –æ–±—ä—è–≤–∏–ª –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–≤–æ–µ–π –≤—Å—Ç—Ä–µ—á–∏ —Å –ü—É—Ç–∏–Ω—ã–º –∏ –ó–µ–ª–µ–Ω—Å–∫–∏–º**  \nüóì 2025-08-14 00:44  \nüîó [https://www.dw.com/ru/—Ç—Ä–∞–º–ø-–æ–±—ä—è–≤–∏–ª-–æ-–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏-—Å–≤–æ–µ–π-–≤—Å—Ç—Ä–µ—á–∏-—Å-–ø—É—Ç–∏–Ω—ã–º-–∏-–∑–µ–ª–µ–Ω—Å–∫–∏–º/a-73629967?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/—Ç—Ä–∞–º–ø-–æ–±—ä—è–≤–∏–ª-–æ-–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏-—Å–≤–æ–µ–π-–≤—Å—Ç—Ä–µ—á–∏-—Å-–ø—É—Ç–∏–Ω—ã–º-–∏-–∑–µ–ª–µ–Ω—Å–∫–∏–º/a-73629967?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[4] [DW] \"–ú–∞—Ä—à—Ä—É—Ç –¢—Ä–∞–º–ø–∞\" –¥–ª—è –ï—Ä–µ–≤–∞–Ω–∞ –∏ –ë–∞–∫—É: –≤ –¢–µ–≥–µ—Ä–∞–Ω–µ –µ–º—É –Ω–µ –æ—á–µ–Ω—å —Ä–∞–¥—ã?**  \nüóì 2025-08-13 22:10  \nüîó [https://www.dw.com/ru/–º–∞—Ä—à—Ä—É—Ç-—Ç—Ä–∞–º–ø–∞-–¥–ª—è-–µ—Ä–µ–≤–∞–Ω–∞-–∏-–±–∞–∫—É-–≤-—Ç–µ–≥–µ—Ä–∞–Ω–µ-–µ–º—É-–Ω–µ-–æ—á–µ–Ω—å-—Ä–∞–¥—ã/a-73623655?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–º–∞—Ä—à—Ä—É—Ç-—Ç—Ä–∞–º–ø–∞-–¥–ª—è-–µ—Ä–µ–≤–∞–Ω–∞-–∏-–±–∞–∫—É-–≤-—Ç–µ–≥–µ—Ä–∞–Ω–µ-–µ–º—É-–Ω–µ-–æ—á–µ–Ω—å-—Ä–∞–¥—ã/a-73623655?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[5] [DW] –ú–∞–∫—Ä–æ–Ω: –¢—Ä–∞–º–ø –Ω–∞ –≤—Å—Ç—Ä–µ—á–µ —Å –ü—É—Ç–∏–Ω—ã–º –Ω–∞–º–µ—Ä–µ–Ω –¥–æ–±–∏—Ç—å—Å—è –ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏—è –æ–≥–Ω—è**  \nüóì 2025-08-13 21:35  \nüîó [https://www.dw.com/ru/–º–∞–∫—Ä–æ–Ω-—Ç—Ä–∞–º–ø-–Ω–∞-–≤—Å—Ç—Ä–µ—á–µ-—Å-–ø—É—Ç–∏–Ω—ã–º-–Ω–∞–º–µ—Ä–µ–Ω-–¥–æ–±–∏—Ç—å—Å—è-–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏—è-–æ–≥–Ω—è/a-73628326?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–º–∞–∫—Ä–æ–Ω-—Ç—Ä–∞–º–ø-–Ω–∞-–≤—Å—Ç—Ä–µ—á–µ-—Å-–ø—É—Ç–∏–Ω—ã–º-–Ω–∞–º–µ—Ä–µ–Ω-–¥–æ–±–∏—Ç—å—Å—è-–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏—è-–æ–≥–Ω—è/a-73628326?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[6] [DW] –ú–µ—Ä—Ü: –ù–∞ –≤—Å—Ç—Ä–µ—á–µ –¢—Ä–∞–º–ø–∞ –∏ –ü—É—Ç–∏–Ω–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–±–µ—Å–ø–µ—á–µ–Ω—ã –∏–Ω—Ç–µ—Ä–µ—Å—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ï–≤—Ä–æ–ø—ã –∏ –£–∫—Ä–∞–∏–Ω—ã**  \nüóì 2025-08-13 21:24  \nüîó [https://www.dw.com/ru/–º–µ—Ä—Ü-–Ω–∞-–≤—Å—Ç—Ä–µ—á–µ-—Ç—Ä–∞–º–ø–∞-–∏-–ø—É—Ç–∏–Ω–∞-–¥–æ–ª–∂–Ω—ã-–±—ã—Ç—å-–æ–±–µ—Å–ø–µ—á–µ–Ω—ã-–∏–Ω—Ç–µ—Ä–µ—Å—ã-–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏-–µ–≤—Ä–æ–ø—ã-–∏-—É–∫—Ä–∞–∏–Ω—ã/a-73627775?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–º–µ—Ä—Ü-–Ω–∞-–≤—Å—Ç—Ä–µ—á–µ-—Ç—Ä–∞–º–ø–∞-–∏-–ø—É—Ç–∏–Ω–∞-–¥–æ–ª–∂–Ω—ã-–±—ã—Ç—å-–æ–±–µ—Å–ø–µ—á–µ–Ω—ã-–∏–Ω—Ç–µ—Ä–µ—Å—ã-–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏-–µ–≤—Ä–æ–ø—ã-–∏-—É–∫—Ä–∞–∏–Ω—ã/a-73627775?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[7] [DW] –¢—Ä–∞–º–ø —Ä–∞—Å–∫—Ä–∏—Ç–∏–∫–æ–≤–∞–ª —Å–æ–æ–±—â–µ–Ω–∏—è –°–ú–ò –æ –µ–≥–æ –≤—Å—Ç—Ä–µ—á–µ —Å –ü—É—Ç–∏–Ω—ã–º**  \nüóì 2025-08-13 20:50  \nüîó [https://www.dw.com/ru/—Ç—Ä–∞–º–ø-—Ä–∞—Å–∫—Ä–∏—Ç–∏–∫–æ–≤–∞–ª-—Å–æ–æ–±—â–µ–Ω–∏—è-—Å–º–∏-–æ-–µ–≥–æ-–≤—Å—Ç—Ä–µ—á–µ-—Å-–ø—É—Ç–∏–Ω—ã–º/a-73627414?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/—Ç—Ä–∞–º–ø-—Ä–∞—Å–∫—Ä–∏—Ç–∏–∫–æ–≤–∞–ª-—Å–æ–æ–±—â–µ–Ω–∏—è-—Å–º–∏-–æ-–µ–≥–æ-–≤—Å—Ç—Ä–µ—á–µ-—Å-–ø—É—Ç–∏–Ω—ã–º/a-73627414?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[8] [DW] –í–æ–µ–Ω–Ω–∞—è –ø–æ–º–æ—â—å –£–∫—Ä–∞–∏–Ω–µ: –ï–≤—Ä–æ–ø–∞ –ø–µ—Ä–µ–Ω–∏–º–∞–µ—Ç –ª–∏–¥–µ—Ä—Å—Ç–≤–æ —É –°–®–ê**  \nüóì 2025-08-13 20:20  \nüîó [https://www.dw.com/ru/–≤–æ–µ–Ω–Ω–∞—è-–ø–æ–º–æ—â—å-—É–∫—Ä–∞–∏–Ω–µ-–µ–≤—Ä–æ–ø–∞-–ø–µ—Ä–µ–Ω–∏–º–∞–µ—Ç-–ª–∏–¥–µ—Ä—Å—Ç–≤–æ-—É-—Å—à–∞/a-73626115?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–≤–æ–µ–Ω–Ω–∞—è-–ø–æ–º–æ—â—å-—É–∫—Ä–∞–∏–Ω–µ-–µ–≤—Ä–æ–ø–∞-–ø–µ—Ä–µ–Ω–∏–º–∞–µ—Ç-–ª–∏–¥–µ—Ä—Å—Ç–≤–æ-—É-—Å—à–∞/a-73626115?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[9] [DW] –≠—Å—Ç–æ–Ω–∏—è –æ–±—ä—è–≤–∏–ª–∞ –ø–µ—Ä—Å–æ–Ω–æ–π –Ω–æ–Ω –≥—Ä–∞—Ç–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –¥–∏–ø–ª–æ–º–∞—Ç–∞ –∏ –≤—ã—Å–ª–∞–ª–∞ –µ–≥–æ –∏–∑ —Å—Ç—Ä–∞–Ω—ã**  \nüóì 2025-08-13 18:14  \nüîó [https://www.dw.com/ru/—ç—Å—Ç–æ–Ω–∏—è-–æ–±—ä—è–≤–∏–ª–∞-–ø–µ—Ä—Å–æ–Ω–æ–π-–Ω–æ–Ω-–≥—Ä–∞—Ç–∞-—Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ-–¥–∏–ø–ª–æ–º–∞—Ç–∞-–∏-–≤—ã—Å–ª–∞–ª–∞-–µ–≥–æ-–∏–∑-—Å—Ç—Ä–∞–Ω—ã/a-73624234?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/—ç—Å—Ç–æ–Ω–∏—è-–æ–±—ä—è–≤–∏–ª–∞-–ø–µ—Ä—Å–æ–Ω–æ–π-–Ω–æ–Ω-–≥—Ä–∞—Ç–∞-—Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ-–¥–∏–ø–ª–æ–º–∞—Ç–∞-–∏-–≤—ã—Å–ª–∞–ª–∞-–µ–≥–æ-–∏–∑-—Å—Ç—Ä–∞–Ω—ã/a-73624234?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[10] [DW] –¢—Ä–∞–º–ø –ø–æ—à–µ–ª –Ω–∞ —É—Å—Ç—É–ø–∫–∏? –ö–∞–∫–æ–π —Ç–µ–ø–µ—Ä—å –±—É–¥–µ—Ç —Ç–æ—Ä–≥–æ–≤–ª—è –°–®–ê –∏ –ö–ù–†**  \nüóì 2025-08-13 18:05  \nüîó [https://www.dw.com/ru/—Ç—Ä–∞–º–ø-–ø–æ—à–µ–ª-–Ω–∞-—É—Å—Ç—É–ø–∫–∏-–∫–∞–∫–æ–π-—Ç–µ–ø–µ—Ä—å-–±—É–¥–µ—Ç-—Ç–æ—Ä–≥–æ–≤–ª—è-—Å—à–∞-–∏-–∫–Ω—Ä/a-73622792?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/—Ç—Ä–∞–º–ø-–ø–æ—à–µ–ª-–Ω–∞-—É—Å—Ç—É–ø–∫–∏-–∫–∞–∫–æ–π-—Ç–µ–ø–µ—Ä—å-–±—É–¥–µ—Ç-—Ç–æ—Ä–≥–æ–≤–ª—è-—Å—à–∞-–∏-–∫–Ω—Ä/a-73622792?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[11] [DW] –ó–µ–ª–µ–Ω—Å–∫–∏–π –ø—Ä–∏–±—ã–ª –≤ –ë–µ—Ä–ª–∏–Ω –Ω–∞ –≤–∏–¥–µ–æ–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—é —Å –¢—Ä–∞–º–ø–æ–º**  \nüóì 2025-08-13 17:57  \nüîó [https://www.dw.com/ru/–∑–µ–ª–µ–Ω—Å–∫–∏–π-–ø—Ä–∏–±—ã–ª-–≤-–±–µ—Ä–ª–∏–Ω-–Ω–∞-–≤–∏–¥–µ–æ–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—é-—Å-—Ç—Ä–∞–º–ø–æ–º/a-73618767?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–∑–µ–ª–µ–Ω—Å–∫–∏–π-–ø—Ä–∏–±—ã–ª-–≤-–±–µ—Ä–ª–∏–Ω-–Ω–∞-–≤–∏–¥–µ–æ–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—é-—Å-—Ç—Ä–∞–º–ø–æ–º/a-73618767?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[12] [DW] –î–µ–≤–æ—á–∫–∏ –≤ –ì–µ—Ä–º–∞–Ω–∏–∏ –≤ –¥–≤–∞ —Ä–∞–∑–∞ —á–∞—â–µ —Å—Ç–∞–ª–∏ –±–æ–ª–µ—Ç—å –∞–Ω–æ—Ä–µ–∫—Å–∏–µ–π**  \nüóì 2025-08-12 19:07  \nüîó [https://www.dw.com/ru/–¥–µ–≤–æ—á–∫–∏-–≤-–≥–µ—Ä–º–∞–Ω–∏–∏-–≤-–¥–≤–∞-—Ä–∞–∑–∞-—á–∞—â–µ-—Å—Ç–∞–ª–∏-–±–æ–ª–µ—Ç—å-–∞–Ω–æ—Ä–µ–∫—Å–∏–µ–π/a-73606787?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–¥–µ–≤–æ—á–∫–∏-–≤-–≥–µ—Ä–º–∞–Ω–∏–∏-–≤-–¥–≤–∞-—Ä–∞–∑–∞-—á–∞—â–µ-—Å—Ç–∞–ª–∏-–±–æ–ª–µ—Ç—å-–∞–Ω–æ—Ä–µ–∫—Å–∏–µ–π/a-73606787?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[13] [DW] \"–ì–æ–ª–∞—è –≤—ã—Å—Ç–∞–≤–∫–∞\" –≤ –ì–µ—Ä–º–∞–Ω–∏–∏: –º—É–∑–µ–π –æ—Ç–∫—Ä–æ–µ—Ç –¥–≤–µ—Ä–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±–Ω–∞–∂–µ–Ω–Ω—ã—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π**  \nüóì 2025-08-12 14:29  \nüîó [https://www.dw.com/ru/–≥–æ–ª–∞—è-–≤—ã—Å—Ç–∞–≤–∫–∞-–≤-–≥–µ—Ä–º–∞–Ω–∏–∏-–º—É–∑–µ–π-–æ—Ç–∫—Ä–æ–µ—Ç-–¥–≤–µ—Ä–∏-—Ç–æ–ª—å–∫–æ-–¥–ª—è-–æ–±–Ω–∞–∂–µ–Ω–Ω—ã—Ö-–ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π/a-73603507?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–≥–æ–ª–∞—è-–≤—ã—Å—Ç–∞–≤–∫–∞-–≤-–≥–µ—Ä–º–∞–Ω–∏–∏-–º—É–∑–µ–π-–æ—Ç–∫—Ä–æ–µ—Ç-–¥–≤–µ—Ä–∏-—Ç–æ–ª—å–∫–æ-–¥–ª—è-–æ–±–Ω–∞–∂–µ–Ω–Ω—ã—Ö-–ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π/a-73603507?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[14] [DW] –í—ã —É–≤–æ–ª–µ–Ω—ã: –∫–∞–∫ –¢—Ä–∞–º–ø –±–æ—Ä–µ—Ç—Å—è —Å —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –°–®–ê**  \nüóì 2025-08-12 12:56  \nüîó [https://www.dw.com/ru/–≤—ã-—É–≤–æ–ª–µ–Ω—ã-–∫–∞–∫-—Ç—Ä–∞–º–ø-–±–æ—Ä–µ—Ç—Å—è-—Å-—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π-—Å—à–∞/a-73604373?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–≤—ã-—É–≤–æ–ª–µ–Ω—ã-–∫–∞–∫-—Ç—Ä–∞–º–ø-–±–æ—Ä–µ—Ç—Å—è-—Å-—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π-—Å—à–∞/a-73604373?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[15] [DW] –ê–≤—Ç–æ–ø—Ä–æ–º –≤ –†–æ—Å—Å–∏–∏ –∑–∞–≥–ª–æ—Ö? –ü—Ä–æ–¥–∞–∂–∏ —Ä—É—Ö–Ω—É–ª–∏, –∑–∞–≤–æ–¥—ã –≤ –ø—Ä–æ—Å—Ç–æ–µ**  \nüóì 2025-08-12 12:56  \nüîó [https://www.dw.com/ru/–∞–≤—Ç–æ–ø—Ä–æ–º-–≤-—Ä–æ—Å—Å–∏–∏-–∑–∞–≥–ª–æ—Ö-–ø—Ä–æ–¥–∞–∂–∏-—Ä—É—Ö–Ω—É–ª–∏-–∑–∞–≤–æ–¥—ã-–≤-–ø—Ä–æ—Å—Ç–æ–µ/a-73604599?maca=rus-rss-ru-all-1126-rdf](https://www.dw.com/ru/–∞–≤—Ç–æ–ø—Ä–æ–º-–≤-—Ä–æ—Å—Å–∏–∏-–∑–∞–≥–ª–æ—Ö-–ø—Ä–æ–¥–∞–∂–∏-—Ä—É—Ö–Ω—É–ª–∏-–∑–∞–≤–æ–¥—ã-–≤-–ø—Ä–æ—Å—Ç–æ–µ/a-73604599?maca=rus-rss-ru-all-1126-rdf)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import re\n",
        "import html\n",
        "import time\n",
        "import urllib.parse\n",
        "import xml.etree.ElementTree as ET\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ================= –ù–ê–°–¢–†–û–ô–ö–ò =================\n",
        "KEYWORDS = ['—Ç—Ä–∞–º–ø', 'trump', '–¥–æ–Ω–∞–ª—å–¥', '—Å—à–∞', '–≥–µ—Ä–º–∞–Ω–∏', '–∫–∏—Ç–∞–π', '—Ä–æ—Å—Å–∏', '—ç–∫–æ–Ω–æ–º', '–∏–Ω—Ñ–ª—è—Ü', '—Å—Ç–∞–≤–∫']\n",
        "DAYS_BACK = 2\n",
        "MAX_NEWS_PER_SITE = 50\n",
        "MAX_LINKS_FINMARKET = 150  # —Å–∫–æ–ª—å–∫–æ –∫–∞—Ä—Ç–æ—á–µ–∫ Finmarket –æ—Ç–∫—Ä—ã–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º\n",
        "DEBUG = True               # –≤–∫–ª—é—á–∏, —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å —Å—á—ë—Ç—á–∏–∫–∏/–ø—Ä–∏—á–∏–Ω—ã –æ—Ç–±—Ä–∞–∫–æ–≤–∫–∏\n",
        "\n",
        "# DW RSS\n",
        "DW_RSS_URL = \"https://rss.dw.com/rdf/rss-ru-all\"\n",
        "\n",
        "# Finmarket\n",
        "FINMARKET_LIST_URL = \"https://www.finmarket.ru/news/\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                   \"Chrome/125.0 Safari/537.36\"),\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "    \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
        "    \"Connection\": \"keep-alive\",\n",
        "}\n",
        "\n",
        "RU_MONTHS = {\n",
        "    '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4, '–º–∞—è': 5, '–∏—é–Ω—è': 6,\n",
        "    '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8, '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12\n",
        "}\n",
        "\n",
        "ISO_REGEX = re.compile(r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}(?::\\d{2})?(?:Z|[+\\-]\\d{2}:\\d{2})')\n",
        "FM_LINK_RE = re.compile(\n",
        "    r'href=[\"\\'](?:https?://[^\"\\']+)?(/news/\\d+/?)(?:\\?[^\"\\']*)?[\"\\']',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "RU_NUMERIC_RE = re.compile(r'(?P<d>\\d{1,2})\\.(?P<m>\\d{1,2})\\.(?P<y>\\d{4})(?:\\s+(?P<h>\\d{1,2}):(?P<min>\\d{2}))?')\n",
        "RU_TEXTUAL_RE = re.compile(\n",
        "    r'(?P<d>\\d{1,2})\\s+(?P<mon>[–ê-–Ø–∞-—è]+)\\s+(?P<y>\\d{4})'\n",
        "    r'(?:\\s*–≥\\.?|(?:\\s*–≥–æ–¥–∞)?)?(?:\\s+(?P<h>\\d{1,2}):(?P<min>\\d{2}))?',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "def now_utc():\n",
        "    return datetime.now(timezone.utc)\n",
        "\n",
        "def within_days(dt: datetime, days: int) -> bool:\n",
        "    if dt.tzinfo is None:\n",
        "        dt = dt.replace(tzinfo=timezone.utc)\n",
        "    return (now_utc() - dt) <= timedelta(days=days)\n",
        "\n",
        "def title_matches(title: str) -> bool:\n",
        "    tl = title.lower()\n",
        "    return any(kw in tl for kw in KEYWORDS)\n",
        "\n",
        "def parse_iso_or_rfc2822(s: str) -> datetime | None:\n",
        "    if not s:\n",
        "        return None\n",
        "    s = s.strip()\n",
        "    try:\n",
        "        if s.endswith('Z'):\n",
        "            dt = datetime.fromisoformat(s.replace('Z', '+00:00'))\n",
        "        else:\n",
        "            dt = datetime.fromisoformat(s)\n",
        "        return dt.astimezone(timezone.utc)\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        from email.utils import parsedate_to_datetime\n",
        "        dt = parsedate_to_datetime(s)\n",
        "        if dt.tzinfo is None:\n",
        "            dt = dt.replace(tzinfo=timezone.utc)\n",
        "        return dt.astimezone(timezone.utc)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def parse_russian_date(text: str) -> datetime | None:\n",
        "    if not text:\n",
        "        return None\n",
        "    s = html.unescape(text.strip().lower())\n",
        "\n",
        "    m = RU_NUMERIC_RE.search(s)\n",
        "    if m:\n",
        "        d, mth, y = int(m.group('d')), int(m.group('m')), int(m.group('y'))\n",
        "        hh = int(m.group('h') or 0)\n",
        "        mm = int(m.group('min') or 0)\n",
        "        try:\n",
        "            dt = datetime(y, mth, d, hh, mm, tzinfo=timezone(timedelta(hours=3)))\n",
        "            return dt.astimezone(timezone.utc)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    m = RU_TEXTUAL_RE.search(s)\n",
        "    if m:\n",
        "        d = int(m.group('d'))\n",
        "        mon_name = m.group('mon')\n",
        "        y = int(m.group('y'))\n",
        "        mth = RU_MONTHS.get(mon_name, None)\n",
        "        if not mth:\n",
        "            return None\n",
        "        hh = int(m.group('h') or 0)\n",
        "        mm = int(m.group('min') or 0)\n",
        "        try:\n",
        "            dt = datetime(y, mth, d, hh, mm, tzinfo=timezone(timedelta(hours=3)))\n",
        "            return dt.astimezone(timezone.utc)\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "# --------- DW (RSS) ----------\n",
        "def fetch_dw():\n",
        "    out = []\n",
        "    try:\n",
        "        r = requests.get(DW_RSS_URL, headers=HEADERS, timeout=20)\n",
        "        r.raise_for_status()\n",
        "        root = ET.fromstring(r.content)\n",
        "        items = [el for el in root.iter() if el.tag.lower().endswith('item')]\n",
        "        if DEBUG: print(f\"[DW] items in RSS: {len(items)}\")\n",
        "        for it in items:\n",
        "            title = link = date_text = None\n",
        "            for ch in it:\n",
        "                tag = ch.tag.lower()\n",
        "                if tag.endswith('title'):\n",
        "                    title = (ch.text or '').strip()\n",
        "                elif tag.endswith('link'):\n",
        "                    link = (ch.text or '').strip()\n",
        "                elif tag.endswith('date') or tag.endswith('pubdate'):\n",
        "                    date_text = (ch.text or '').strip()\n",
        "            if not title or not link:\n",
        "                continue\n",
        "            if not title_matches(title):\n",
        "                continue\n",
        "            dt = parse_iso_or_rfc2822(date_text)\n",
        "            if not dt or not within_days(dt, DAYS_BACK):\n",
        "                continue\n",
        "            out.append({'source': 'DW', 'title': title, 'link': link, 'date': dt})\n",
        "            if len(out) >= MAX_NEWS_PER_SITE:\n",
        "                break\n",
        "    except Exception as e:\n",
        "        print(f\"[DW][ERROR] {e}\")\n",
        "    return out\n",
        "\n",
        "# --------- Finmarket ----------\n",
        "def _req(url: str, timeout=20) -> requests.Response | None:\n",
        "    try:\n",
        "        r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "        enc = r.encoding or r.apparent_encoding or 'utf-8'\n",
        "        r.encoding = enc\n",
        "        if r.status_code == 200:\n",
        "            return r\n",
        "        if DEBUG: print(f\"[Finmarket][WARN] HTTP {r.status_code} for {url}\")\n",
        "    except Exception as e:\n",
        "        if DEBUG: print(f\"[Finmarket][ERR] {e} for {url}\")\n",
        "    return None\n",
        "\n",
        "def _extract_finmarket_links(list_html: str) -> list[str]:\n",
        "    links = []\n",
        "    seen = set()\n",
        "    for m in FM_LINK_RE.finditer(list_html):\n",
        "        rel = m.group(1)\n",
        "        if rel and rel not in seen:\n",
        "            seen.add(rel)\n",
        "            links.append(urllib.parse.urljoin(FINMARKET_LIST_URL, rel))\n",
        "        if len(links) >= MAX_LINKS_FINMARKET:\n",
        "            break\n",
        "    return links\n",
        "\n",
        "def _iso_from_attrs(soup: BeautifulSoup) -> str | None:\n",
        "    # <time datetime=\"2025-08-13T19:04:00+03:00\"> –∏–ª–∏ <meta itemprop=\"datePublished\" content=\"...\">\n",
        "    t = soup.find('time', attrs={'datetime': True}) or soup.find('time', attrs={'content': True})\n",
        "    if t:\n",
        "        return t.get('datetime') or t.get('content')\n",
        "    meta = soup.find('meta', attrs={'itemprop': 'datePublished'})\n",
        "    if meta and meta.get('content'):\n",
        "        return meta['content']\n",
        "    meta2 = soup.find('meta', attrs={'property': 'article:published_time'})\n",
        "    if meta2 and meta2.get('content'):\n",
        "        return meta2['content']\n",
        "    return None\n",
        "\n",
        "def _extract_finmarket_date(html_text: str) -> datetime | None:\n",
        "    soup = BeautifulSoup(html_text, 'html.parser')\n",
        "    # 1) ISO –∏–∑ –∞—Ç—Ä–∏–±—É—Ç–æ–≤\n",
        "    iso = _iso_from_attrs(soup)\n",
        "    if iso:\n",
        "        dt = parse_iso_or_rfc2822(iso)\n",
        "        if dt:\n",
        "            return dt\n",
        "    # 2) ISO –≤ —Å—ã—Ä–æ–º —Ç–µ–∫—Å—Ç–µ\n",
        "    m = ISO_REGEX.search(html_text)\n",
        "    if m:\n",
        "        dt = parse_iso_or_rfc2822(m.group(0))\n",
        "        if dt:\n",
        "            return dt\n",
        "    # 3) –†—É—Å—Å–∫–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã\n",
        "    flat_text = ' '.join(soup.stripped_strings)\n",
        "    return parse_russian_date(flat_text)\n",
        "\n",
        "def _extract_finmarket_title(html_text: str) -> str:\n",
        "    soup = BeautifulSoup(html_text, 'html.parser')\n",
        "    h1 = soup.find('h1')\n",
        "    if h1 and h1.get_text(strip=True):\n",
        "        return h1.get_text(strip=True)\n",
        "    og = soup.find('meta', attrs={'property': 'og:title'})\n",
        "    if og and og.get('content'):\n",
        "        return og['content'].strip()\n",
        "    if soup.title and soup.title.text:\n",
        "        return soup.title.text.strip()\n",
        "    return \"\"\n",
        "\n",
        "def fetch_finmarket():\n",
        "    out = []\n",
        "    list_resp = _req(FINMARKET_LIST_URL)\n",
        "    if not list_resp:\n",
        "        print(\"[Finmarket][ERROR] –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–µ–Ω—Ç—É\")\n",
        "        return out\n",
        "\n",
        "    cand = _extract_finmarket_links(list_resp.text)\n",
        "    if DEBUG: print(f\"[Finmarket] –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(cand)}\")\n",
        "\n",
        "    no_title=0; no_kw=0; old_dt=0; no_dt=0\n",
        "    for url in cand:\n",
        "        r = _req(url, timeout=20)\n",
        "        if not r:\n",
        "            continue\n",
        "        title = _extract_finmarket_title(r.text)\n",
        "        if not title:\n",
        "            no_title += 1\n",
        "            continue\n",
        "        if not title_matches(title):\n",
        "            no_kw += 1\n",
        "            continue\n",
        "        dt = _extract_finmarket_date(r.text)\n",
        "        if not dt:\n",
        "            no_dt += 1\n",
        "            continue\n",
        "        if not within_days(dt, DAYS_BACK):\n",
        "            old_dt += 1\n",
        "            continue\n",
        "        out.append({'source': 'Finmarket', 'title': title, 'link': url, 'date': dt})\n",
        "        if len(out) >= MAX_NEWS_PER_SITE:\n",
        "            break\n",
        "\n",
        "    if DEBUG:\n",
        "        print(f\"[Finmarket] –∏—Ç–æ–≥: {len(out)} –ø—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä | –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞: {no_title}, –±–µ–∑ KEYWORDS: {no_kw}, –±–µ–∑ –¥–∞—Ç—ã: {no_dt}, —Å—Ç–∞—Ä—à–µ {DAYS_BACK} –¥–Ω.: {old_dt}\")\n",
        "    return out\n",
        "\n",
        "# --------- –í–´–í–û–î ---------\n",
        "def print_articles(items):\n",
        "    if not items:\n",
        "        print(\"‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.\")\n",
        "        return\n",
        "    items.sort(key=lambda x: x['date'] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\n",
        "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(items)} –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ {DAYS_BACK} –¥–Ω.\\n\")\n",
        "    try:\n",
        "        from IPython.display import display, Markdown\n",
        "        for i, a in enumerate(items, 1):\n",
        "            dt_local = a['date'].astimezone(timezone(timedelta(hours=6)))  # Asia/Bishkek\n",
        "            display(Markdown(\n",
        "                f\"**[{i}] [{a['source']}] {a['title']}**  \\n\"\n",
        "                f\"üóì {dt_local.strftime('%Y-%m-%d %H:%M')}  \\n\"\n",
        "                f\"üîó [{a['link']}]({a['link']})\\n\"\n",
        "            ))\n",
        "    except Exception:\n",
        "        for i, a in enumerate(items, 1):\n",
        "            dt_local = a['date'].astimezone(timezone(timedelta(hours=6)))\n",
        "            print(f\"[{i}] [{a['source']}] {a['title']}\\n\"\n",
        "                  f\"üóì {dt_local.strftime('%Y-%m-%d %H:%M')}\\n\"\n",
        "                  f\"üîó {a['link']}\\n\")\n",
        "\n",
        "def main():\n",
        "    dw = fetch_dw()\n",
        "    fm = fetch_finmarket()\n",
        "    all_items = dw + fm\n",
        "    print_articles(all_items)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ljviDVVPMib",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "2ljviDVVPMib",
        "outputId": "7ebd8aff-e63b-4b93-ffd8-8b583f2d7377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ –ù–∞–π–¥–µ–Ω–æ 4 –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ 2 –¥–Ω.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": "**[1] [TASS] –ê–∫—Å–∞–∫–æ–≤: —Å 2026 –≥–æ–¥–∞ —Ä–æ—Å—Å–∏—è–Ω–∞–º –∑–∞–ø—Ä–µ—Ç—è—Ç –±—Ä–∞—Ç—å –±–æ–ª–µ–µ –æ–¥–Ω–æ–≥–æ –∑–∞–π–º–∞ –≤ –ú–§–û**  \nüóì 2025-08-14 07:34  \nüîó [https://tass.ru/ekonomika/24776439](https://tass.ru/ekonomika/24776439)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[2] [TASS] –í –†–æ—Å—Å–∏–∏ –æ—Ç–º–µ–Ω–∏–ª–∏ –ø–ª–∞–Ω–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞—É–¥–∏—Ç–æ—Ä—Å–∫–∏—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∏—Å–∫–∞**  \nüóì 2025-08-14 01:18  \nüîó [https://tass.ru/ekonomika/24775349](https://tass.ru/ekonomika/24775349)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[3] [TASS] –ë—Ä–∞–∑–∏–ª–∏—è –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Ç–∞—Ä–∏—Ñ—ã –°–®–ê –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã —á–ª–µ–Ω–æ–≤ –ë–†–ò–ö–°**  \nüóì 2025-08-14 01:03  \nüîó [https://tass.ru/ekonomika/24775379](https://tass.ru/ekonomika/24775379)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "**[4] [TASS] –ú–∏–Ω—Ñ–∏–Ω –°–®–ê –¥–æ 20 –∞–≤–≥—É—Å—Ç–∞ —Ä–∞–∑—Ä–µ—à–∏–ª —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Å –†–§, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è —Å–∞–º–º–∏—Ç–∞**  \nüóì 2025-08-14 00:25  \nüîó [https://tass.ru/ekonomika/24775275](https://tass.ru/ekonomika/24775275)\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import re\n",
        "import html\n",
        "import json\n",
        "import urllib.parse\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ================= –ù–ê–°–¢–†–û–ô–ö–ò =================\n",
        "KEYWORDS = [\n",
        "    '—Ç—Ä–∞–º–ø', 'trump', '–¥–æ–Ω–∞–ª—å–¥',\n",
        "    '—Å—à–∞', '–≥–µ—Ä–º–∞–Ω', '–∫–∏—Ç–∞–π', '—Ä–æ—Å—Å–∏',\n",
        "    '—ç–∫–æ–Ω–æ–º', '–∏–Ω—Ñ–ª—è—Ü', '—Å—Ç–∞–≤–∫', '–≤–≤–ø', 'moex', '—Ñ—Ä—Å', 'ecb'\n",
        "]\n",
        "DAYS_BACK = 2\n",
        "MAX_NEWS_PER_SITE = 50           # –º–∞–∫—Å–∏–º—É–º –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Å–∞–π—Ç (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤)\n",
        "MAX_LINKS_TASS = 120             # —Å–∫–æ–ª—å–∫–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –ø—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —É TASS\n",
        "MAX_LINKS_INTERFAX = 120         # —Å–∫–æ–ª—å–∫–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –ø—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —É Interfax\n",
        "DEBUG = False                    # –≤–∫–ª—é—á–∏—Ç—å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É –æ—Ç–±—Ä–∞–∫–æ–≤–∫–∏\n",
        "\n",
        "# –ò—Å—Ç–æ—á–Ω–∏–∫–∏\n",
        "TASS_LIST_URL = \"https://tass.ru/ekonomika\"\n",
        "INTERFAX_LIST_URL = \"https://www.interfax.ru/business\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                   \"Chrome/125.0 Safari/537.36\"),\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "    \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
        "    \"Connection\": \"keep-alive\",\n",
        "    \"Referer\": \"https://google.com\"\n",
        "}\n",
        "\n",
        "# –†—É—Å—Å–∫–∏–µ –º–µ—Å—è—Ü—ã ‚Üí –Ω–æ–º–µ—Ä\n",
        "RU_MONTHS = {\n",
        "    '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4, '–º–∞—è': 5, '–∏—é–Ω—è': 6,\n",
        "    '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8, '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12\n",
        "}\n",
        "\n",
        "ISO_REGEX = re.compile(r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}(?::\\d{2})?(?:Z|[+\\-]\\d{2}:\\d{2})', re.IGNORECASE)\n",
        "\n",
        "# ================= –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–û–ï =================\n",
        "def now_utc():\n",
        "    return datetime.now(timezone.utc)\n",
        "\n",
        "def within_days(dt: datetime, days: int) -> bool:\n",
        "    \"\"\"dt –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å tz-aware; —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ç–µ–∫—É—â–∏–º UTC.\"\"\"\n",
        "    if dt.tzinfo is None:\n",
        "        dt = dt.replace(tzinfo=timezone.utc)\n",
        "    return (now_utc() - dt) <= timedelta(days=days)\n",
        "\n",
        "def title_matches(title: str) -> bool:\n",
        "    tl = (title or \"\").lower()\n",
        "    return any(kw in tl for kw in KEYWORDS)\n",
        "\n",
        "def _req(url: str, timeout=20) -> requests.Response | None:\n",
        "    try:\n",
        "        r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "        enc = r.encoding or r.apparent_encoding or 'utf-8'\n",
        "        r.encoding = enc\n",
        "        if r.status_code == 200:\n",
        "            return r\n",
        "        if DEBUG: print(f\"[WARN] HTTP {r.status_code} for {url}\")\n",
        "    except Exception as e:\n",
        "        if DEBUG: print(f\"[ERR] {e} for {url}\")\n",
        "    return None\n",
        "\n",
        "# -------- –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç --------\n",
        "def parse_iso_or_rfc2822(s: str) -> datetime | None:\n",
        "    if not s:\n",
        "        return None\n",
        "    s = s.strip()\n",
        "    # ISO 8601\n",
        "    try:\n",
        "        if s.endswith('Z'):\n",
        "            dt = datetime.fromisoformat(s.replace('Z', '+00:00'))\n",
        "        else:\n",
        "            dt = datetime.fromisoformat(s)\n",
        "        return dt.astimezone(timezone.utc)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # RFC2822\n",
        "    try:\n",
        "        from email.utils import parsedate_to_datetime\n",
        "        dt = parsedate_to_datetime(s)\n",
        "        if dt.tzinfo is None:\n",
        "            dt = dt.replace(tzinfo=timezone.utc)\n",
        "        return dt.astimezone(timezone.utc)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "RU_NUMERIC_RE = re.compile(\n",
        "    r'(?P<d>\\d{1,2})\\.(?P<m>\\d{1,2})\\.(?P<y>\\d{4})(?:\\s+(?P<h>\\d{1,2}):(?P<min>\\d{2}))?'\n",
        ")\n",
        "RU_TEXTUAL_RE = re.compile(\n",
        "    r'(?P<d>\\d{1,2})\\s+(?P<mon>[–ê-–Ø–∞-—è]+)\\s+(?P<y>\\d{4})'\n",
        "    r'(?:\\s*–≥\\.?|(?:\\s*–≥–æ–¥–∞)?)?(?:,\\s*|\\s+)?(?:(?P<h>\\d{1,2}):(?P<min>\\d{2}))?',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "def parse_russian_date(text: str, default_tz=timezone(timedelta(hours=3))) -> datetime | None:\n",
        "    \"\"\"–ü–∞—Ä—Å–∏–º '13 –∞–≤–≥—É—Å—Ç–∞ 2025, 16:49' –∏–ª–∏ '13.08.2025 16:49'. –í–æ–∑–≤—Ä–∞—â–∞–µ–º UTC.\"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "    s = html.unescape(text.strip().lower())\n",
        "\n",
        "    m = RU_NUMERIC_RE.search(s)\n",
        "    if m:\n",
        "        d, mth, y = int(m.group('d')), int(m.group('m')), int(m.group('y'))\n",
        "        hh = int(m.group('h') or 0)\n",
        "        mm = int(m.group('min') or 0)\n",
        "        try:\n",
        "            dt = datetime(y, mth, d, hh, mm, tzinfo=default_tz)\n",
        "            return dt.astimezone(timezone.utc)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    m = RU_TEXTUAL_RE.search(s)\n",
        "    if m:\n",
        "        d = int(m.group('d'))\n",
        "        mon_name = m.group('mon')\n",
        "        y = int(m.group('y'))\n",
        "        mth = RU_MONTHS.get(mon_name, None)\n",
        "        if not mth:\n",
        "            return None\n",
        "        hh = int(m.group('h') or 0)\n",
        "        mm = int(m.group('min') or 0)\n",
        "        try:\n",
        "            dt = datetime(y, mth, d, hh, mm, tzinfo=default_tz)\n",
        "            return dt.astimezone(timezone.utc)\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def extract_jsonld_datetime(soup: BeautifulSoup) -> datetime | None:\n",
        "    \"\"\"–ß–∏—Ç–∞–µ–º datePublished –∏–∑ JSON-LD (NewsArticle).\"\"\"\n",
        "    for tag in soup.find_all(\"script\", attrs={\"type\": \"application/ld+json\"}):\n",
        "        try:\n",
        "            data = json.loads(tag.string or tag.text or \"\")\n",
        "        except Exception:\n",
        "            continue\n",
        "        objs = data if isinstance(data, list) else [data]\n",
        "        for obj in objs:\n",
        "            if not isinstance(obj, dict):\n",
        "                continue\n",
        "            dp = obj.get(\"datePublished\") or obj.get(\"dateCreated\")\n",
        "            if dp:\n",
        "                dt = parse_iso_or_rfc2822(dp)\n",
        "                if dt:\n",
        "                    return dt\n",
        "    return None\n",
        "\n",
        "def extract_datetime_generic(html_text: str, soup: BeautifulSoup) -> datetime | None:\n",
        "    # 1) JSON-LD\n",
        "    dt = extract_jsonld_datetime(soup)\n",
        "    if dt:\n",
        "        return dt\n",
        "    # 2) <time datetime=\"...\"> –∏–ª–∏ <meta itemprop=\"datePublished\" content=\"...\"> –∏–ª–∏ og:published_time\n",
        "    t = soup.find('time', attrs={'datetime': True}) or soup.find('time', attrs={'content': True})\n",
        "    if t:\n",
        "        dt = parse_iso_or_rfc2822(t.get('datetime') or t.get('content'))\n",
        "        if dt:\n",
        "            return dt\n",
        "    meta = soup.find('meta', attrs={'itemprop': 'datePublished'}) or \\\n",
        "           soup.find('meta', attrs={'property': 'article:published_time'})\n",
        "    if meta and meta.get('content'):\n",
        "        dt = parse_iso_or_rfc2822(meta['content'])\n",
        "        if dt:\n",
        "            return dt\n",
        "    # 3) ISO –≤ —Å—ã—Ä–æ–º HTML\n",
        "    m = ISO_REGEX.search(html_text)\n",
        "    if m:\n",
        "        dt = parse_iso_or_rfc2822(m.group(0))\n",
        "        if dt:\n",
        "            return dt\n",
        "    # 4) –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç (–±–µ—Ä—ë–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã)\n",
        "    flat = ' '.join(soup.stripped_strings)\n",
        "    return parse_russian_date(flat)\n",
        "\n",
        "def extract_title_generic(soup: BeautifulSoup) -> str:\n",
        "    h1 = soup.find('h1')\n",
        "    if h1 and h1.get_text(strip=True):\n",
        "        return h1.get_text(strip=True)\n",
        "    og = soup.find('meta', attrs={'property': 'og:title'})\n",
        "    if og and og.get('content'):\n",
        "        return og['content'].strip()\n",
        "    if soup.title and soup.title.text:\n",
        "        return soup.title.text.strip()\n",
        "    return \"\"\n",
        "\n",
        "# ================= TASS =================\n",
        "# –õ–æ–≤–∏–º —Å—Å—ã–ª–∫–∏ —Å—Ç–∞—Ç–µ–π —Ä–∞–∑–¥–µ–ª–∞ /ekonomika/<id>...\n",
        "TASS_LINK_RE = re.compile(\n",
        "    r'href=[\"\\'](?:https?://(?:www\\.)?tass\\.ru)?(/ekonomika/\\d+[^\\s\"\\']*)[\"\\']',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "def fetch_tass():\n",
        "    out = []\n",
        "    list_resp = _req(TASS_LIST_URL)\n",
        "    if not list_resp:\n",
        "        print(\"[TASS][ERROR] –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–µ–Ω—Ç—É\")\n",
        "        return out\n",
        "\n",
        "    html_list = list_resp.text\n",
        "    links = []\n",
        "    seen = set()\n",
        "    for m in TASS_LINK_RE.finditer(html_list):\n",
        "        rel = m.group(1)\n",
        "        if not rel:\n",
        "            continue\n",
        "        url = urllib.parse.urljoin(TASS_LIST_URL, rel)\n",
        "        if url not in seen:\n",
        "            seen.add(url)\n",
        "            links.append(url)\n",
        "        if len(links) >= MAX_LINKS_TASS:\n",
        "            break\n",
        "\n",
        "    if DEBUG:\n",
        "        print(f\"[TASS] –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(links)}\")\n",
        "\n",
        "    no_title = no_kw = no_dt = old_dt = 0\n",
        "    for url in links:\n",
        "        r = _req(url, timeout=20)\n",
        "        if not r:\n",
        "            continue\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        title = extract_title_generic(soup)\n",
        "        if not title:\n",
        "            no_title += 1\n",
        "            continue\n",
        "        if not title_matches(title):\n",
        "            no_kw += 1\n",
        "            continue\n",
        "        dt = extract_datetime_generic(r.text, soup)\n",
        "        if not dt:\n",
        "            no_dt += 1\n",
        "            continue\n",
        "        if not within_days(dt, DAYS_BACK):\n",
        "            old_dt += 1\n",
        "            continue\n",
        "        out.append({'source': 'TASS', 'title': title, 'link': url, 'date': dt})\n",
        "        if len(out) >= MAX_NEWS_PER_SITE:\n",
        "            break\n",
        "\n",
        "    if DEBUG:\n",
        "        print(f\"[TASS] –ø—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä: {len(out)} | –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞: {no_title}, –±–µ–∑ KEYWORDS: {no_kw}, –±–µ–∑ –¥–∞—Ç—ã: {no_dt}, —Å—Ç–∞—Ä—à–µ {DAYS_BACK} –¥–Ω.: {old_dt}\")\n",
        "    return out\n",
        "\n",
        "# ================= Interfax =================\n",
        "# –°—Å—ã–ª–∫–∏ –≤–∏–¥–∞ /business/123456\n",
        "INTERFAX_LINK_RE = re.compile(\n",
        "    r'href=[\"\\'](?:https?://(?:www\\.)?interfax\\.ru)?(/business/\\d+[^\\s\"\\']*)[\"\\']',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "def fetch_interfax():\n",
        "    out = []\n",
        "    list_resp = _req(INTERFAX_LIST_URL)\n",
        "    if not list_resp:\n",
        "        print(\"[Interfax][ERROR] –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–µ–Ω—Ç—É\")\n",
        "        return out\n",
        "\n",
        "    html_list = list_resp.text\n",
        "    links = []\n",
        "    seen = set()\n",
        "    for m in INTERFAX_LINK_RE.finditer(html_list):\n",
        "        rel = m.group(1)\n",
        "        if not rel:\n",
        "            continue\n",
        "        url = urllib.parse.urljoin(INTERFAX_LIST_URL, rel)\n",
        "        if url not in seen:\n",
        "            seen.add(url)\n",
        "            links.append(url)\n",
        "        if len(links) >= MAX_LINKS_INTERFAX:\n",
        "            break\n",
        "\n",
        "    if DEBUG:\n",
        "        print(f\"[Interfax] –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(links)}\")\n",
        "\n",
        "    no_title = no_kw = no_dt = old_dt = 0\n",
        "    for url in links:\n",
        "        r = _req(url, timeout=20)\n",
        "        if not r:\n",
        "            continue\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        title = extract_title_generic(soup)\n",
        "        if not title:\n",
        "            no_title += 1\n",
        "            continue\n",
        "        if not title_matches(title):\n",
        "            no_kw += 1\n",
        "            continue\n",
        "        dt = extract_datetime_generic(r.text, soup)\n",
        "        if not dt:\n",
        "            no_dt += 1\n",
        "            continue\n",
        "        if not within_days(dt, DAYS_BACK):\n",
        "            old_dt += 1\n",
        "            continue\n",
        "        out.append({'source': 'Interfax', 'title': title, 'link': url, 'date': dt})\n",
        "        if len(out) >= MAX_NEWS_PER_SITE:\n",
        "            break\n",
        "\n",
        "    if DEBUG:\n",
        "        print(f\"[Interfax] –ø—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä: {len(out)} | –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞: {no_title}, –±–µ–∑ KEYWORDS: {no_kw}, –±–µ–∑ –¥–∞—Ç—ã: {no_dt}, —Å—Ç–∞—Ä—à–µ {DAYS_BACK} –¥–Ω.: {old_dt}\")\n",
        "    return out\n",
        "\n",
        "# ================= –í–´–í–û–î =================\n",
        "def print_articles(items):\n",
        "    if not items:\n",
        "        print(\"‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.\")\n",
        "        return\n",
        "    items.sort(key=lambda x: x['date'] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\n",
        "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(items)} –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ {DAYS_BACK} –¥–Ω.\\n\")\n",
        "\n",
        "    # Jupyter: –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏\n",
        "    try:\n",
        "        from IPython.display import display, Markdown\n",
        "        for i, a in enumerate(items, 1):\n",
        "            # –≤—ã–≤–æ–¥–∏–º –ø–æ Asia/Bishkek (UTC+6), —á—Ç–æ–±—ã –±—ã–ª–æ –≤ —Ç–≤–æ–µ–π TZ\n",
        "            dt_local = a['date'].astimezone(timezone(timedelta(hours=6)))\n",
        "            display(Markdown(\n",
        "                f\"**[{i}] [{a['source']}] {a['title']}**  \\n\"\n",
        "                f\"üóì {dt_local.strftime('%Y-%m-%d %H:%M')}  \\n\"\n",
        "                f\"üîó [{a['link']}]({a['link']})\\n\"\n",
        "            ))\n",
        "    except Exception:\n",
        "        for i, a in enumerate(items, 1):\n",
        "            dt_local = a['date'].astimezone(timezone(timedelta(hours=6)))\n",
        "            print(f\"[{i}] [{a['source']}] {a['title']}\\n\"\n",
        "                  f\"üóì {dt_local.strftime('%Y-%m-%d %H:%M')}\\n\"\n",
        "                  f\"üîó {a['link']}\\n\")\n",
        "\n",
        "def main():\n",
        "    tass_items = fetch_tass()\n",
        "    interfax_items = fetch_interfax()\n",
        "    all_items = tass_items + interfax_items\n",
        "    print_articles(all_items)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RuvrIt4hTR5u",
      "metadata": {
        "id": "RuvrIt4hTR5u"
      },
      "outputs": [],
      "source": [
        "–ì–æ—Ç–æ–≤–æ. –Ø —Å–æ–±—Ä–∞–ª –¥–ª—è —Ç–µ–±—è **–ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π Jupyter Notebook** + **—ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã–π .py-—Å–∫—Ä–∏–ø—Ç** —Å–æ –≤—Å–µ–º –ø–∞–π–ø–ª–∞–π–Ω–æ–º (–∑–∞–≥—Ä—É–∑–∫–∞, –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –≤ —É—Ä–æ–≤–Ω—è—Ö –∏ —Ä–∞–∑–Ω–æ—Å—Ç—è—Ö, rolling/cross-corr, –¥–µ—Ç–µ–∫—Ü–∏—è —à–æ–∫–æ–≤, –Ω–∞–≥–ª—è–¥–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å —à–µ–π–¥–∏–Ω–≥–æ–º, —Ä–∞—Å—á—ë—Ç –ø—Ä–∏–±–ª–∏–∂—ë–Ω–Ω—ã—Ö bond returns –∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –≤ shock-–ø–µ—Ä–∏–æ–¥–∞—Ö).\n",
        "\n",
        "### –§–∞–π–ª—ã\n",
        "\n",
        "* **–ù–æ—É—Ç–±—É–∫:** [ust\\_infl\\_yield\\_fed\\_correlation\\_analysis.ipynb](sandbox:/mnt/data/ust_infl_yield_fed_correlation_analysis.ipynb)\n",
        "* **–°–∫—Ä–∏–ø—Ç:** [analysis.py](sandbox:/mnt/data/analysis.py)\n",
        "* **–î–∞—Ç–∞—Å–µ—Ç (—Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π, –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–ª–æ–∂–∏—à—å —Å–≤–æ–∏ CSV):** [ust\\_infl\\_ffr\\_dataset.csv](sandbox:/mnt/data/ust_infl_ffr_dataset.csv)\n",
        "* **–í–æ–∑–≤—Ä–∞—Ç—ã –æ–±–ª–∏–≥–∞—Ü–∏–π (duration √ó Œîy):** [approx\\_returns.csv](sandbox:/mnt/data/approx_returns.csv)\n",
        "* **–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –ø–æ shock-–æ–∫–Ω–∞–º:** [correlations\\_shock\\_windows.csv](sandbox:/mnt/data/correlations_shock_windows.csv)\n",
        "* –ü–ª—é—Å –≤—Å–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã (corr\\_infl\\_vs\\_yields\\_*.csv, rolling\\_corr\\_*.csv, crosscorr\\_\\*.csv) ‚Äî —É–∂–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ `/mnt/data/`.\n",
        "\n",
        "### –ß—Ç–æ –≤–Ω—É—Ç—Ä–∏ –Ω–æ—É—Ç–±—É–∫–∞\n",
        "\n",
        "1. **–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö** –∏–∑ `/mnt/data/*.csv` (auto-detect –∫–æ–ª–æ–Ω–æ–∫). –ï—Å–ª–∏ —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è —Å–∏–Ω—Ç–µ—Ç–∏–∫–∞, —á—Ç–æ–±—ã –∫–æ–¥ —Ä–∞–±–æ—Ç–∞–ª end-to-end.\n",
        "2. **–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∫ month-end**, —Ä–∞—Å—á—ë—Ç **Œî** –∏ –Ω–∞–∫–ª–æ–Ω–æ–≤ –∫—Ä–∏–≤–æ–π (slope\\_30\\_5, slope\\_10\\_5).\n",
        "3. **–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏**: —É—Ä–æ–≤–Ω–∏ –∏ Œî –º–µ–∂–¥—É `infl_exp`, `fed_funds` –∏ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—è–º–∏ `y5/y10/y20/y30`.\n",
        "4. **Rolling corr (36m)** –¥–ª—è infl ‚Üî –∫–∞–∂–¥–æ–π –¥—é—Ä–∞—Ü–∏–∏.\n",
        "5. **Cross-correlation** (lead/lag ‚àí12‚Ä¶+12) –¥–ª—è infl‚Üîyields –∏ ŒîFed‚ÜîŒîyields.\n",
        "6. **Shock detection**: –æ–∫–Ω–∞ —à–æ–∫–æ–≤ –ø–æ |Œî|‚â•95-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å (—Ä–∞—Å—à–∏—Ä–µ–Ω—ã –Ω–∞ ¬±1 –º–µ—Å—è—Ü), **—à–µ–π–¥–∏–Ω–≥** –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–∞—Ö:\n",
        "\n",
        "   * *Inflation expectations + UST yields*\n",
        "   * *Fed Funds + UST yields*\n",
        "7. **Scatter-–¥–∏–∞–≥—Ä–∞–º–º—ã**: yields vs infl / Œîy vs Œîinfl / yields vs Fed Funds (—Å corr –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö).\n",
        "8. **Bond returns (approx)** —á–µ—Ä–µ–∑ `return ‚âà ‚àíDuration √ó Œîyield` –∏ **–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –≤ shock-–æ–∫–Ω–∞—Ö**:\n",
        "\n",
        "   * (Œîinfl\\_exp, returns\\_{5,10,20,30}) –≤ –∏–Ω—Ñ–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö —à–æ–∫–∞—Ö;\n",
        "   * (Œîfed\\_funds, returns\\_{5,10,20,30}) –≤ —à–æ–∫–∞—Ö —Å—Ç–∞–≤–∫–∏.\n",
        "9. –ë–ª–æ–∫ ¬´**Optional: OLS (Newey‚ÄìWest)**¬ª ‚Äî –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω, –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ `statsmodels` –º–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –∏ –ø–æ–ª—É—á–∏—Ç—å —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ Œ≤ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –¥—é—Ä–∞—Ü–∏—è—Ö –∏ Wald-—Ç–µ—Å—Ç —Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ Œ≤(30Y) –∏ Œ≤(5Y).\n",
        "\n",
        "### –ö–∞–∫ –∑–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∞ **—Ç–≤–æ–∏—Ö** –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "–ü–æ–ª–æ–∂–∏ —Ñ–∞–π–ª—ã –≤ `/mnt/data/` –∏ –ø—Ä–æ—Å—Ç–æ –æ—Ç–∫—Ä–æ–π/–∑–∞–ø—É—Å—Ç–∏ –Ω–æ—É—Ç–±—É–∫:\n",
        "\n",
        "* `inflation_expectations.csv` ‚Äî –∫–æ–ª–æ–Ω–∫–∏ `date, infl_exp` (–∏–ª–∏ `value/breakeven/expectations`)\n",
        "* `treasury_yields.csv` ‚Äî `date, y5, y10, y20, y30` (–∏–ª–∏ `DGS5,DGS10,DGS20,DGS30`)\n",
        "* `fed_funds.csv` ‚Äî `date, fed_funds` (–∏–ª–∏ `DFF/EFFR/fedrate/ffr`)\n",
        "\n",
        "–ö–æ–¥ —Å–∞–º –≤—ã—Ä–æ–≤–Ω—è–µ—Ç —á–∞—Å—Ç–æ—Ç—É, –ø–µ—Ä–µ—Å—á–∏—Ç–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏ –ø–µ—Ä–µ—Ä–∏—Å—É–µ—Ç –≤—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º —à–æ–∫–æ–≤—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤.\n",
        "\n",
        "–ï—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî –¥–æ–±–∞–≤–ª—é –≤ –Ω–æ—É—Ç–±—É–∫:\n",
        "\n",
        "* —Ç–æ—á–Ω—ã–µ **total returns** —Å –∫—É–ø–æ–Ω–æ–º –∏ convexity (–≤–º–µ—Å—Ç–æ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è duration√óŒîy),\n",
        "* —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Å **Newey‚ÄìWest** –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–æ–≥–æ vs –ª–µ–≤–æ–≥–æ –∫—Ä–∞—è (Wald-—Ç–µ—Å—Ç),\n",
        "* **Local Projections** (Jord√†) –¥–ª—è –∏–º–ø—É–ª—å—Å-—Ä–µ–∞–∫—Ü–∏–π –Ω–∞ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞—Ö 1‚Äì12 –º–µ—Å.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94ZJdk1iTT3R",
      "metadata": {
        "id": "94ZJdk1iTT3R"
      },
      "source": [
        "–ì–æ—Ç–æ–≤–æ. –Ø —Å–æ–±—Ä–∞–ª –¥–ª—è —Ç–µ–±—è **–ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π Jupyter Notebook** + **—ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã–π .py-—Å–∫—Ä–∏–ø—Ç** —Å–æ –≤—Å–µ–º –ø–∞–π–ø–ª–∞–π–Ω–æ–º (–∑–∞–≥—Ä—É–∑–∫–∞, –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –≤ —É—Ä–æ–≤–Ω—è—Ö –∏ —Ä–∞–∑–Ω–æ—Å—Ç—è—Ö, rolling/cross-corr, –¥–µ—Ç–µ–∫—Ü–∏—è —à–æ–∫–æ–≤, –Ω–∞–≥–ª—è–¥–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å —à–µ–π–¥–∏–Ω–≥–æ–º, —Ä–∞—Å—á—ë—Ç –ø—Ä–∏–±–ª–∏–∂—ë–Ω–Ω—ã—Ö bond returns –∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –≤ shock-–ø–µ—Ä–∏–æ–¥–∞—Ö).\n",
        "\n",
        "### –§–∞–π–ª—ã\n",
        "\n",
        "* **–ù–æ—É—Ç–±—É–∫:** [ust\\_infl\\_yield\\_fed\\_correlation\\_analysis.ipynb](sandbox:/mnt/data/ust_infl_yield_fed_correlation_analysis.ipynb)\n",
        "* **–°–∫—Ä–∏–ø—Ç:** [analysis.py](sandbox:/mnt/data/analysis.py)\n",
        "* **–î–∞—Ç–∞—Å–µ—Ç (—Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π, –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–ª–æ–∂–∏—à—å —Å–≤–æ–∏ CSV):** [ust\\_infl\\_ffr\\_dataset.csv](sandbox:/mnt/data/ust_infl_ffr_dataset.csv)\n",
        "* **–í–æ–∑–≤—Ä–∞—Ç—ã –æ–±–ª–∏–≥–∞—Ü–∏–π (duration √ó Œîy):** [approx\\_returns.csv](sandbox:/mnt/data/approx_returns.csv)\n",
        "* **–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –ø–æ shock-–æ–∫–Ω–∞–º:** [correlations\\_shock\\_windows.csv](sandbox:/mnt/data/correlations_shock_windows.csv)\n",
        "* –ü–ª—é—Å –≤—Å–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã (corr\\_infl\\_vs\\_yields\\_*.csv, rolling\\_corr\\_*.csv, crosscorr\\_\\*.csv) ‚Äî —É–∂–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ `/mnt/data/`.\n",
        "\n",
        "### –ß—Ç–æ –≤–Ω—É—Ç—Ä–∏ –Ω–æ—É—Ç–±—É–∫–∞\n",
        "\n",
        "1. **–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö** –∏–∑ `/mnt/data/*.csv` (auto-detect –∫–æ–ª–æ–Ω–æ–∫). –ï—Å–ª–∏ —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç ‚Äî –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è —Å–∏–Ω—Ç–µ—Ç–∏–∫–∞, —á—Ç–æ–±—ã –∫–æ–¥ —Ä–∞–±–æ—Ç–∞–ª end-to-end.\n",
        "2. **–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∫ month-end**, —Ä–∞—Å—á—ë—Ç **Œî** –∏ –Ω–∞–∫–ª–æ–Ω–æ–≤ –∫—Ä–∏–≤–æ–π (slope\\_30\\_5, slope\\_10\\_5).\n",
        "3. **–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏**: —É—Ä–æ–≤–Ω–∏ –∏ Œî –º–µ–∂–¥—É `infl_exp`, `fed_funds` –∏ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—è–º–∏ `y5/y10/y20/y30`.\n",
        "4. **Rolling corr (36m)** –¥–ª—è infl ‚Üî –∫–∞–∂–¥–æ–π –¥—é—Ä–∞—Ü–∏–∏.\n",
        "5. **Cross-correlation** (lead/lag ‚àí12‚Ä¶+12) –¥–ª—è infl‚Üîyields –∏ ŒîFed‚ÜîŒîyields.\n",
        "6. **Shock detection**: –æ–∫–Ω–∞ —à–æ–∫–æ–≤ –ø–æ |Œî|‚â•95-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å (—Ä–∞—Å—à–∏—Ä–µ–Ω—ã –Ω–∞ ¬±1 –º–µ—Å—è—Ü), **—à–µ–π–¥–∏–Ω–≥** –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–∞—Ö:\n",
        "\n",
        "   * *Inflation expectations + UST yields*\n",
        "   * *Fed Funds + UST yields*\n",
        "7. **Scatter-–¥–∏–∞–≥—Ä–∞–º–º—ã**: yields vs infl / Œîy vs Œîinfl / yields vs Fed Funds (—Å corr –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö).\n",
        "8. **Bond returns (approx)** —á–µ—Ä–µ–∑ `return ‚âà ‚àíDuration √ó Œîyield` –∏ **–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –≤ shock-–æ–∫–Ω–∞—Ö**:\n",
        "\n",
        "   * (Œîinfl\\_exp, returns\\_{5,10,20,30}) –≤ –∏–Ω—Ñ–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö —à–æ–∫–∞—Ö;\n",
        "   * (Œîfed\\_funds, returns\\_{5,10,20,30}) –≤ —à–æ–∫–∞—Ö —Å—Ç–∞–≤–∫–∏.\n",
        "9. –ë–ª–æ–∫ ¬´**Optional: OLS (Newey‚ÄìWest)**¬ª ‚Äî –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω, –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ `statsmodels` –º–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –∏ –ø–æ–ª—É—á–∏—Ç—å —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ Œ≤ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –¥—é—Ä–∞—Ü–∏—è—Ö –∏ Wald-—Ç–µ—Å—Ç —Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ Œ≤(30Y) –∏ Œ≤(5Y).\n",
        "\n",
        "### –ö–∞–∫ –∑–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∞ **—Ç–≤–æ–∏—Ö** –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "–ü–æ–ª–æ–∂–∏ —Ñ–∞–π–ª—ã –≤ `/mnt/data/` –∏ –ø—Ä–æ—Å—Ç–æ –æ—Ç–∫—Ä–æ–π/–∑–∞–ø—É—Å—Ç–∏ –Ω–æ—É—Ç–±—É–∫:\n",
        "\n",
        "* `inflation_expectations.csv` ‚Äî –∫–æ–ª–æ–Ω–∫–∏ `date, infl_exp` (–∏–ª–∏ `value/breakeven/expectations`)\n",
        "* `treasury_yields.csv` ‚Äî `date, y5, y10, y20, y30` (–∏–ª–∏ `DGS5,DGS10,DGS20,DGS30`)\n",
        "* `fed_funds.csv` ‚Äî `date, fed_funds` (–∏–ª–∏ `DFF/EFFR/fedrate/ffr`)\n",
        "\n",
        "–ö–æ–¥ —Å–∞–º –≤—ã—Ä–æ–≤–Ω—è–µ—Ç —á–∞—Å—Ç–æ—Ç—É, –ø–µ—Ä–µ—Å—á–∏—Ç–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏ –ø–µ—Ä–µ—Ä–∏—Å—É–µ—Ç –≤—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º —à–æ–∫–æ–≤—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤.\n",
        "\n",
        "–ï—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî –¥–æ–±–∞–≤–ª—é –≤ –Ω–æ—É—Ç–±—É–∫:\n",
        "\n",
        "* —Ç–æ—á–Ω—ã–µ **total returns** —Å –∫—É–ø–æ–Ω–æ–º –∏ convexity (–≤–º–µ—Å—Ç–æ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è duration√óŒîy),\n",
        "* —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Å **Newey‚ÄìWest** –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–æ–≥–æ vs –ª–µ–≤–æ–≥–æ –∫—Ä–∞—è (Wald-—Ç–µ—Å—Ç),\n",
        "* **Local Projections** (Jord√†) –¥–ª—è –∏–º–ø—É–ª—å—Å-—Ä–µ–∞–∫—Ü–∏–π –Ω–∞ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞—Ö 1‚Äì12 –º–µ—Å.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8BkUwr1VTUnH",
      "metadata": {
        "id": "8BkUwr1VTUnH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ktVKZgp3TgUq",
      "metadata": {
        "id": "ktVKZgp3TgUq"
      },
      "source": [
        "–û—Ç–ª–∏—á–Ω–æ, –≤–æ—Ç –ø–æ–ª–Ω—ã–π, —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π Python-—Å–∫—Ä–∏–ø—Ç (–æ–¥–∏–Ω —Ñ–∞–π–ª), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –≤–µ—Å—å –ø–∞–π–ø–ª–∞–π–Ω: –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–∏–ª–∏ —Å–∏–Ω—Ç–µ—Ç–∏–∫–∞, –µ—Å–ª–∏ —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç), –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∫ month-end, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (levels/Œî), rolling & cross-corr, –≤—ã–¥–µ–ª–µ–Ω–∏–µ shock-–æ–∫–æ–Ω, –≥—Ä–∞—Ñ–∏–∫–∏ —Å —à–µ–π–¥–∏–Ω–≥–æ–º, –ø—Ä–∏–±–ª–∏–∂—ë–Ω–Ω—ã–µ bond returns —á–µ—Ä–µ–∑ duration√óŒîy –∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (Œîinfl_exp, returns) –∏ (Œîfed_funds, returns) –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –æ–∫–Ω–∞—Ö.\n",
        "–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏ –≤ –ø–∞–ø–∫—É ./outputs.\n",
        "\n",
        "–°–∫–æ–ø–∏—Ä—É–π —ç—Ç–æ –≤ —Ñ–∞–π–ª analysis.py –∏ –∑–∞–ø—É—Å—Ç–∏ python analysis.py.\n",
        "–ï—Å–ª–∏ –ø–æ–ª–æ–∂–∏—à—å —Å–≤–æ–∏ CSV –≤ –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ ‚Äî –∫–æ–¥ –∏—Ö –ø–æ–¥—Ö–≤–∞—Ç–∏—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OfXBrHzGTg2y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfXBrHzGTg2y",
        "outputId": "4b414a2e-2565-433b-cca2-c9500625e4c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1638783889.py:60: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
            "  idxM = pd.date_range(start, pd.Timestamp.today().strftime(\"%Y-%m-01\"), freq=\"M\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] CSV –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚Üí –∏—Å–ø–æ–ª—å–∑—É—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ä—è–¥—ã –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏.\n",
            "[OK] Dataset saved ‚Üí outputs/ust_infl_ffr_dataset.csv\n",
            "[OK] Static correlations saved.\n",
            "[DONE] –í—Å–µ —Ç–∞–±–ª–∏—Ü—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ./outputs\n",
            "   infl_vs_y5_level  infl_vs_y30_level  infl_diff_vs_y5  infl_diff_vs_y30  \\\n",
            "0             0.236              0.224             0.17             0.181   \n",
            "\n",
            "   fed_diff_vs_y5  fed_diff_vs_y30  slope_30_5_vs_infl  \\\n",
            "0           0.151            0.153                0.08   \n",
            "\n",
            "   right_minus_left_infl_level  right_minus_left_infl_diff  \\\n",
            "0                       -0.013                       0.011   \n",
            "\n",
            "   right_minus_left_fed_diff  \n",
            "0                      0.001  \n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Inflation expectations, Treasury yields (5Y/10Y/20Y/30Y), and Fed Funds ‚Äî correlation & shock analysis.\n",
        "Author: ChatGPT (for Daytook)\n",
        "\n",
        "–ó–∞–ø—É—Å–∫:\n",
        "    python analysis.py\n",
        "\n",
        "–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ø–æ–ª–æ–∂–∏ —Ä–µ–∞–ª—å–Ω—ã–µ CSV —Ä—è–¥–æ–º —Å–æ —Å–∫—Ä–∏–ø—Ç–æ–º:\n",
        "    - inflation_expectations.csv  (–∫–æ–ª–æ–Ω–∫–∏: date, infl_exp  | –∏–ª–∏ value/breakeven/expectations)\n",
        "    - treasury_yields.csv         (–∫–æ–ª–æ–Ω–∫–∏: date, y5, y10, y20, y30 | –∏–ª–∏ DGS5,DGS10,DGS20,DGS30)\n",
        "    - fed_funds.csv               (–∫–æ–ª–æ–Ω–∫–∏: date, fed_funds | –∏–ª–∏ DFF,EFFR,fedrate,ffr)\n",
        "\n",
        "–ï—Å–ª–∏ —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç ‚Äî —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è —Å–∏–Ω—Ç–µ—Ç–∏–∫–∞ (–¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞).\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–≤–æ–¥–∞ ----\n",
        "OUT_DIR = Path(\"./outputs\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "INFL_CSV = Path(\"./inflation_expectations.csv\")\n",
        "UST_CSV  = Path(\"./treasury_yields.csv\")\n",
        "FFR_CSV  = Path(\"./fed_funds.csv\")\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5)  # –±–µ–∑ seaborn, –±–µ–∑ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Ü–≤–µ—Ç–æ–≤\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Helpers\n",
        "# =========================\n",
        "def parse_date_col(df: pd.DataFrame) -> pd.DatetimeIndex:\n",
        "    for cand in [\"date\", \"Date\", \"DATE\", \"observation_date\", \"period\", \"time\"]:\n",
        "        if cand in df.columns:\n",
        "            return pd.to_datetime(df[cand])\n",
        "    if isinstance(df.index, pd.DatetimeIndex):\n",
        "        return df.index\n",
        "    raise ValueError(\"–ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü –¥–∞—Ç—ã. –î–æ–±–∞–≤—å 'date' –∏–ª–∏ –ø–æ—Å—Ç–∞–≤—å DatetimeIndex.\")\n",
        "\n",
        "def pick_first_present(df: pd.DataFrame, names: list[str]) -> pd.Series:\n",
        "    for n in names:\n",
        "        if n in df.columns:\n",
        "            return df[n].astype(float)\n",
        "    raise KeyError(f\"–ù–µ—Ç –Ω–∏ –æ–¥–Ω–æ–π –∏–∑ –∫–æ–ª–æ–Ω–æ–∫ {names}. –ù–∞–π–¥–µ–Ω–æ: {list(df.columns)}\")\n",
        "\n",
        "def to_month_end(s: pd.Series) -> pd.Series:\n",
        "    s = s.sort_index()\n",
        "    sm = s.resample(\"M\").last()\n",
        "    return sm.ffill(limit=2)\n",
        "\n",
        "def load_or_fake_data(infl_csv: Path, ust_csv: Path, ffr_csv: Path) -> pd.DataFrame:\n",
        "    \"\"\"–ì—Ä—É–∑–∏–º —Ä–µ–∞–ª—å–Ω—ã–µ CSV, –∏–Ω–∞—á–µ —Å–∏–Ω—Ç–µ—Ç–∏–∫–∞ —Å 2005-01 –ø–æ —Ç–µ–∫—É—â–∏–π –º–µ—Å—è—Ü.\"\"\"\n",
        "    start = \"2005-01-01\"\n",
        "    idxM = pd.date_range(start, pd.Timestamp.today().strftime(\"%Y-%m-01\"), freq=\"M\")\n",
        "    rng = np.random.default_rng(42)\n",
        "\n",
        "    # --- —Å–∏–Ω—Ç–µ—Ç–∏–∫–∞ (fallback) ---\n",
        "    infl_syn = pd.Series(\n",
        "        2.5 + 0.3*np.sin(np.linspace(0, 25, len(idxM))) + 0.5*rng.normal(size=len(idxM)),\n",
        "        index=idxM, name=\"infl_exp\"\n",
        "    ).clip(0, None)\n",
        "\n",
        "    y5_syn  = pd.Series(2.2 + 0.5*infl_syn/2.5 + 0.3*rng.normal(size=len(idxM)), index=idxM, name=\"y5\").clip(0, None)\n",
        "    y10_syn = pd.Series(y5_syn + 0.2 + 0.1*np.sin(np.linspace(0, 10, len(idxM))) + 0.2*rng.normal(size=len(idxM)),\n",
        "                        index=idxM, name=\"y10\").clip(0, None)\n",
        "    y20_syn = pd.Series(y10_syn + 0.15 + 0.15*rng.normal(size=len(idxM)), index=idxM, name=\"y20\").clip(0, None)\n",
        "    y30_syn = pd.Series(y10_syn + 0.25 + 0.2*rng.normal(size=len(idxM)), index=idxM, name=\"y30\").clip(0, None)\n",
        "    ffr_syn = pd.Series((1.0 + 0.6*infl_syn + 0.3*rng.normal(size=len(idxM))).clip(0, None),\n",
        "                        index=idxM, name=\"fed_funds\")\n",
        "\n",
        "    have_files = INFL_CSV.exists() and UST_CSV.exists() and FFR_CSV.exists()\n",
        "    if have_files:\n",
        "        print(\"[INFO] –ó–∞–≥—Ä—É–∂–∞—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ CSV...\")\n",
        "        infl_df = pd.read_csv(infl_csv)\n",
        "        ust_df  = pd.read_csv(ust_csv)\n",
        "        ffr_df  = pd.read_csv(ffr_csv)\n",
        "\n",
        "        infl = pd.Series(\n",
        "            pick_first_present(infl_df, [\"infl_exp\", \"inflation\", \"value\", \"breakeven\", \"expectations\"]).values,\n",
        "            index=parse_date_col(infl_df), name=\"infl_exp\"\n",
        "        )\n",
        "        infl = to_month_end(infl)\n",
        "\n",
        "        ust_df.index = parse_date_col(ust_df)\n",
        "        y5  = to_month_end(pick_first_present(ust_df, [\"y5\", \"DGS5\", \"5y\", \"UST5Y\"]))\n",
        "        y10 = to_month_end(pick_first_present(ust_df, [\"y10\", \"DGS10\", \"10y\", \"UST10Y\"]))\n",
        "        y20 = to_month_end(pick_first_present(ust_df, [\"y20\", \"DGS20\", \"20y\", \"UST20Y\"]))\n",
        "        y30 = to_month_end(pick_first_present(ust_df, [\"y30\", \"DGS30\", \"30y\", \"UST30Y\"]))\n",
        "\n",
        "        ffr = pd.Series(\n",
        "            pick_first_present(ffr_df, [\"fed_funds\", \"DFF\", \"EFFR\", \"fedrate\", \"ffr\"]).values,\n",
        "            index=parse_date_col(ffr_df), name=\"fed_funds\"\n",
        "        )\n",
        "        ffr = to_month_end(ffr)\n",
        "    else:\n",
        "        print(\"[WARN] CSV –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚Üí –∏—Å–ø–æ–ª—å–∑—É—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ä—è–¥—ã –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏.\")\n",
        "        infl, y5, y10, y20, y30, ffr = infl_syn, y5_syn, y10_syn, y20_syn, y30_syn, ffr_syn\n",
        "\n",
        "    df = pd.concat([infl, y5, y10, y20, y30, ffr], axis=1).dropna()\n",
        "    df = df[df.index >= \"2005-01-31\"]\n",
        "    return df\n",
        "\n",
        "def cross_corr(x: pd.Series, y: pd.Series, max_lag: int = 12) -> pd.Series:\n",
        "    lags = list(range(-max_lag, max_lag + 1))\n",
        "    x, y = x - x.mean(), y - y.mean()\n",
        "    out = []\n",
        "    for k in lags:\n",
        "        if k > 0:\n",
        "            v = x.shift(k).corr(y)\n",
        "        elif k < 0:\n",
        "            v = x.corr(y.shift(-k))\n",
        "        else:\n",
        "            v = x.corr(y)\n",
        "        out.append(v)\n",
        "    return pd.Series(out, index=lags)\n",
        "\n",
        "def expand_windows(shock_months: pd.DatetimeIndex) -> list[tuple[pd.Timestamp, pd.Timestamp]]:\n",
        "    \"\"\"–û–∫–Ω–æ = [–ø—Ä–µ–¥—ã–¥—É—â–∏–π –º–µ—Å—è—Ü .. —Å–ª–µ–¥—É—é—â–∏–π –º–µ—Å—è—Ü] –≤–æ–∫—Ä—É–≥ —à–æ–∫–∞ + –º–µ—Ä–¥–∂ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π.\"\"\"\n",
        "    win = []\n",
        "    for dt in shock_months:\n",
        "        start = (dt - pd.offsets.MonthBegin(1)).normalize()\n",
        "        end   = (dt + pd.offsets.MonthEnd(1)).normalize()\n",
        "        win.append([start, end])\n",
        "    win.sort(key=lambda x: x[0])\n",
        "    merged = []\n",
        "    for s, e in win:\n",
        "        if not merged or s > merged[-1][1]:\n",
        "            merged.append([s, e])\n",
        "        else:\n",
        "            merged[-1][1] = max(merged[-1][1], e)\n",
        "    return [(s, e) for s, e in merged]\n",
        "\n",
        "\n",
        "# =========================\n",
        "# –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω\n",
        "# =========================\n",
        "def main():\n",
        "    df = load_or_fake_data(INFL_CSV, UST_CSV, FFR_CSV)\n",
        "    df.to_csv(OUT_DIR / \"ust_infl_ffr_dataset.csv\", index_label=\"date\")\n",
        "    print(f\"[OK] Dataset saved ‚Üí {OUT_DIR/'ust_infl_ffr_dataset.csv'}\")\n",
        "\n",
        "    mats = [\"y5\", \"y10\", \"y20\", \"y30\"]\n",
        "\n",
        "    # Œî –∏ –Ω–∞–∫–ª–æ–Ω—ã\n",
        "    d = df.diff().dropna().rename(columns=lambda c: c + \"_diff\")\n",
        "    dat = pd.concat([df, d], axis=1).dropna()\n",
        "    dat[\"slope_30_5\"] = dat[\"y30\"] - dat[\"y5\"]\n",
        "    dat[\"slope_10_5\"] = dat[\"y10\"] - dat[\"y5\"]\n",
        "\n",
        "    # ---------- –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (levels / diffs) ----------\n",
        "    corr_levels = dat[[\"infl_exp\"] + mats].corr().loc[[\"infl_exp\"], mats]\n",
        "    corr_diffs  = dat[[\"infl_exp_diff\"] + [m+\"_diff\" for m in mats]].corr().loc[[\"infl_exp_diff\"], [m+\"_diff\" for m in mats]]\n",
        "    corr_ff_levels = dat[[\"fed_funds\"] + mats].corr().loc[[\"fed_funds\"], mats]\n",
        "    corr_ff_diffs  = dat[[\"fed_funds_diff\"] + [m+\"_diff\" for m in mats]].corr().loc[[\"fed_funds_diff\"], [m+\"_diff\" for m in mats]]\n",
        "\n",
        "    corr_levels.to_csv(OUT_DIR / \"corr_infl_vs_yields_levels.csv\")\n",
        "    corr_diffs.to_csv(OUT_DIR / \"corr_infl_vs_yields_diffs.csv\")\n",
        "    corr_ff_levels.to_csv(OUT_DIR / \"corr_fedfunds_vs_yields_levels.csv\")\n",
        "    corr_ff_diffs.to_csv(OUT_DIR / \"corr_fedfunds_vs_yields_diffs.csv\")\n",
        "\n",
        "    print(\"[OK] Static correlations saved.\")\n",
        "\n",
        "    # ---------- Rolling 36m corr: infl vs yields ----------\n",
        "    window = 36\n",
        "    roll = pd.DataFrame({m: dat[\"infl_exp\"].rolling(window).corr(dat[m]) for m in mats})\n",
        "    roll.to_csv(OUT_DIR / \"rolling_corr_infl_vs_yields_levels_36m.csv\")\n",
        "\n",
        "    for m in mats:\n",
        "        plt.figure()\n",
        "        plt.plot(roll.index, roll[m].values)\n",
        "        plt.title(f\"Rolling {window}m corr: Inflation vs {m.upper()} (levels)\")\n",
        "        plt.xlabel(\"Date\"); plt.ylabel(\"Correlation\"); plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "        plt.tight_layout(); plt.savefig(OUT_DIR / f\"rolling_corr_infl_{m}.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "    # ---------- Cross-correlation (lead/lag) ----------\n",
        "    cc_infl = pd.DataFrame({m: cross_corr(dat[\"infl_exp\"], dat[m], 12) for m in mats})\n",
        "    cc_infl.to_csv(OUT_DIR / \"crosscorr_infl_vs_yields_levels_lag12.csv\")\n",
        "    for m in mats:\n",
        "        cc = cc_infl[m]\n",
        "        plt.figure()\n",
        "        plt.plot(cc.index, cc.values, marker=\"o\")\n",
        "        plt.axhline(0, linestyle=\"--\")\n",
        "        plt.title(f\"Cross-corr: Inflation vs {m.upper()} (levels)\")\n",
        "        plt.xlabel(\"Lag (months) ‚Äî positive: inflation leads\"); plt.ylabel(\"Correlation\")\n",
        "        plt.grid(True, linestyle=\"--\", alpha=0.6); plt.tight_layout()\n",
        "        plt.savefig(OUT_DIR / f\"crosscorr_infl_{m}.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "    cc_ff = pd.DataFrame({m+\"_diff\": cross_corr(dat[\"fed_funds_diff\"], dat[m+\"_diff\"], 12) for m in mats})\n",
        "    cc_ff.to_csv(OUT_DIR / \"crosscorr_fedfundsdiff_vs_yieldsdiff_lag12.csv\")\n",
        "    for m in mats:\n",
        "        cc = cc_ff[m+\"_diff\"]\n",
        "        plt.figure()\n",
        "        plt.plot(cc.index, cc.values, marker=\"o\")\n",
        "        plt.axhline(0, linestyle=\"--\")\n",
        "        plt.title(f\"Cross-corr: ŒîFed Funds vs Œî{m.upper()}\")\n",
        "        plt.xlabel(\"Lag (months) ‚Äî positive: ŒîFed leads\"); plt.ylabel(\"Correlation\")\n",
        "        plt.grid(True, linestyle=\"--\", alpha=0.6); plt.tight_layout()\n",
        "        plt.savefig(OUT_DIR / f\"crosscorr_fed_{m}.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "    # ---------- Shock detection ----------\n",
        "    infl_thr = dat[\"infl_exp_diff\"].abs().quantile(0.95)\n",
        "    rate_thr = dat[\"fed_funds_diff\"].abs().quantile(0.95)\n",
        "    infl_shock_months = dat.index[dat[\"infl_exp_diff\"].abs() >= infl_thr]\n",
        "    rate_shock_months = dat.index[dat[\"fed_funds_diff\"].abs() >= rate_thr]\n",
        "    infl_windows = expand_windows(infl_shock_months)\n",
        "    rate_windows = expand_windows(rate_shock_months)\n",
        "\n",
        "    # —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–∫–Ω–∞ –≤ CSV\n",
        "    pd.DataFrame(infl_windows, columns=[\"start\", \"end\"]).to_csv(OUT_DIR / \"inflation_shock_windows.csv\", index=False)\n",
        "    pd.DataFrame(rate_windows, columns=[\"start\", \"end\"]).to_csv(OUT_DIR / \"rate_shock_windows.csv\", index=False)\n",
        "\n",
        "    # ---------- –õ–∏–Ω–µ–π–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å —à–µ–π–¥–∏–Ω–≥–æ–º ----------\n",
        "    # Infl + yields\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(df.index, df[\"infl_exp\"], label=\"Inflation exp.\")\n",
        "    for m in mats:\n",
        "        plt.plot(df.index, df[m], label=m.upper())\n",
        "    for s, e in infl_windows:\n",
        "        plt.axvspan(s, e, alpha=0.15)\n",
        "    plt.title(\"Inflation expectations & UST yields ‚Äî shaded: inflation-shock windows\")\n",
        "    plt.xlabel(\"Date\"); plt.ylabel(\"Level (%)\"); plt.grid(True, linestyle=\"--\", alpha=0.6); plt.legend()\n",
        "    plt.tight_layout(); plt.savefig(OUT_DIR / \"inflation_vs_yields_shaded.png\", dpi=170)\n",
        "    plt.close()\n",
        "\n",
        "    # Fed Funds + yields\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(df.index, df[\"fed_funds\"], label=\"Fed Funds\")\n",
        "    for m in mats:\n",
        "        plt.plot(df.index, df[m], label=m.upper())\n",
        "    for s, e in rate_windows:\n",
        "        plt.axvspan(s, e, alpha=0.15)\n",
        "    plt.title(\"Fed Funds & UST yields ‚Äî shaded: rate-shock windows\")\n",
        "    plt.xlabel(\"Date\"); plt.ylabel(\"Level (%)\"); plt.grid(True, linestyle=\"--\", alpha=0.6); plt.legend()\n",
        "    plt.tight_layout(); plt.savefig(OUT_DIR / \"fedfunds_vs_yields_shaded.png\", dpi=170)\n",
        "    plt.close()\n",
        "\n",
        "    # ---------- Scatter: yields vs inflation (levels & diffs) ----------\n",
        "    for m in mats:\n",
        "        plt.figure(); plt.scatter(df[\"infl_exp\"], df[m], s=10)\n",
        "        corr = df[m].corr(df[\"infl_exp\"])\n",
        "        plt.title(f\"{m.upper()} vs Inflation (levels) ‚Äî corr={corr:.2f}\")\n",
        "        plt.xlabel(\"Inflation expectations (%)\"); plt.ylabel(f\"{m.upper()} yield (%)\")\n",
        "        plt.grid(True, linestyle=\"--\", alpha=0.6); plt.tight_layout()\n",
        "        plt.savefig(OUT_DIR / f\"scatter_levels_{m}_infl.png\", dpi=150); plt.close()\n",
        "\n",
        "    for m in mats:\n",
        "        plt.figure(); plt.scatter(dat[\"infl_exp_diff\"], dat[m+\"_diff\"], s=10)\n",
        "        corr = dat[m+\"_diff\"].corr(dat[\"infl_exp_diff\"])\n",
        "        plt.title(f\"Œî{m.upper()} vs ŒîInflation ‚Äî corr={corr:.2f}\")\n",
        "        plt.xlabel(\"ŒîInflation (%)\"); plt.ylabel(f\"Œî{m.upper()} (%)\")\n",
        "        plt.grid(True, linestyle=\"--\", alpha=0.6); plt.tight_layout()\n",
        "        plt.savefig(OUT_DIR / f\"scatter_diffs_{m}_infl.png\", dpi=150); plt.close()\n",
        "\n",
        "    # ---------- Scatter: yields vs Fed Funds (levels) ----------\n",
        "    for m in mats:\n",
        "        plt.figure(); plt.scatter(df[\"fed_funds\"], df[m], s=10)\n",
        "        corr = df[m].corr(df[\"fed_funds\"])\n",
        "        plt.title(f\"{m.upper()} vs Fed Funds (levels) ‚Äî corr={corr:.2f}\")\n",
        "        plt.xlabel(\"Fed Funds (%)\"); plt.ylabel(f\"{m.upper()} yield (%)\")\n",
        "        plt.grid(True, linestyle=\"--\", alpha=0.6); plt.tight_layout()\n",
        "        plt.savefig(OUT_DIR / f\"scatter_levels_{m}_fed.png\", dpi=150); plt.close()\n",
        "\n",
        "    # ---------- Approx bond returns (duration√óŒîy) –∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –≤ shock-–æ–∫–Ω–∞—Ö ----------\n",
        "    dur = {\"y5\": 4.7, \"y10\": 8.7, \"y20\": 13.5, \"y30\": 18.0}\n",
        "    rets = pd.DataFrame(index=dat.index)\n",
        "    for m in mats:\n",
        "        dy_dec = dat[m + \"_diff\"] / 100.0\n",
        "        rets[m.replace(\"y\", \"ret_\")] = -dur[m] * dy_dec\n",
        "    rets.to_csv(OUT_DIR / \"approx_returns.csv\", index_label=\"date\")\n",
        "\n",
        "    def corr_in_windows(x: pd.Series, y: pd.Series, windows: list[tuple[pd.Timestamp, pd.Timestamp]]):\n",
        "        mask = pd.Series(False, index=x.index)\n",
        "        for s, e in windows:\n",
        "            mask |= ((x.index >= s) & (x.index <= e))\n",
        "        xw, yw = x[mask], y[mask]\n",
        "        return float(\"nan\") if len(xw) < 3 else xw.corr(yw)\n",
        "\n",
        "    rows = []\n",
        "    # (Œîinfl_exp, returns_m) –≤ –æ–∫–Ω–∞—Ö –∏–Ω—Ñ–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö —à–æ–∫–æ–≤\n",
        "    for m in mats:\n",
        "        ret_col = m.replace(\"y\", \"ret_\")\n",
        "        r = corr_in_windows(dat[\"infl_exp_diff\"], rets[ret_col], infl_windows)\n",
        "        rows.append({\"window\": \"inflation_shocks\", \"pair\": f\"Œîinfl_exp vs {ret_col}\", \"corr\": r})\n",
        "    # (Œîfed_funds, returns_m) –≤ –æ–∫–Ω–∞—Ö —à–æ–∫–æ–≤ —Å—Ç–∞–≤–∫–∏\n",
        "    for m in mats:\n",
        "        ret_col = m.replace(\"y\", \"ret_\")\n",
        "        r = corr_in_windows(dat[\"fed_funds_diff\"], rets[ret_col], rate_windows)\n",
        "        rows.append({\"window\": \"rate_shocks\", \"pair\": f\"Œîfed_funds vs {ret_col}\", \"corr\": r})\n",
        "\n",
        "    corr_shocks = pd.DataFrame(rows)\n",
        "    corr_shocks.to_csv(OUT_DIR / \"correlations_shock_windows.csv\", index=False)\n",
        "\n",
        "    # ---------- –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞ –ø—Ä–∞–≤—ã–π vs –ª–µ–≤—ã–π –∫—Ä–∞–π ----------\n",
        "    summary = pd.DataFrame({\n",
        "        \"infl_vs_y5_level\":   [corr_levels.loc[\"infl_exp\", \"y5\"]],\n",
        "        \"infl_vs_y30_level\":  [corr_levels.loc[\"infl_exp\", \"y30\"]],\n",
        "        \"infl_diff_vs_y5\":    [corr_diffs.loc[\"infl_exp_diff\", \"y5_diff\"]],\n",
        "        \"infl_diff_vs_y30\":   [corr_diffs.loc[\"infl_exp_diff\", \"y30_diff\"]],\n",
        "        \"fed_diff_vs_y5\":     [corr_ff_diffs.loc[\"fed_funds_diff\", \"y5_diff\"]],\n",
        "        \"fed_diff_vs_y30\":    [corr_ff_diffs.loc[\"fed_funds_diff\", \"y30_diff\"]],\n",
        "        \"slope_30_5_vs_infl\": [dat[[\"slope_30_5\",\"infl_exp\"]].corr().iloc[0,1]],\n",
        "    })\n",
        "    summary[\"right_minus_left_infl_level\"] = summary[\"infl_vs_y30_level\"] - summary[\"infl_vs_y5_level\"]\n",
        "    summary[\"right_minus_left_infl_diff\"]  = summary[\"infl_diff_vs_y30\"]   - summary[\"infl_diff_vs_y5\"]\n",
        "    summary[\"right_minus_left_fed_diff\"]   = summary[\"fed_diff_vs_y30\"]    - summary[\"fed_diff_vs_y5\"]\n",
        "    summary.to_csv(OUT_DIR / \"summary_right_vs_left.csv\", index=False)\n",
        "\n",
        "    print(\"[DONE] –í—Å–µ —Ç–∞–±–ª–∏—Ü—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ./outputs\")\n",
        "    print(summary.round(3))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9-4io_IJTxNB",
      "metadata": {
        "id": "9-4io_IJTxNB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5pfypi1ETxoO",
      "metadata": {
        "id": "5pfypi1ETxoO"
      },
      "source": [
        "–ß—Ç–æ –ø–æ–ª—É—á–∏—à—å –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞\n",
        "\n",
        "–í –ø–∞–ø–∫–µ ./outputs –±—É–¥—É—Ç:\n",
        "\n",
        "ust_infl_ffr_dataset.csv (—Å–æ–±—Ä–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —É—Ä–æ–≤–Ω–µ–π, month-end)\n",
        "\n",
        "corr_*.csv, rolling_corr_*.csv, crosscorr_*.csv\n",
        "\n",
        "inflation_shock_windows.csv, rate_shock_windows.csv\n",
        "\n",
        "approx_returns.csv –∏ correlations_shock_windows.csv\n",
        "\n",
        "PNG-–≥—Ä–∞—Ñ–∏–∫–∏: rolling/cross-corr, –ª–∏–Ω–µ–π–Ω—ã–µ —Å —à–µ–π–¥–∏–Ω–≥–æ–º, scatter-–¥–∏–∞–≥—Ä–∞–º–º—ã\n",
        "\n",
        "–ö–∞–∫ –ø–æ–¥—Å—Ç–∞–≤–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "\n",
        "–ü–æ–ª–æ–∂–∏ —Ä—è–¥–æ–º —Å–æ —Å–∫—Ä–∏–ø—Ç–æ–º:\n",
        "\n",
        "inflation_expectations.csv ‚Äî date, infl_exp (–∏–ª–∏ value/breakeven/expectations)\n",
        "\n",
        "treasury_yields.csv ‚Äî date, y5, y10, y20, y30 (–∏–ª–∏ DGS5,DGS10,DGS20,DGS30)\n",
        "\n",
        "fed_funds.csv ‚Äî date, fed_funds (–∏–ª–∏ DFF/EFFR/fedrate/ffr)\n",
        "\n",
        "–°–∫—Ä–∏–ø—Ç —Å–∞–º:\n",
        "\n",
        "–ø—Ä–∏–≤–µ–¥—ë—Ç –∫ month-end,\n",
        "\n",
        "–Ω–∞–π–¥—ë—Ç shock-–ø–µ—Ä–∏–æ–¥—ã –ø–æ 95-–º—É –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—é |Œî|,\n",
        "\n",
        "–ø–æ—Å—Ç—Ä–æ–∏—Ç –≤—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ –∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏,\n",
        "\n",
        "—Å—Ä–∞–≤–Ω–∏—Ç –ø—Ä–∞–≤—ã–π vs –ª–µ–≤—ã–π –∫—Ä–∞–π (30Y –ø—Ä–æ—Ç–∏–≤ 5Y) –∏ –Ω–∞–∫–ª–æ–Ω slope_30_5.\n",
        "\n",
        "–ï—Å–ª–∏ –Ω—É–∂–Ω–æ ‚Äî –¥–æ–∫–∏–Ω—É –±–ª–æ–∫–∏ OLS (Newey‚ÄìWest) –∏/–∏–ª–∏ —Ä–∞—Å—á—ë—Ç total return –ø–∞—Ä-–±–æ–Ω–¥–æ–≤ —Å –∫—É–ø–æ–Ω–æ–º –∏ convexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cyOrpkm-TyDN",
      "metadata": {
        "id": "cyOrpkm-TyDN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}