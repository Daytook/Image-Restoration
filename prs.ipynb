{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, html, json, urllib.parse\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ========= –ù–ê–°–¢–†–û–ô–ö–ò =========\n",
    "KEYWORDS = [\n",
    "    '—Ç—Ä–∞–º–ø','trump','–¥–æ–Ω–∞–ª—å–¥','—Å—à–∞','–≥–µ—Ä–º–∞–Ω','–∫–∏—Ç–∞–π','—Ä–æ—Å—Å–∏',\n",
    "    '—ç–∫–æ–Ω–æ–º','–∏–Ω—Ñ–ª—è—Ü','—Å—Ç–∞–≤–∫','–≤–≤–ø','moex','—Ñ—Ä—Å','ecb'\n",
    "]\n",
    "DAYS_BACK = 2\n",
    "TZ_OUT = timezone(timedelta(hours=6))  # Asia/Bishkek\n",
    "MAX_NEWS_PER_SITE = 50\n",
    "# –ª–∏–º–∏—Ç—ã –Ω–∞ —á–∏—Å–ª–æ –∫–∞—Ä—Ç–æ—á–µ–∫, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –Ω–∞ ¬´—Ç—è–∂—ë–ª—ã—Ö¬ª –ª–µ–Ω—Ç–∞—Ö\n",
    "MAX_LINKS_TASS = 120\n",
    "MAX_LINKS_INTERFAX = 120\n",
    "MAX_LINKS_FINMARKET = 150\n",
    "\n",
    "# ========= –ò–°–¢–û–ß–ù–ò–ö–ò =========\n",
    "RBC_URL = \"https://www.rbc.ru/economics/\"\n",
    "IZ_URL  = \"https://iz.ru/news\"\n",
    "IZ_BASE = \"https://iz.ru\"\n",
    "DW_RSS  = \"https://rss.dw.com/rdf/rss-ru-all\"\n",
    "FINMARKET_LIST = \"https://www.finmarket.ru/news/\"\n",
    "TASS_LIST = \"https://tass.ru/ekonomika\"\n",
    "INTERFAX_LIST = \"https://www.interfax.ru/business\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                   \"Chrome/125.0 Safari/537.36\"),\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Referer\": \"https://google.com\"\n",
    "}\n",
    "\n",
    "# ========= –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–û–ï =========\n",
    "RU_MONTHS = {\n",
    "    '—è–Ω–≤–∞—Ä—è':1,'—Ñ–µ–≤—Ä–∞–ª—è':2,'–º–∞—Ä—Ç–∞':3,'–∞–ø—Ä–µ–ª—è':4,'–º–∞—è':5,'–∏—é–Ω—è':6,\n",
    "    '–∏—é–ª—è':7,'–∞–≤–≥—É—Å—Ç–∞':8,'—Å–µ–Ω—Ç—è–±—Ä—è':9,'–æ–∫—Ç—è–±—Ä—è':10,'–Ω–æ—è–±—Ä—è':11,'–¥–µ–∫–∞–±—Ä—è':12\n",
    "}\n",
    "ISO_RE = re.compile(r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}(?::\\d{2})?(?:Z|[+\\-]\\d{2}:\\d{2})', re.I)\n",
    "RU_NUM_RE = re.compile(r'(?P<d>\\d{1,2})\\.(?P<m>\\d{1,2})\\.(?P<y>\\d{4})(?:\\s+(?P<h>\\d{1,2}):(?P<min>\\d{2}))?')\n",
    "RU_TXT_RE = re.compile(\n",
    "    r'(?P<d>\\d{1,2})\\s+(?P<mon>[–ê-–Ø–∞-—è]+)\\s+(?P<y>\\d{4})(?:\\s*–≥\\.?|(?:\\s*–≥–æ–¥–∞)?)?(?:,\\s*|\\s+)?(?:(?P<h>\\d{1,2}):(?P<min>\\d{2}))?',\n",
    "    re.I\n",
    ")\n",
    "\n",
    "def _now_utc(): return datetime.now(timezone.utc)\n",
    "def _within_days(dt, days):\n",
    "    if dt.tzinfo is None: dt = dt.replace(tzinfo=timezone.utc)\n",
    "    return (_now_utc() - dt) <= timedelta(days=days)\n",
    "def _kw_ok(title):\n",
    "    tl = (title or '').lower()\n",
    "    return any(kw in tl for kw in KEYWORDS)\n",
    "\n",
    "def _req(session, url, timeout=20):\n",
    "    r = session.get(url, headers=HEADERS, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.encoding or r.apparent_encoding or 'utf-8'\n",
    "    return r\n",
    "\n",
    "def _parse_iso_or_rfc(s):\n",
    "    if not s: return None\n",
    "    s = s.strip()\n",
    "    try:\n",
    "        if s.endswith('Z'): dt = datetime.fromisoformat(s.replace('Z', '+00:00'))\n",
    "        else: dt = datetime.fromisoformat(s)\n",
    "        return dt.astimezone(timezone.utc)\n",
    "    except: pass\n",
    "    try:\n",
    "        from email.utils import parsedate_to_datetime\n",
    "        dt = parsedate_to_datetime(s)\n",
    "        if dt.tzinfo is None: dt = dt.replace(tzinfo=timezone.utc)\n",
    "        return dt.astimezone(timezone.utc)\n",
    "    except: return None\n",
    "\n",
    "def _parse_ru_date(text, default_tz=timezone(timedelta(hours=3))):\n",
    "    if not text: return None\n",
    "    s = html.unescape(text.strip().lower())\n",
    "    m = RU_NUM_RE.search(s)\n",
    "    if m:\n",
    "        d,mn,y = int(m['d']), int(m['m']), int(m['y'])\n",
    "        hh = int(m['h'] or 0); mm = int(m['min'] or 0)\n",
    "        try: return datetime(y,mn,d,hh,mm,tzinfo=default_tz).astimezone(timezone.utc)\n",
    "        except: return None\n",
    "    m = RU_TXT_RE.search(s)\n",
    "    if m:\n",
    "        d = int(m['d']); mn = RU_MONTHS.get(m['mon']); y = int(m['y'])\n",
    "        if not mn: return None\n",
    "        hh = int(m['h'] or 0); mm = int(m['min'] or 0)\n",
    "        try: return datetime(y,mn,d,hh,mm,tzinfo=default_tz).astimezone(timezone.utc)\n",
    "        except: return None\n",
    "    return None\n",
    "\n",
    "def _jsonld_datetime(soup):\n",
    "    for tag in soup.find_all(\"script\", attrs={\"type\":\"application/ld+json\"}):\n",
    "        try: data = json.loads(tag.string or tag.text or \"\")\n",
    "        except: continue\n",
    "        objs = data if isinstance(data, list) else [data]\n",
    "        for obj in objs:\n",
    "            if isinstance(obj, dict):\n",
    "                dp = obj.get(\"datePublished\") or obj.get(\"dateCreated\")\n",
    "                if dp:\n",
    "                    dt = _parse_iso_or_rfc(dp)\n",
    "                    if dt: return dt\n",
    "    return None\n",
    "\n",
    "def _extract_datetime(html_text, soup):\n",
    "    dt = _jsonld_datetime(soup)\n",
    "    if dt: return dt\n",
    "    t = soup.find('time', attrs={'datetime':True}) or soup.find('time', attrs={'content':True})\n",
    "    if t:\n",
    "        dt = _parse_iso_or_rfc(t.get('datetime') or t.get('content'));\n",
    "        if dt: return dt\n",
    "    meta = soup.find('meta', attrs={'itemprop':'datePublished'}) or soup.find('meta', attrs={'property':'article:published_time'})\n",
    "    if meta and meta.get('content'):\n",
    "        dt = _parse_iso_or_rfc(meta['content']);\n",
    "        if dt: return dt\n",
    "    m = ISO_RE.search(html_text)\n",
    "    if m:\n",
    "        dt = _parse_iso_or_rfc(m.group(0))\n",
    "        if dt: return dt\n",
    "    flat = ' '.join(soup.stripped_strings)\n",
    "    return _parse_ru_date(flat)\n",
    "\n",
    "def _title_generic(soup):\n",
    "    h1 = soup.find('h1')\n",
    "    if h1 and h1.get_text(strip=True): return h1.get_text(strip=True)\n",
    "    og = soup.find('meta', attrs={'property':'og:title'})\n",
    "    if og and og.get('content'): return og['content'].strip()\n",
    "    if soup.title and soup.title.text: return soup.title.text.strip()\n",
    "    return \"\"\n",
    "\n",
    "def _emit(items):\n",
    "    if not items:\n",
    "        print(\"‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π.\"); return\n",
    "    items.sort(key=lambda x: x['date'] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)\n",
    "    print(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(items)} –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ {DAYS_BACK} –¥–Ω.\\n\")\n",
    "    try:\n",
    "        from IPython.display import display, Markdown\n",
    "        for i, a in enumerate(items, 1):\n",
    "            dt = a['date'].astimezone(TZ_OUT) if a['date'] else None\n",
    "            ds = dt.strftime('%Y-%m-%d %H:%M') if dt else ''\n",
    "            display(Markdown(f\"**[{i}] [{a['source']}] {a['title']}**  \\n\"\n",
    "                             f\"{'üóì '+ds if ds else ''}  \\n\"\n",
    "                             f\"üîó [{a['link']}]({a['link']})\\n\"))\n",
    "    except:\n",
    "        for i, a in enumerate(items, 1):\n",
    "            dt = a['date'].astimezone(TZ_OUT) if a['date'] else None\n",
    "            ds = dt.strftime('%Y-%m-%d %H:%M') if dt else ''\n",
    "            print(f\"[{i}] [{a['source']}] {a['title']}\\n\"\n",
    "                  f\"{('üóì '+ds+'\\n') if ds else ''}\"\n",
    "                  f\"üîó {a['link']}\\n\")\n",
    "\n",
    "# ========= –ò–°–¢–û–ß–ù–ò–ö–ò: –ü–ê–†–°–ò–ù–ì =========\n",
    "\n",
    "def fetch_rbc(session):\n",
    "    \"\"\"RBC –≠–∫–æ–Ω–æ–º–∏–∫–∞: –ø–∞—Ä—Å–∏–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–¥–∞—Ç–∞ –±–µ—Ä—ë—Ç—Å—è –∏–∑ URL).\"\"\"\n",
    "    out = []\n",
    "    html_text = _req(session, RBC_URL).text\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    items = soup.find_all('div', class_='item')\n",
    "    rx = re.compile(r'/(\\d{2})/(\\d{2})/(\\d{4})/')\n",
    "    cutoff = datetime.now() - timedelta(days=DAYS_BACK)\n",
    "    for it in items:\n",
    "        title_tag = it.find('span','item__title'); link_tag = it.find('a','item__link')\n",
    "        if not title_tag or not link_tag: continue\n",
    "        title = ''.join(title_tag.stripped_strings)\n",
    "        link = link_tag.get('href','').strip()\n",
    "        m = rx.search(link);\n",
    "        if not m: continue\n",
    "        d,mn,y = map(int, m.groups())\n",
    "        dt = datetime(y,mn,d)\n",
    "        if dt < cutoff: continue\n",
    "        if not _kw_ok(title): continue\n",
    "        out.append({'source':'RBC','title':title,'link':link,'date':dt.replace(tzinfo=timezone.utc)})\n",
    "        if len(out) >= MAX_NEWS_PER_SITE: break\n",
    "    return out\n",
    "\n",
    "def _extract_iz_json(html_text):\n",
    "    m = re.search(r'window\\.recommendationBlockList\\s*=\\s*{', html_text)\n",
    "    if not m: return None\n",
    "    start = m.end()-1; braces=0; end=start\n",
    "    for i,ch in enumerate(html_text[start:], start=start):\n",
    "        if ch=='{': braces+=1\n",
    "        elif ch=='}':\n",
    "            braces-=1\n",
    "            if braces==0: end=i; break\n",
    "    return html_text[start:end+1]\n",
    "\n",
    "def fetch_iz(session):\n",
    "    \"\"\"Izvestia –ù–æ–≤–æ—Å—Ç–∏: –±–µ—Ä—ë–º JSON –∏–∑ window.recommendationBlockList; —Ñ–∏–ª—å—Ç—Ä—É–µ–º –≤–Ω–µ—à–Ω–∏–µ –¥–æ–º–µ–Ω—ã.\"\"\"\n",
    "    out, seen = [], set()\n",
    "    html_text = _req(session, IZ_URL).text\n",
    "    js = _extract_iz_json(html_text)\n",
    "    if not js: return out\n",
    "    data = json.loads(js)\n",
    "    for key in ('even','odd'):\n",
    "        for item in (data.get(key) or []):\n",
    "            raw = (item.get('path') or item.get('reference') or '').strip()\n",
    "            if not raw: continue\n",
    "            full = raw if raw.startswith('http') else (IZ_BASE + raw if raw.startswith('/') else IZ_BASE+'/'+raw)\n",
    "            if 'iz.ru' not in urllib.parse.urlparse(full).netloc:  # –≤—ã–∫–∏–¥—ã–≤–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —Å–∞–π—Ç—ã\n",
    "                continue\n",
    "            title = (item.get('title') or '').strip()\n",
    "            if not _kw_ok(title): continue\n",
    "            if full in seen: continue\n",
    "            seen.add(full)\n",
    "            out.append({'source':'Izvestia','title':title,'link':full,'date':None})\n",
    "            if len(out) >= MAX_NEWS_PER_SITE: break\n",
    "        if len(out) >= MAX_NEWS_PER_SITE: break\n",
    "    return out\n",
    "\n",
    "def fetch_dw(session):\n",
    "    \"\"\"DW —á–µ—Ä–µ–∑ RSS ‚Äî –±—ã—Å—Ç—Ä–æ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ.\"\"\"\n",
    "    out = []\n",
    "    r = _req(session, DW_RSS, timeout=20)\n",
    "    import xml.etree.ElementTree as ET\n",
    "    root = ET.fromstring(r.content)\n",
    "    items = [el for el in root.iter() if el.tag.lower().endswith('item')]\n",
    "    for it in items:\n",
    "        title=link=date_text=None\n",
    "        for ch in it:\n",
    "            tag = ch.tag.lower()\n",
    "            if tag.endswith('title'): title = (ch.text or '').strip()\n",
    "            elif tag.endswith('link'): link = (ch.text or '').strip()\n",
    "            elif tag.endswith('date') or tag.endswith('pubdate'): date_text = (ch.text or '').strip()\n",
    "        if not title or not link: continue\n",
    "        if not _kw_ok(title): continue\n",
    "        dt = _parse_iso_or_rfc(date_text)\n",
    "        if not dt or not _within_days(dt, DAYS_BACK): continue\n",
    "        out.append({'source':'DW','title':title,'link':link,'date':dt})\n",
    "        if len(out) >= MAX_NEWS_PER_SITE: break\n",
    "    return out\n",
    "\n",
    "# Finmarket\n",
    "FM_LINK_RE = re.compile(r'href=[\"\\'](?:https?://[^\"\\']+)?(/news/\\d+/?)(?:\\?[^\"\\']*)?[\"\\']', re.I)\n",
    "def _fin_title(html_text):\n",
    "    s = BeautifulSoup(html_text,'html.parser')\n",
    "    h1 = s.find('h1');\n",
    "    if h1 and h1.get_text(strip=True): return h1.get_text(strip=True)\n",
    "    og = s.find('meta', attrs={'property':'og:title'})\n",
    "    if og and og.get('content'): return og['content'].strip()\n",
    "    return s.title.text.strip() if s.title and s.title.text else \"\"\n",
    "\n",
    "def _fin_date(html_text):\n",
    "    s = BeautifulSoup(html_text,'html.parser')\n",
    "    t = s.find('time', attrs={'datetime':True}) or s.find('time', attrs={'content':True})\n",
    "    if t:\n",
    "        d = _parse_iso_or_rfc(t.get('datetime') or t.get('content'))\n",
    "        if d: return d\n",
    "    meta = s.find('meta', attrs={'itemprop':'datePublished'}) or s.find('meta', attrs={'property':'article:published_time'})\n",
    "    if meta and meta.get('content'):\n",
    "        d = _parse_iso_or_rfc(meta['content'])\n",
    "        if d: return d\n",
    "    m = ISO_RE.search(html_text)\n",
    "    if m:\n",
    "        d = _parse_iso_or_rfc(m.group(0))\n",
    "        if d: return d\n",
    "    flat = ' '.join(s.stripped_strings for s in [soup for soup in [s]][0])  # trick to keep it one-liner\n",
    "    # —É–ø—Ä–æ—â—ë–Ω–Ω–æ: –ø–∞—Ä—Å–∏–º –∏–∑ –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "    flat = ' '.join(BeautifulSoup(html_text,'html.parser').stripped_strings)\n",
    "    return _parse_ru_date(flat)\n",
    "\n",
    "def fetch_finmarket(session):\n",
    "    out, seen = [], set()\n",
    "    lst = _req(session, FINMARKET_LIST).text\n",
    "    cands = []\n",
    "    for m in FM_LINK_RE.finditer(lst):\n",
    "        rel = m.group(1)\n",
    "        if rel and rel not in seen:\n",
    "            seen.add(rel)\n",
    "            cands.append(urllib.parse.urljoin(FINMARKET_LIST, rel))\n",
    "        if len(cands) >= MAX_LINKS_FINMARKET: break\n",
    "    for url in cands:\n",
    "        r = _req(session, url)\n",
    "        title = _fin_title(r.text)\n",
    "        if not title or not _kw_ok(title): continue\n",
    "        dt = _fin_date(r.text)\n",
    "        if not dt or not _within_days(dt, DAYS_BACK): continue\n",
    "        out.append({'source':'Finmarket','title':title,'link':url,'date':dt})\n",
    "        if len(out) >= MAX_NEWS_PER_SITE: break\n",
    "    return out\n",
    "\n",
    "# TASS\n",
    "TASS_LINK_RE = re.compile(r'href=[\"\\'](?:https?://(?:www\\.)?tass\\.ru)?(/ekonomika/\\d+[^\\s\"\\']*)[\"\\']', re.I)\n",
    "def fetch_tass(session):\n",
    "    out, seen = [], set()\n",
    "    lst = _req(session, TASS_LIST).text\n",
    "    links = []\n",
    "    for m in TASS_LINK_RE.finditer(lst):\n",
    "        rel = m.group(1)\n",
    "        if not rel: continue\n",
    "        url = urllib.parse.urljoin(TASS_LIST, rel)\n",
    "        if url in seen: continue\n",
    "        seen.add(url); links.append(url)\n",
    "        if len(links) >= MAX_LINKS_TASS: break\n",
    "    for url in links:\n",
    "        r = _req(session, url)\n",
    "        soup = BeautifulSoup(r.text,'html.parser')\n",
    "        title = _title_generic(soup)\n",
    "        if not title or not _kw_ok(title): continue\n",
    "        dt = _extract_datetime(r.text, soup)\n",
    "        if not dt or not _within_days(dt, DAYS_BACK): continue\n",
    "        out.append({'source':'TASS','title':title,'link':url,'date':dt})\n",
    "        if len(out) >= MAX_NEWS_PER_SITE: break\n",
    "    return out\n",
    "\n",
    "# Interfax\n",
    "INTERFAX_LINK_RE = re.compile(r'href=[\"\\'](?:https?://(?:www\\.)?interfax\\.ru)?(/business/\\d+[^\\s\"\\']*)[\"\\']', re.I)\n",
    "def fetch_interfax(session):\n",
    "    out, seen = [], set()\n",
    "    lst = _req(session, INTERFAX_LIST).text\n",
    "    links = []\n",
    "    for m in INTERFAX_LINK_RE.finditer(lst):\n",
    "        rel = m.group(1)\n",
    "        if not rel: continue\n",
    "        url = urllib.parse.urljoin(INTERFAX_LIST, rel)\n",
    "        if url in seen: continue\n",
    "        seen.add(url); links.append(url)\n",
    "        if len(links) >= MAX_LINKS_INTERFAX: break\n",
    "    for url in links:\n",
    "        r = _req(session, url)\n",
    "        soup = BeautifulSoup(r.text,'html.parser')\n",
    "        title = _title_generic(soup)\n",
    "        if not title or not _kw_ok(title): continue\n",
    "        dt = _extract_datetime(r.text, soup)\n",
    "        if not dt or not _within_days(dt, DAYS_BACK): continue\n",
    "        out.append({'source':'Interfax','title':title,'link':url,'date':dt})\n",
    "        if len(out) >= MAX_NEWS_PER_SITE: break\n",
    "    return out\n",
    "\n",
    "# ========= MAIN =========\n",
    "def main():\n",
    "    sess = requests.Session()\n",
    "    results, seen_links = [], set()\n",
    "\n",
    "    for fetch in (fetch_rbc, fetch_iz, fetch_dw, fetch_finmarket, fetch_tass, fetch_interfax):\n",
    "        try:\n",
    "            items = fetch(sess)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {fetch.__name__}: {e}\")\n",
    "            items = []\n",
    "        for a in items:\n",
    "            if a['link'] in seen_links: continue\n",
    "            seen_links.add(a['link']); results.append(a)\n",
    "\n",
    "    _emit(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
